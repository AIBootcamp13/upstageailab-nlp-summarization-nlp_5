#!/usr/bin/env python
"""
ğŸš€ Final Boost Inference Script
ê¸°ì¡´ best ëª¨ë¸ì„ í™œìš©í•œ ìµœì í™”ëœ ì¶”ë¡  ì‹¤í–‰
"""

import argparse
import yaml
import torch
import sys
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_dir = "/data/ephemeral/home/nlp-5/lyj"
sys.path.append(project_dir)

from models.BART import load_tokenizer_and_model_for_inference
from inference.inference_modified import inference
from dataset.preprocess import Preprocess

def main():
    parser = argparse.ArgumentParser(description='Final Boost Inference')
    parser.add_argument('--config', type=str, default='config_final_boost.yaml',
                       help='Config file name in src/configs/')
    args = parser.parse_args()
    
    # Config íŒŒì¼ ë¡œë“œ
    config_path = os.path.join(project_dir, 'src/configs', args.config)
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.load(f, Loader=yaml.FullLoader)
    
    print("ğŸš€ Final Boost Inference Started!")
    print(f"ğŸ“ Config: {args.config}")
    print(f"ğŸ¯ Model checkpoint: {config['inference']['ckt_dir']}")
    print(f"ğŸ“Š Target score: {config.get('experiment', {}).get('target_score', 'N/A')}")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ Device: {device}")
    
    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("ğŸ“¦ Loading model and tokenizer...")
    model, tokenizer = load_tokenizer_and_model_for_inference(config, device)
    
    # ì¶”ë¡  ì‹¤í–‰
    print("ğŸ”® Starting inference...")
    result = inference(config, model, tokenizer)
    
    print(f"âœ… Inference completed!")
    print(f"ğŸ“ Results saved to: {config['inference']['result_path']}/result1")
    print(f"ğŸ“Š Generated {len(result)} summaries")
    
    # ìƒ˜í”Œ ê²°ê³¼ ì¶œë ¥
    print("\nğŸ” Sample results:")
    for i in range(min(3, len(result))):
        print(f"  {result.iloc[i]['fname']}: {result.iloc[i]['summary'][:100]}...")

if __name__ == "__main__":
    main()
