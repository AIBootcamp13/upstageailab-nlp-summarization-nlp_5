#!/usr/bin/env python
"""
âš¡ Quick Inference Script (WandB ì—†ì´)
ê¸°ì¡´ ëª¨ë¸ë¡œ ë¹ ë¥¸ ì¶”ë¡  ì‹¤í–‰
"""

import os
import torch
import yaml
import sys
import pandas as pd

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append("/data/ephemeral/home/nlp-5/lyj")

from models.BART import load_tokenizer_and_model_for_inference
from inference.inference_modified import inference
from dataset.preprocess import Preprocess

def quick_inference(config_name="config_quick_boost.yaml"):
    """ë¹ ë¥¸ ì¶”ë¡  ì‹¤í–‰"""
    
    # Config ë¡œë“œ
    config_path = f"/data/ephemeral/home/nlp-5/lyj/src/configs/{config_name}"
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.load(f, Loader=yaml.FullLoader)
    
    print(f"âš¡ Quick Inference Started!")
    print(f"ğŸ“ Config: {config_name}")
    print(f"ğŸ¯ Using model: {config['inference']['ckt_dir']}")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ Device: {device}")
    
    try:
        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        print("ğŸ“¦ Loading model and tokenizer...")
        model, tokenizer = load_tokenizer_and_model_for_inference(config, device)
        
        # ì¶”ë¡  ì‹¤í–‰
        print("ğŸ”® Starting inference...")
        result = inference(config, model, tokenizer)
        
        print(f"âœ… Inference completed!")
        print(f"ğŸ“ Results saved to: {config['inference']['result_path']}/result1")
        print(f"ğŸ“Š Generated {len(result)} summaries")
        
        return result
        
    except Exception as e:
        print(f"âŒ Error during inference: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', default='config_quick_boost.yaml')
    args = parser.parse_args()
    
    quick_inference(args.config)
