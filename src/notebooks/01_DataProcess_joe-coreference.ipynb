{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2079ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 구성 정보를 YAML 파일로 저장합니다.\n",
    "project_dir = \"/data/ephemeral/home/nlp-5/song/\"\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\n",
    "    project_dir\n",
    ")\n",
    "from src.utils.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(project_dir, 'data','train.csv'))\n",
    "val_df = pd.read_csv(os.path.join(project_dir, 'data','dev.csv'))\n",
    "\n",
    "test_df = pd.read_csv(os.path.join(project_dir, 'data','test.csv'))\n",
    "sub_df = pd.read_csv(os.path.join(project_dir, 'outputs', 'exp_aug_0730191312', 'submission_0730191312-0.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acf73067",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'train': train_df,\n",
    "    'dev': val_df,\n",
    "    'test': test_df,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b1814f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [1] 약어/이모티콘 빈도 분석\n",
      "\n",
      "TRAIN 상위 약어 100개:\n",
      "[('Mr', 748), ('TV', 230), ('PhoneNumber', 228), ('Mary', 190), ('Smith', 149), ('Mrs', 136), ('Tom', 134), ('John', 121), ('Bob', 101), ('Dr', 94), ('Jack', 92), ('Miss', 91), ('Address', 89), ('Jane', 84), ('Mike', 81), ('David', 76), ('Ms', 76), ('Brown', 75), ('Bill', 66), ('Susan', 63), ('Jim', 62), ('Steven', 62), ('Green', 58), ('Li', 58), ('White', 55), ('Wang', 49), ('Sam', 44), ('Alice', 42), ('Mark', 42), ('Paul', 41), ('CD', 40), ('Jenny', 39), ('The', 39), ('Peter', 38), ('James', 37), ('Ann', 37), ('Tim', 35), ('Sarah', 35), ('ABC', 34), ('Johnson', 34), ('of', 33), ('Helen', 32), ('Jones', 32), ('Kate', 31), ('Taylor', 31), ('Janet', 30), ('George', 30), ('Amy', 29), ('the', 28), ('Daniel', 27), ('Lily', 27), ('Steve', 27), ('Sally', 27), ('Black', 27), ('Jason', 26), ('Michael', 26), ('Lucy', 25), ('IT', 25), ('Anna', 25), ('Betty', 25), ('Liu', 25), ('DVD', 24), ('Nancy', 24), ('Rick', 24), ('Karen', 24), ('DateOfBirth', 24), ('Lisa', 23), ('Frank', 23), ('Linda', 23), ('Dan', 23), ('Julia', 22), ('Cindy', 22), ('Zhang', 22), ('Henry', 22), ('Rose', 21), ('Charles', 21), ('Joe', 21), ('Parker', 20), ('Veronica', 20), ('and', 19), ('Laura', 19), ('Lin', 18), ('Wilson', 18), ('Nick', 18), ('CIF', 18), ('ATM', 18), ('Richard', 17), ('China', 17), ('Ben', 17), ('Joanne', 17), ('Kathy', 17), ('NBA', 16), ('Jimmy', 16), ('Tony', 16), ('Julie', 16), ('Dave', 16), ('Street', 16), ('Maria', 16), ('Charlie', 16), ('Sue', 15)]\n",
      "\n",
      "DEV 상위 약어 100개:\n",
      "[('Mr', 23), ('TV', 10), ('Mrs', 8), ('John', 8), ('Li', 6), ('Miss', 6), ('Mary', 6), ('Bill', 5), ('Sara', 5), ('PhoneNumber', 5), ('Brown', 5), ('Wang', 5), ('David', 5), ('Green', 5), ('Sandals', 4), ('Rebecca', 4), ('Veronica', 4), ('Joe', 4), ('Bradley', 4), ('Karen', 3), ('Ms', 3), ('Dr', 3), ('and', 3), ('Tom', 3), ('Monica', 3), ('Jenny', 3), ('DateOfBirth', 3), ('Jerry', 3), ('Lizzy', 3), ('Peter', 3), ('DVD', 3), ('Jimmy', 2), ('Murphy', 2), ('Brandon', 2), ('Gary', 2), ('Ben', 2), ('Jayden', 2), ('Suzy', 2), ('ABC', 2), ('Address', 2), ('Peace', 2), ('Lucy', 2), ('Mike', 2), ('Street', 2), ('Vicky', 2), ('BA', 2), ('Rick', 2), ('Ames', 2), ('Helen', 2), ('Dave', 2), ('Kitty', 2), ('Bobby', 2), ('Sally', 2), ('Ghost', 2), ('Johnson', 2), ('Jennifer', 2), ('Barbara', 2), ('Herbert', 2), ('Stern', 2), ('Adam', 2), ('Alice', 2), ('Susan', 2), ('Anderson', 2), ('Parsons', 2), ('Lisa', 2), ('UFO', 1), ('White', 1), ('Sherry', 1), ('Gate', 1), ('Zhang', 1), ('Jackie', 1), ('Brad', 1), ('Qi', 1), ('Word', 1), ('Anne', 1), ('Caroline', 1), ('Sound', 1), ('Vision', 1), ('Eve', 1), ('Cruise', 1), ('Rainbow', 1), ('Heath', 1), ('LUX', 1), ('LIPTON', 1), ('WALLS', 1), ('Patrick', 1), ('Black', 1), ('Wangfujing', 1), ('Grand', 1), ('genova', 1), ('Kevin', 1), ('Laura', 1), ('Andrea', 1), ('LA', 1), ('Sabrina', 1), ('Jason', 1), ('Neal', 1), ('Middlesex', 1), ('Sarah', 1), ('Sunny', 1)]\n",
      "\n",
      "TEST 상위 약어 100개:\n",
      "[('Mr', 26), ('Ms', 9), ('TV', 9), ('Bill', 8), ('Andrew', 8), ('Tom', 6), ('Mike', 6), ('Mrs', 6), ('Jack', 6), ('Mary', 6), ('Steven', 5), ('Mark', 5), ('PhoneNumber', 5), ('Henry', 5), ('Brian', 4), ('Susan', 4), ('Simon', 4), ('Jane', 4), ('John', 4), ('Lin', 3), ('ABC', 3), ('Smith', 3), ('Ice', 3), ('Ann', 3), ('Robin', 3), ('Bob', 3), ('SIM', 3), ('Stanley', 3), ('Kayne', 3), ('Kate', 2), ('IBM', 2), ('Polly', 2), ('Maggie', 2), ('Burman', 2), ('Fang', 2), ('Lucy', 2), ('Thomas', 2), ('James', 2), ('Dick', 2), ('Adam', 2), ('CD', 2), ('ROM', 2), ('Brain', 2), ('Locker', 2), ('ATM', 2), ('Frank', 2), ('Movie', 2), ('Comp', 2), ('Lit', 2), ('Dan', 2), ('Ben', 2), ('George', 2), ('Nathan', 2), ('Panda', 2), ('Peter', 2), ('Vanilla', 2), ('KFC', 2), ('Anne', 2), ('Lee', 2), ('Martin', 2), ('Lamb', 2), ('Jones', 2), ('Jim', 2), ('SUI', 2), ('Tim', 2), ('Murphy', 2), ('IBA', 2), ('GS', 2), ('Pete', 2), ('Sally', 2), ('Miss', 2), ('Charlie', 2), ('Karl', 2), ('Ford', 2), ('Paula', 2), ('IC', 2), ('Steve', 2), ('Katie', 2), ('Dawson', 1), ('Carrefour', 1), ('Hero', 1), ('Francis', 1), ('Tony', 1), ('First', 1), ('Certificate', 1), ('of', 1), ('Proficiency', 1), ('Muriel', 1), ('Douglas', 1), ('Todd', 1), ('Cleo', 1), ('Turner', 1), ('Sals', 1), ('bury', 1), ('People', 1), ('You', 1), ('HP', 1), ('Lulu', 1), ('Nathaniel', 1), ('Que', 1)]\n",
      "\n",
      " [2] 지시표현 비율\n",
      "TRAIN 지시표현 포함 비율: 48.88%\n",
      "DEV 지시표현 포함 비율: 53.31%\n",
      "TEST 지시표현 포함 비율: 52.91%\n",
      "\n",
      " [3] 요약문 단어 개수 분석 (train/dev)\n",
      "\n",
      "TRAIN 요약 길이 통계:\n",
      "count: 12457\n",
      "min: 4\n",
      "max: 75\n",
      "mean: 16.48\n",
      "median: 15.0\n",
      "\n",
      "DEV 요약 길이 통계:\n",
      "count: 499\n",
      "min: 5\n",
      "max: 54\n",
      "mean: 15.35\n",
      "median: 14.0\n",
      "\n",
      " [4] 중복 요약 비율 (train/dev)\n",
      "TRAIN\n",
      "DEV\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd # pandas 라이브러리가 사용되므로 import 추가\n",
    "\n",
    "### 1. 약어 / 이모티콘 빈도 분석\n",
    "def extract_abbreviations(texts):\n",
    "    \"\"\"\n",
    "    주어진 텍스트 목록에서 약어 또는 이모티콘으로 추정되는 패턴을 추출하고 빈도를 분석합니다.\n",
    "    - 한글 자음/모음이 2개 이상 연속되거나 (ex: ㅋㅋ, ㅎㅎ, ㅠㅠ)\n",
    "    - 영문 알파벳이 2개 이상 연속되는 (ex: lol, wtf) 패턴을 찾습니다.\n",
    "\n",
    "    Args:\n",
    "        texts (list or pd.Series): 분석할 텍스트 문자열들의 목록 또는 pandas Series.\n",
    "\n",
    "    Returns:\n",
    "        Counter: 추출된 약어/이모티콘과 그 빈도를 담은 Counter 객체.\n",
    "    \"\"\"\n",
    "    # 정규 표현식 패턴 정의:\n",
    "    # r'\\b[ㄱ-ㅎㅏ-ㅣ]{2,}\\b' : 단어 경계(\\b) 내에서 한글 자음/모음이 2개 이상 반복 (예: ㅋㅋ, ㅠㅠ)\n",
    "    # r'\\b[a-zA-Z]{2,}\\b'   : 단어 경계(\\b) 내에서 영문 알파벳이 2개 이상 반복 (예: lol, omg)\n",
    "    # r'[ㄱ-ㅎ]{2,}'         : 단어 경계에 상관없이 한글 자음이 2개 이상 반복 (예: ㅋㅋㅋ, ㅎㅎㅎ - 단어 중간에도 매칭)\n",
    "    # 첫 번째와 세 번째 패턴은 한글 초성/중성을 주로 잡기 위함이고, 두 번째는 영문 약어를 위함입니다.\n",
    "    pattern = r'\\b[ㄱ-ㅎㅏ-ㅣ]{2,}\\b|\\b[a-zA-Z]{2,}\\b|[ㄱ-ㅎ]{2,}'\n",
    "    matches = [] # 추출된 약어/이모티콘을 저장할 리스트\n",
    "\n",
    "    for text in texts:\n",
    "        if pd.isna(text): # 텍스트가 NaN (결측값)인 경우 건너뛰기\n",
    "            continue\n",
    "        found = re.findall(pattern, text) # 현재 텍스트에서 패턴에 맞는 모든 문자열 찾기\n",
    "        matches.extend(found) # 찾은 문자열들을 matches 리스트에 추가\n",
    "    return Counter(matches) # matches 리스트의 각 항목 빈도를 계산하여 반환\n",
    "\n",
    "print(\"\\n [1] 약어/이모티콘 빈도 분석\")\n",
    "# 'datasets'는 외부에서 정의된 딕셔너리로 가정하며, \n",
    "# 각 키(name)는 데이터셋의 이름이고 값(df)은 pandas DataFrame입니다.\n",
    "for name, df in datasets.items():\n",
    "    count = extract_abbreviations(df['dialogue']) # 'dialogue' 열에서 약어/이모티콘 추출 및 빈도 계산\n",
    "    print(f\"\\n{name.upper()} 상위 약어 100개:\")\n",
    "    print(count.most_common(100)) # 가장 빈번한 상위 20개 출력\n",
    "\n",
    "\n",
    "### 2. 지시표현 비율\n",
    "# 지시표현으로 간주할 단어들의 목록\n",
    "deictic_words = ['그 사람', '이 사람', '그거', '이거', '그건', '이건', '거기', '저기', '여기']\n",
    "\n",
    "def count_deictic_ratio(texts):\n",
    "    \"\"\"\n",
    "    주어진 텍스트 목록에서 지시표현이 포함된 텍스트의 비율을 계산합니다.\n",
    "    'deictic_words' 리스트에 정의된 단어 중 하나라도 텍스트에 포함되어 있으면 지시표현이 있는 것으로 간주합니다.\n",
    "\n",
    "    Args:\n",
    "        texts (list or pd.Series): 분석할 텍스트 문자열들의 목록 또는 pandas Series.\n",
    "\n",
    "    Returns:\n",
    "        float: 지시표현을 포함하는 텍스트의 비율 (소수점 넷째 자리까지 반올림).\n",
    "               총 텍스트가 0개인 경우 0.0을 반환.\n",
    "    \"\"\"\n",
    "    total, cnt = 0, 0 # total: 총 텍스트 수, cnt: 지시표현을 포함하는 텍스트 수\n",
    "    for text in texts:\n",
    "        if pd.isna(text): # 텍스트가 NaN (결측값)인 경우 건너뛰기\n",
    "            continue\n",
    "        total += 1 # 유효한 텍스트이므로 총 텍스트 수 증가\n",
    "        # deictic_words 리스트의 어떤 단어라도 현재 텍스트에 포함되어 있는지 확인\n",
    "        if any(word in text for word in deictic_words):\n",
    "            cnt += 1 # 포함되어 있다면 지시표현 포함 텍스트 수 증가\n",
    "    \n",
    "    # 지시표현 포함 비율 계산 (총 텍스트 수가 0이 아니면 계산, 아니면 0.0 반환)\n",
    "    return round(cnt / total, 4) if total > 0 else 0.0\n",
    "\n",
    "print(\"\\n [2] 지시표현 비율\")\n",
    "for name, df in datasets.items():\n",
    "    ratio = count_deictic_ratio(df['dialogue']) # 'dialogue' 열에서 지시표현 포함 비율 계산\n",
    "    print(f\"{name.upper()} 지시표현 포함 비율: {ratio*100:.2f}%\") # 백분율로 출력\n",
    "\n",
    "\n",
    "### 3. 요약문 길이 분석 (train/dev만)\n",
    "def get_length_stats(series):\n",
    "    \"\"\"\n",
    "    주어진 텍스트 시리즈(주로 요약문)의 길이에 대한 통계(개수, 최소, 최대, 평균, 중앙값)를 계산합니다.\n",
    "    텍스트 길이는 공백으로 구분된 단어의 수로 정의됩니다.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): 길이를 분석할 문자열(요약문)이 담긴 pandas Series.\n",
    "\n",
    "    Returns:\n",
    "        dict: 'count', 'min', 'max', 'mean', 'median' 키를 가진 통계 결과 딕셔너리.\n",
    "    \"\"\"\n",
    "    # 결측값을 제거하고 문자열 타입으로 변환 후, 각 문자열을 공백으로 분리하여 단어 수(길이) 계산\n",
    "    lengths = series.dropna().astype(str).apply(lambda x: len(x.split()))\n",
    "    return {\n",
    "        'count': len(lengths), # 유효한 요약문의 개수\n",
    "        'min': lengths.min(),  # 최소 길이\n",
    "        'max': lengths.max(),  # 최대 길이\n",
    "        'mean': round(lengths.mean(), 2), # 평균 길이 (소수점 둘째 자리까지 반올림)\n",
    "        'median': lengths.median() # 중앙값 길이\n",
    "    }\n",
    "\n",
    "print(\"\\n [3] 요약문 단어 개수 분석 (train/dev)\")\n",
    "# 'train'과 'dev' 데이터셋에 대해서만 요약문 길이 분석 수행\n",
    "for name in ['train', 'dev']:\n",
    "    # datasets 딕셔너리에서 해당 데이터셋의 'summary' 열을 가져와 통계 계산\n",
    "    stats = get_length_stats(datasets[name]['summary'])\n",
    "    print(f\"\\n{name.upper()} 요약 길이 통계:\")\n",
    "    for k, v in stats.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "### 4. 중복 요약 확인 (train/dev만)\n",
    "print(\"\\n [4] 중복 요약 비율 (train/dev)\")\n",
    "# 'train'과 'dev' 데이터셋에 대해서만 중복 요약 확인 수행\n",
    "for name in ['train', 'dev']:\n",
    "    # 'summary' 열에서 결측값을 제거한 요약문들 가져오기\n",
    "    summaries = datasets[name]['summary'].dropna()\n",
    "    total = len(summaries) # 총 요약문의 개수\n",
    "    duplicated = summaries.duplicated().sum() # 중복되는 요약문의 개수 계산\n",
    "    \n",
    "    # 중복 비율 계산 (총 요약문이 0개가 아니면 계산, 아니면 0.0 반환)\n",
    "    ratio = duplicated / total if total > 0 else 0.0\n",
    "    print(f\"{name.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d295034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [1] 약어/이모티콘 빈도 분석\n",
      "\n",
      "TRAIN 상위 약어 100개:\n",
      "[('ㅎㅎ', 1)]\n",
      "\n",
      "DEV 상위 약어 100개:\n",
      "[]\n",
      "\n",
      "TEST 상위 약어 100개:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "### 1. 약어 / 이모티콘 빈도 분석\n",
    "def extract_abbreviations(texts):\n",
    "    \"\"\"\n",
    "    주어진 텍스트 목록에서 약어 또는 이모티콘으로 추정되는 패턴을 추출하고 빈도를 분석합니다.\n",
    "    - 한글 자음/모음이 2개 이상 연속되거나 (ex: ㅋㅋ, ㅎㅎ, ㅠㅠ)\n",
    "    - 영문 알파벳이 2개 이상 연속되는 (ex: lol, wtf) 패턴을 찾습니다.\n",
    "\n",
    "    Args:\n",
    "        texts (list or pd.Series): 분석할 텍스트 문자열들의 목록 또는 pandas Series.\n",
    "\n",
    "    Returns:\n",
    "        Counter: 추출된 약어/이모티콘과 그 빈도를 담은 Counter 객체.\n",
    "    \"\"\"\n",
    "    # 정규 표현식 패턴 정의:\n",
    "    # r'\\b[ㄱ-ㅎㅏ-ㅣ]{2,}\\b' : 단어 경계(\\b) 내에서 한글 자음/모음이 2개 이상 반복 (예: ㅋㅋ, ㅠㅠ)\n",
    "    # r'\\b[a-zA-Z]{2,}\\b'   : 단어 경계(\\b) 내에서 영문 알파벳이 2개 이상 반복 (예: lol, omg)\n",
    "    # r'[ㄱ-ㅎ]{2,}'         : 단어 경계에 상관없이 한글 자음이 2개 이상 반복 (예: ㅋㅋㅋ, ㅎㅎㅎ - 단어 중간에도 매칭)\n",
    "    # 첫 번째와 세 번째 패턴은 한글 초성/중성을 주로 잡기 위함이고, 두 번째는 영문 약어를 위함입니다.\n",
    "    pattern = r'[ㄱ-ㅎ]{2,}'\n",
    "    matches = [] # 추출된 약어/이모티콘을 저장할 리스트\n",
    "\n",
    "    for text in texts:\n",
    "        if pd.isna(text): # 텍스트가 NaN (결측값)인 경우 건너뛰기\n",
    "            continue\n",
    "        found = re.findall(pattern, text) # 현재 텍스트에서 패턴에 맞는 모든 문자열 찾기\n",
    "        matches.extend(found) # 찾은 문자열들을 matches 리스트에 추가\n",
    "    return Counter(matches) # matches 리스트의 각 항목 빈도를 계산하여 반환\n",
    "\n",
    "print(\"\\n [1] 약어/이모티콘 빈도 분석\")\n",
    "# 'datasets'는 외부에서 정의된 딕셔너리로 가정하며, \n",
    "# 각 키(name)는 데이터셋의 이름이고 값(df)은 pandas DataFrame입니다.\n",
    "for name, df in datasets.items():\n",
    "    count = extract_abbreviations(df['dialogue']) # 'dialogue' 열에서 약어/이모티콘 추출 및 빈도 계산\n",
    "    print(f\"\\n{name.upper()} 상위 약어 100개:\")\n",
    "    print(count.most_common(100)) # 가장 빈번한 상위 20개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5d3315e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: 요즘 잘 지내고 있어요?<br>#Person2#: 제 코치가 제 혈압을 체크해 달라고 부탁했어요.<br>#Person1#: 전에 고혈압 있다고 들은 적 있나요?<br>#Person2#: 고혈압 증상은 없어요.<br>#Person1#: 고혈압 있는 사람들은 대부분 본인이 모르는 경우가 많아요. 그래서 조용한 살인자라고 부르죠.<br>#Person2#: 고혈압 검사는 어떻게 해요?<br>#Person1#: 이 커프에 공기를 넣어서 측정할 거예요.<br>#Person2#: 측정 후에 어떤 정보를 알 수 있나요?<br>#Person1#: 심장이 얼마나 강하게 피를 펌프하는지와 동맥이 쉴 때 얼마나 이완되는지를 알려줘요.<br>#Person2#: 이번 검사가 잘 됐으면 좋겠어요.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(train_df[train_df['dialogue'].str.contains('<br>')]['dialogue'].values[0])\n",
    "print(val_df[val_df['dialogue'].str.contains('<br>')]['dialogue'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "175f9216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#과 #Person2#는 전 남자친구에 대해 이야기한 후, #Person1#이 #Person2#에게 사랑을 고백하고, #Person2#는 #Person1#의 여자친구가 되기로 합니다. 둘은 서로의 감정을 나누며 행복해합니다.\n"
     ]
    }
   ],
   "source": [
    "print(train_df[train_df['dialogue'].str.contains('ㅎㅎ')]['summary'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10f43108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n sdfld \\n dsfsdf\n"
     ]
    }
   ],
   "source": [
    "text = \"<br> sdfld <br> dsfsdf\"\n",
    "text = text.replace('<br>', '\\\\n')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344b9e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67644d99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
