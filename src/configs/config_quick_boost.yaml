# ⚡ Quick Boost - 기존 모델 최대 활용 (inference only)
# WandB 비활성화 + 에러 수정

general:
  data_path: ./data/
  model_name: digit82/kobart-summarization
  output_dir: ./outputs/exp_quick_boost_lyj
  train_data: train_japan_backtranslation_filtered.csv

inference:
  batch_size: 48
  ckt_dir: ./outputs/exp_optimized_lyj/best
  early_stopping: true
  generate_max_length: 230
  num_beams: 12
  length_penalty: 2.0
  repetition_penalty: 1.25
  no_repeat_ngram_size: 6
  remove_tokens:
  - <usr>
  - <s>
  - </s>
  - <pad>
  - <unk>
  result_path: ./outputs/exp_quick_boost_lyj/submission_quick.csv

tokenizer:
  bos_token: <s>
  decoder_max_len: 230
  encoder_max_len: 768
  eos_token: </s>
  special_tokens:
  - '#Person1#'
  - '#Person2#'
  - '#Person3#'
  - '#Person4#'
  - '#Person5#'
  - '#Person6#'
  - '#Person7#'
  - '#PhoneNumber#'
  - '#Address#'
  - '#DateOfBirth#'
  - '#PassportNumber#'
  - '#SSN#'
  - '#CardNumber#'
  - '#CarNumber#'
  - '#Email#'
  - '#Topic#'
  - '#SEP#'

# Inference Only 모드 (training 건너뛰기)
training:
  num_train_epochs: 0
  per_device_train_batch_size: 1
  seed: 42
  report_to: []  # WandB 비활성화