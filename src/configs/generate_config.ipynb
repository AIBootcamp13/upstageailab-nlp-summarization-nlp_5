{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "642f7ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py311/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BartForConditionalGeneration, BartConfig, AutoModelForSeq2SeqLM, AutoConfig\n",
    "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "# model_name = \"digit82/kobart-summarization\"\n",
    "\n",
    "# bart_config = BartConfig().from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, legacy=False)\n",
    "# model = BartForConditionalGeneration.from_pretrained(model_name, config=bart_config)\n",
    "# config = AutoConfig.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ac036c",
   "metadata": {},
   "source": [
    "- ⚠️ config의 inference/remove_tokens에는 Special_token을 추가하지 전에 토크나이저가 기본적으로 갖고 있는 special tokens를 설정해줘야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0dd15b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</s>', '<unk>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    remove_tokens = list(tokenizer.special_tokens_map.values())\n",
    "except:\n",
    "    remove_tokens = []\n",
    "print(remove_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7941ed",
   "metadata": {},
   "source": [
    "- ⚠️ 만약 bos_token이 None이라면 토크나이저에 맞는 처리를 해야 한다.\n",
    "    - T5 계열 모델의 토크나이저는 bos_token을 사용하지 않는다. 대신 해당 자리에 pad_token을 삽입한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a28bf67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    BOS : None,\n",
      "    EOS : </s>,\n",
      "    PAD : <pad>\n",
      "    Special_tokens : {'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'},\n",
      "    Tokenizer's max_model_input_sizes : 1000000000000000019884624838656\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    BART Embedding Layer: {model.get_encoder().embed_tokens}, {model.get_encoder().embed_positions}\n",
    "    tokenizer의 encoder_max_len은 {model.get_encoder().embed_positions.num_embeddings} 이하여야 한다.\n",
    "\n",
    "    BART Decoder Layer: {model.get_decoder().embed_tokens}, {model.get_decoder().embed_positions}\n",
    "    tokenizer의 decoder_max_len은 {model.get_decoder().embed_positions.num_embeddings} 이하여야 한다.\n",
    "'''\n",
    "\n",
    "# from pprint import pprint\n",
    "print(f'''\n",
    "    BOS : {tokenizer.bos_token_id},\n",
    "    EOS : {tokenizer.eos_token},\n",
    "    PAD : {tokenizer.pad_token}\n",
    "    Special_tokens : {tokenizer.special_tokens_map},\n",
    "    Tokenizer's max_model_input_sizes : {tokenizer.model_max_length}\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1dfda04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5 모델 계열은 아래 코드를 실행\n",
    "# tokenizer.bos_token = tokenizer.pad_token\n",
    "\n",
    "\n",
    "# print(tokenizer.bos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4a9cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 구성 정보를 YAML 파일로 저장합니다.\n",
    "project_dir = \"/data/ephemeral/home/nlp-5/auto1p/\"\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\n",
    "    project_dir\n",
    ")\n",
    "from src.utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "514c6d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08020426\n"
     ]
    }
   ],
   "source": [
    "current_time = get_current_time()\n",
    "concept = \"base\" # 실험 컨셉을 작성. output 디렉토리 이름 및 config 파일 이름 설정 시 반영됨.\n",
    "output_dir = f\"./outputs/exp_{concept}_{current_time}\"\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85f6ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_eval_log_steps = 200 # save_step은 eval, log step의 배수여야 한다. 같이 맞춰주는 것이 편하다.\n",
    "train_batch_size = 8\n",
    "inference_batch_size = 8 # eval batch size 도 동일하게 설정됨\n",
    "encoder_max_len = 1500\n",
    "decoder_max_len = 170 # inference max length 도 동일하게 설정됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13d51eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = {\n",
    "    \"general\": {\n",
    "        \"data_path\": \"./data/\", # 모델 생성에 필요한 데이터 경로를 사용자 환경에 맞게 지정합니다.\n",
    "        \"model_name\": model_name, # 불러올 모델의 이름을 사용자 환경에 맞게 지정할 수 있습니다.\n",
    "        \"output_dir\": output_dir, # 모델의 최종 출력 값을 저장할 경로를 설정합니다.\n",
    "        \"train_data\": \"train.csv\", # 학습 데이터 파일을 설정한다. data_path 폴더 안에 존재해야 한다.\n",
    "        \"val_data\": \"dev.csv\",\n",
    "        \"test_data\": \"test.csv\", # 추론 데이터 파일을 설정한다. 추론 데이터에 topic이나 ner을 추가할 경우에 대비.\n",
    "        \"eval_tokenizer\": \"upstage/solar-pro2-tokenizer\",\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"encoder_max_len\": encoder_max_len,\n",
    "        \"decoder_max_len\": decoder_max_len,\n",
    "        \"bos_token\": f\"{tokenizer.bos_token}\",\n",
    "        \"eos_token\": f\"{tokenizer.eos_token}\",\n",
    "        # 특정 단어들이 분해되어 tokenization이 수행되지 않도록 special_tokens을 지정해줍니다.\n",
    "        \"special_tokens\": [ '#Person1#', '#Person2#', '#Person3#', '#Person4#',\n",
    "            '#Person5#', '#Person6#', '#Person7#',\n",
    "            '#PhoneNumber#', '#Address#', '#DateOfBirth#','#PassportNumber#','#SSN#','#CardNumber#','#CarNumber#','#Email#',\n",
    "            # '#Topic#','#Dialogue#','#SEP#',\n",
    "        ]\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"seed\": 42,\n",
    "        \"output_dir\": output_dir,\n",
    "        \"overwrite_output_dir\": False,\n",
    "\n",
    "        \"save_total_limit\": 1,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"save_steps\": save_eval_log_steps,\n",
    "\n",
    "        \"logging_dir\": output_dir,\n",
    "        \"logging_steps\": save_eval_log_steps,\n",
    "\n",
    "        \"num_train_epochs\": 5,\n",
    "        \"per_device_train_batch_size\": train_batch_size,\n",
    "        \"remove_unused_columns\": True,\n",
    "        \"fp16\": False, # float16 사용 메모리 절약, But 정밀도 문제 존재 > 구형 GPU에서 사용\n",
    "        \"bf16\": True, # float16 사용 메모리 절약, 정밀도 문제 개선 > 30/40,... 등 최신 GPU 가능\n",
    "        \"dataloader_drop_last\": False,\n",
    "        \"group_by_length\": True,\n",
    "        \n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_checkpointing_kwargs\": {\"use_reentrant\": False},\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"torch_empty_cache_steps\": 1,\n",
    "        \"dataloader_num_workers\": 8,\n",
    "\n",
    "        \"per_device_eval_batch_size\": inference_batch_size,\n",
    "        \"evaluation_strategy\": 'steps',\n",
    "        \"eval_steps\": save_eval_log_steps,\n",
    "        \n",
    "        \"predict_with_generate\": True,\n",
    "        \"generation_max_length\": decoder_max_len,\n",
    "        \n",
    "        # Callbacks\n",
    "        \"early_stopping_patience\": 2,\n",
    "        \"early_stopping_threshold\": 0.001,\n",
    "\n",
    "        # Optimizer\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"warmup_steps\": 10,\n",
    "        \"weight_decay\": 1e-3,\n",
    "        \"num_cycles\": 1,\n",
    "\n",
    "        \"report_to\": \"none\", # 학습 과정을 어느 백엔드에 저장할 것인지. \"none\" 설정 시 wandb 미사용. \"wandb\" 설정 시 사용.\n",
    "\n",
    "        # PEFT\n",
    "        \"LoRA\": False,\n",
    "        \"QLoRA\": False,\n",
    "    },\n",
    "    # (선택) wandb 홈페이지에 가입하여 얻은 정보를 기반으로 작성합니다.\n",
    "    \"wandb\": {\n",
    "        \"entity\": \"skiersong\", # 팀 실험 시 organization 이름\n",
    "        \"project\": \"nlp-5-upgrade\",\n",
    "        \"name\": f\"a_base_{current_time}\", # 개별 실험 이름\n",
    "        # \"group\": \"\", # 유사한 실험들은 같은 그룹으로 설정\n",
    "        \"notes\": \"AutoModel baseline\", # 실험에 대한 추가 설명\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"ckt_dir\": os.path.join(output_dir, 'best'), # 파인튜닝이 진행된 모델의 checkpoint를 저장할 경로를 설정합니다.\n",
    "        \"result_path\": os.path.join(output_dir, f\"submission_{current_time}.csv\"), # 제출할 csv 파일 저장 경로\n",
    "        \"no_repeat_ngram_size\": 2, # \n",
    "        \"early_stopping\": True,\n",
    "        \"generate_max_length\": decoder_max_len,\n",
    "        \"num_beams\": 4,\n",
    "        \"batch_size\" : inference_batch_size,\n",
    "        # 정확한 모델 평가를 위해 제거할 불필요한 생성 토큰들을 정의합니다.\n",
    "        \"remove_tokens\": remove_tokens\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "616c2f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'general': {'data_path': './data/',\n",
      "             'eval_tokenizer': 'upstage/solar-pro2-tokenizer',\n",
      "             'model_name': 'digit82/kobart-summarization',\n",
      "             'output_dir': './outputs/exp_base_08020426',\n",
      "             'test_data': 'test.csv',\n",
      "             'train_data': 'train.csv',\n",
      "             'val_data': 'dev.csv'},\n",
      " 'inference': {'batch_size': 8,\n",
      "               'ckt_dir': './outputs/exp_base_08020426/best',\n",
      "               'early_stopping': True,\n",
      "               'generate_max_length': 170,\n",
      "               'no_repeat_ngram_size': 2,\n",
      "               'num_beams': 4,\n",
      "               'remove_tokens': ['</s>', '<unk>', '<pad>'],\n",
      "               'result_path': './outputs/exp_base_08020426/submission_08020426.csv'},\n",
      " 'tokenizer': {'bos_token': 'None',\n",
      "               'decoder_max_len': 170,\n",
      "               'encoder_max_len': 1500,\n",
      "               'eos_token': '</s>',\n",
      "               'special_tokens': ['#Person1#',\n",
      "                                  '#Person2#',\n",
      "                                  '#Person3#',\n",
      "                                  '#Person4#',\n",
      "                                  '#Person5#',\n",
      "                                  '#Person6#',\n",
      "                                  '#Person7#',\n",
      "                                  '#PhoneNumber#',\n",
      "                                  '#Address#',\n",
      "                                  '#DateOfBirth#',\n",
      "                                  '#PassportNumber#',\n",
      "                                  '#SSN#',\n",
      "                                  '#CardNumber#',\n",
      "                                  '#CarNumber#',\n",
      "                                  '#Email#']},\n",
      " 'training': {'LoRA': False,\n",
      "              'QLoRA': False,\n",
      "              'bf16': True,\n",
      "              'dataloader_drop_last': False,\n",
      "              'dataloader_num_workers': 8,\n",
      "              'early_stopping_patience': 2,\n",
      "              'early_stopping_threshold': 0.001,\n",
      "              'eval_steps': 200,\n",
      "              'evaluation_strategy': 'steps',\n",
      "              'fp16': False,\n",
      "              'generation_max_length': 170,\n",
      "              'gradient_accumulation_steps': 1,\n",
      "              'gradient_checkpointing': True,\n",
      "              'gradient_checkpointing_kwargs': {'use_reentrant': False},\n",
      "              'group_by_length': True,\n",
      "              'learning_rate': 1e-05,\n",
      "              'load_best_model_at_end': True,\n",
      "              'logging_dir': './outputs/exp_base_08020426',\n",
      "              'logging_steps': 200,\n",
      "              'num_cycles': 1,\n",
      "              'num_train_epochs': 5,\n",
      "              'output_dir': './outputs/exp_base_08020426',\n",
      "              'overwrite_output_dir': False,\n",
      "              'per_device_eval_batch_size': 8,\n",
      "              'per_device_train_batch_size': 8,\n",
      "              'predict_with_generate': True,\n",
      "              'remove_unused_columns': True,\n",
      "              'report_to': 'none',\n",
      "              'save_steps': 200,\n",
      "              'save_total_limit': 1,\n",
      "              'seed': 42,\n",
      "              'torch_empty_cache_steps': 1,\n",
      "              'warmup_steps': 10,\n",
      "              'weight_decay': 0.001},\n",
      " 'wandb': {'entity': 'skiersong',\n",
      "           'name': 'a_base_08020426',\n",
      "           'notes': 'AutoModel baseline',\n",
      "           'project': 'nlp-5-upgrade'}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "config_path = os.path.join(\n",
    "    project_dir,'src','configs',\n",
    "    f\"config_exps_{current_time}.yaml\" # config 파일 이름을 설정\n",
    ")\n",
    "with open(config_path, \"w\") as file:\n",
    "    yaml.dump(config_data, file, allow_unicode=True)\n",
    "\n",
    "with open(config_path, \"r\") as file:\n",
    "    loaded_config = yaml.safe_load(file)\n",
    "\n",
    "# 불러온 config 파일의 전체 내용을 확인합니다.\n",
    "pprint(loaded_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27711c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
