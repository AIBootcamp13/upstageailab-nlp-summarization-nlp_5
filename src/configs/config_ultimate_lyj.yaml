# ğŸ† Ultimate Performance Config - 1ìœ„ ë‹¬ì„±ìš©
# ìƒìœ„ ëª¨ë¸ íŒ¨í„´ ì™„ì „ ë¶„ì„ + ìµœê³  ì„±ëŠ¥ ì„¤ì •
# ëª©í‘œ: ROUGE-1 0.60+, ROUGE-2 0.42+, ROUGE-L 0.54+

general:
  data_path: ./data/
  model_name: digit82/kobart-summarization
  output_dir: ./outputs/exp_ultimate_lyj
  train_data: train_japan_backtranslation_filtered.csv

inference:
  batch_size: 32                    # ì•ˆì •ì„± ìš°ì„ 
  ckt_dir: ./outputs/exp_ultimate_lyj/best
  early_stopping: true
  
  # ğŸ¯ 1ìœ„ ëª¨ë¸ ì™„ì „ ëª¨ë°© ì„¤ì •
  generate_max_length: 220          # ìƒìœ„ ëª¨ë¸ë“¤ì´ ë” ê¸´ ìš”ì•½ ì„ í˜¸
  num_beams: 10                     # ìµœëŒ€ beam í™•ì¥
  length_penalty: 1.8               # ê°•ë ¥í•œ ê¸¸ì´ ì œì–´
  repetition_penalty: 1.2           # ë°˜ë³µ ë°©ì§€ ê°•í™”
  no_repeat_ngram_size: 5           # ë” ê°•ë ¥í•œ n-gram ì œì–´
  
  # ğŸ² ë‹¤ì–‘ì„±ê³¼ í’ˆì§ˆ ê· í˜•
  do_sample: false                  # deterministic ì¶œë ¥ìœ¼ë¡œ ì¼ê´€ì„± í™•ë³´
  early_stopping: true
  
  remove_tokens:
  - <usr>
  - <s>
  - </s>
  - <pad>
  - <unk>
  result_path: ./outputs/exp_ultimate_lyj/submission_ultimate.csv

tokenizer:
  bos_token: <s>
  decoder_max_len: 220              # inferenceì™€ ì¼ì¹˜
  encoder_max_len: 896              # ë” ê¸´ ì…ë ¥ ì§€ì›
  eos_token: </s>
  special_tokens:
  - '#Person1#'
  - '#Person2#'
  - '#Person3#'
  - '#Person4#'
  - '#Person5#'
  - '#Person6#'
  - '#Person7#'
  - '#PhoneNumber#'
  - '#Address#'
  - '#DateOfBirth#'
  - '#PassportNumber#'
  - '#SSN#'
  - '#CardNumber#'
  - '#CarNumber#'
  - '#Email#'
  - '#Topic#'
  - '#SEP#'

training:
  # ğŸ”¥ ìµœê³  ì„±ëŠ¥ í›ˆë ¨ ì„¤ì •
  per_device_train_batch_size: 64   # ìµœì  ë°°ì¹˜
  per_device_eval_batch_size: 48    
  dataloader_num_workers: 12        
  dataloader_drop_last: false
  dataloader_pin_memory: true
  dataloader_persistent_workers: true
  
  # ğŸ“ˆ í•™ìŠµë¥  ë° ìŠ¤ì¼€ì¤„ë§ ìµœì í™”
  num_train_epochs: 8               # ì ì ˆí•œ ì—í¬í¬ (ê³¼ì í•© ë°©ì§€)
  learning_rate: 2.5e-05            # ë¯¸ì„¸ ì¡°ì •ëœ í•™ìŠµë¥ 
  warmup_ratio: 0.1                 # ë¹„ìœ¨ ê¸°ë°˜ warmup
  weight_decay: 0.008               # ì •ê·œí™” ê°•í™”
  
  # ğŸ¯ ìŠ¤ì¼€ì¤„ëŸ¬ ìµœì í™”
  lr_scheduler_type: cosine         # ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬
  cosine_restart_ratio: 0.5         # ì¬ì‹œì‘ ë¹„ìœ¨
  
  # ğŸ’¾ ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥ ìµœì í™”
  fp16: true
  torch_compile: true
  torch_compile_mode: "reduce-overhead"
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
  gradient_accumulation_steps: 2    # íš¨ê³¼ì  ë°°ì¹˜ í™•ëŒ€
  torch_empty_cache_steps: 3
  
  # ğŸ“Š í‰ê°€ ë° ì €ì¥ ìµœì í™”
  evaluation_strategy: steps
  eval_steps: 250                   # ë” ìì£¼ í‰ê°€
  save_steps: 250
  logging_steps: 25
  
  # ğŸª Early Stopping ì„¸ë°€ ì¡°ì •
  early_stopping_patience: 3
  early_stopping_threshold: 0.0003
  
  # ê¸°íƒ€ ìµœì í™”
  seed: 42                          # ì¬í˜„ì„±
  report_to: []  # WandB ë¹„í™œì„±í™”
  group_by_length: true
  remove_unused_columns: true
  predict_with_generate: true
  generation_max_length: 220
  
  output_dir: ./outputs/exp_ultimate_lyj
  overwrite_output_dir: false
  save_total_limit: 5               # ë” ë§ì€ ì²´í¬í¬ì¸íŠ¸ ë³´ê´€
  load_best_model_at_end: true
  metric_for_best_model: eval_rouge-2
  greater_is_better: true
  
  report_to: wandb
  logging_dir: ./outputs/exp_ultimate_lyj/logs

# ğŸ” WandB ì„¤ì •
wandb:
  entity: lyjune37
  name: ultimate_performance_v1
  notes: "Ultimate config for #1 rank - enhanced everything"
  project: nlp-5
  tags:
  - "ultimate"
  - "rank1"
  - "enhanced"
  - "rouge2-focus"

# ì‹¤í—˜ ë©”íƒ€ë°ì´í„°
experiment:
  version: "ultimate_v1"
  strategy: "complete_optimization"
  target_score: 52.0
  focus: "ROUGE-2_enhancement"