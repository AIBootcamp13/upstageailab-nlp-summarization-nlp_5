import os
import re
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
import pandas as pd

import sys
sys.path.append(
    "/data/ephemeral/home/nlp-5/lyj"
)
from dataset.dataset_base import *
from dataset.preprocess import *
from models.BART import *

# tokenization ê³¼ì •ê¹Œì§€ ì§„í–‰ëœ ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì— ì…ë ¥ë  ë°ì´í„°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
def prepare_test_dataset(config, preprocessor, tokenizer):

    test_file_path = os.path.join(config['general']['data_path'],'test.csv')

    test_data = preprocessor.make_set_as_df(test_file_path,is_train=False)
    test_id = test_data['fname']

    print('-'*150)
    print(f'test_data:\n{test_data["dialogue"][0]}')
    print('-'*150)

    encoder_input_test , decoder_input_test = preprocessor.make_input(test_data,is_test=True)
    print('-'*10, 'Load data complete', '-'*10,)

    test_tokenized_encoder_inputs = tokenizer(encoder_input_test, return_tensors="pt", padding=True,
                    add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False,)
    test_tokenized_decoder_inputs = tokenizer(decoder_input_test, return_tensors="pt", padding=True,
                    add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False,)

    test_encoder_inputs_dataset = DatasetForInference(test_tokenized_encoder_inputs, test_id, len(encoder_input_test))
    print('-'*10, 'Make dataset complete', '-'*10,)

    return test_data, test_encoder_inputs_dataset


def enhance_summary(summary):
    """ìš”ì•½ë¬¸ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ í›„ì²˜ë¦¬"""
    # 1. ë¬¸ì¥ ì™„ê²°ì„± ì²´í¬
    if not summary.strip().endswith('.'):
        summary += '.'
    
    # 2. Person íƒœê·¸ ì •ê·œí™”
    summary = re.sub(r'#Person(\d+)#', r'#Person\1#', summary)
    
    # 3. ì—°ì†ëœ ê³µë°± ì œê±°
    summary = re.sub(r'\s+', ' ', summary)
    
    # 4. ë¬¸ì¥ ì‹œì‘/ë ê³µë°± ì œê±°
    summary = summary.strip()
    
    # 5. ì¤‘ë³µ ë¬¸êµ¬ ì œê±° (ê°„ë‹¨í•œ ê²½ìš°ë§Œ)
    sentences = summary.split('.')
    if len(sentences) > 1:
        unique_sentences = []
        for sent in sentences:
            sent = sent.strip()
            if sent and sent not in unique_sentences:
                unique_sentences.append(sent)
        if unique_sentences:
            summary = '. '.join(unique_sentences)
            if not summary.endswith('.'):
                summary += '.'
    
    return summary


# í•™ìŠµëœ ëª¨ë¸ì´ ìƒì„±í•œ ìš”ì•½ë¬¸ì˜ ì¶œë ¥ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
def inference(config, generate_model, tokenizer):
    device = torch.device('cuda:0' if torch.cuda.is_available()  else 'cpu')
    print('-'*10, f'device : {device}', '-'*10,)
    print(torch.__version__)

    # generate_model , tokenizer = load_tokenizer_and_model_for_test(config,device)

    data_path = config['general']['data_path']
    preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])

    test_data, test_encoder_inputs_dataset = prepare_test_dataset(config,preprocessor, tokenizer)
    dataloader = DataLoader(test_encoder_inputs_dataset, batch_size=config['inference']['batch_size'])

    summary = []
    text_ids = []
    
    # ğŸš€ ìµœì í™”ëœ generation íŒŒë¼ë¯¸í„° ì¤€ë¹„
    generation_kwargs = {
        'no_repeat_ngram_size': config['inference']['no_repeat_ngram_size'],
        'early_stopping': config['inference']['early_stopping'],
        'max_length': config['inference']['generate_max_length'],
        'num_beams': config['inference']['num_beams'],
    }
    
    # ì¶”ê°€ íŒŒë¼ë¯¸í„°ë“¤ (configì— ìˆìœ¼ë©´ ì‚¬ìš©)
    if 'length_penalty' in config['inference']:
        generation_kwargs['length_penalty'] = config['inference']['length_penalty']
    if 'repetition_penalty' in config['inference']:
        generation_kwargs['repetition_penalty'] = config['inference']['repetition_penalty']
    if 'do_sample' in config['inference'] and config['inference']['do_sample']:
        generation_kwargs['do_sample'] = True
        if 'temperature' in config['inference']:
            generation_kwargs['temperature'] = config['inference']['temperature']
        if 'top_k' in config['inference']:
            generation_kwargs['top_k'] = config['inference']['top_k']
        if 'top_p' in config['inference']:
            generation_kwargs['top_p'] = config['inference']['top_p']
    
    print("Generation parameters:", generation_kwargs)
    
    with torch.no_grad():
        for item in tqdm(dataloader):
            text_ids.extend(item['ID'])
            generated_ids = generate_model.generate(
                input_ids=item['input_ids'].to(device),
                **generation_kwargs
            )
            for ids in generated_ids:
                result = tokenizer.decode(ids)
                summary.append(result)

    # ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•˜ì—¬ ë…¸ì´ì¦ˆì— í•´ë‹¹ë˜ëŠ” ìŠ¤í˜ì…œ í† í°ì„ ì œê±°í•©ë‹ˆë‹¤.
    remove_tokens = config['inference']['remove_tokens']
    preprocessed_summary = summary.copy()
    for token in remove_tokens:
        preprocessed_summary = [sentence.replace(token," ") for sentence in preprocessed_summary]

    # ğŸ¯ ìš”ì•½ë¬¸ í’ˆì§ˆ í–¥ìƒ ì ìš©
    enhanced_summary = [enhance_summary(s) for s in preprocessed_summary]
    
    output = pd.DataFrame(
        {
            "fname": test_data['fname'],
            "summary" : enhanced_summary,
        }
    )
    result_path = config['inference']['result_path'] # submission íŒŒì¼ ê²½ë¡œ
    if not os.path.exists(result_path):
        os.makedirs(result_path, exist_ok=True)
    output.to_csv(os.path.join(result_path, 'result1'), index=False)

    print(f"âœ… Inference complete! Results saved to {result_path}/result1")
    print(f"ğŸ“Š Generated {len(enhanced_summary)} summaries")
    
    return output
