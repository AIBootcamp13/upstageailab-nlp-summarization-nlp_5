#!/usr/bin/env python3
"""
ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì¡°ì¥ë‹˜ì˜ ìµœì‹  ê¸°ìˆ  ìŠ¤íƒ í†µí•© í›„ ì„±ëŠ¥ ê°œì„  íš¨ê³¼ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •í•©ë‹ˆë‹¤.
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ (ê¸°ì¡´ vs QLoRA)
- í•™ìŠµ ì†ë„ ë²¤ì¹˜ë§ˆí¬
- ëª¨ë¸ ë¡œë”© ì‹œê°„ ì¸¡ì •
- GPU/MPS í™œìš©ë¥  ë¶„ì„
"""

import sys
import os
import time
import torch
import psutil
import yaml
import tracemalloc
from pathlib import Path
from typing import Dict, Any, List, Tuple
import logging
from datetime import datetime

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent / 'code'))

from trainer import DialogueSummarizationTrainer
from core.inference import InferenceEngine
from utils.config_manager import ConfigManager
from utils.memory_monitor import MemoryMonitor


class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ê¸°"""
    
    def __init__(self, config_path: str = "config.yaml"):
        self.config_path = config_path
        self.results = {}
        self.logger = self._setup_logger()
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„° ì´ˆê¸°í™”
        self.memory_monitor = MemoryMonitor()
        
        # ì„¤ì • ë¡œë“œ
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
    
    def _get_memory_usage(self) -> Dict[str, float]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        memory = psutil.virtual_memory()
        result = {
            'cpu': memory.used,
            'cpu_percent': memory.percent
        }
        
        if torch.cuda.is_available():
            result['gpu'] = torch.cuda.memory_allocated(0)
            result['gpu_reserved'] = torch.cuda.memory_reserved(0)
        elif torch.backends.mps.is_available():
            # MPSëŠ” ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ê³µìœ 
            result['gpu'] = 0
            result['gpu_reserved'] = 0
        
        return result
    
    def _setup_logger(self):
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger('PerformanceBenchmark')
        logger.setLevel(logging.INFO)
        
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def benchmark_environment(self) -> Dict[str, Any]:
        """í™˜ê²½ ì •ë³´ ë²¤ì¹˜ë§ˆí¬"""
        self.logger.info("=== í™˜ê²½ ì •ë³´ ë²¤ì¹˜ë§ˆí¬ ===")
        
        env_info = {
            'timestamp': datetime.now().isoformat(),
            'python_version': sys.version,
            'torch_version': torch.__version__,
            'device_info': {},
            'memory_info': {},
            'cpu_info': {}
        }
        
        # ë””ë°”ì´ìŠ¤ ì •ë³´
        if torch.cuda.is_available():
            env_info['device_info'] = {
                'type': 'CUDA',
                'device_count': torch.cuda.device_count(),
                'device_name': torch.cuda.get_device_name(0),
                'memory_total': torch.cuda.get_device_properties(0).total_memory / 1024**3
            }
        elif torch.backends.mps.is_available():
            env_info['device_info'] = {
                'type': 'MPS',
                'device_count': 1,
                'device_name': 'Apple Silicon',
                'memory_shared': True
            }
        else:
            env_info['device_info'] = {
                'type': 'CPU',
                'device_count': 1
            }
        
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´
        memory = psutil.virtual_memory()
        env_info['memory_info'] = {
            'total_gb': memory.total / 1024**3,
            'available_gb': memory.available / 1024**3,
            'percent_used': memory.percent
        }
        
        # CPU ì •ë³´
        env_info['cpu_info'] = {
            'physical_cores': psutil.cpu_count(logical=False),
            'logical_cores': psutil.cpu_count(logical=True),
            'frequency_mhz': psutil.cpu_freq().current if psutil.cpu_freq() else 'Unknown'
        }
        
        self.logger.info(f"ë””ë°”ì´ìŠ¤: {env_info['device_info']['type']}")
        self.logger.info(f"ë©”ëª¨ë¦¬: {env_info['memory_info']['total_gb']:.1f}GB ì´ìš©ëŸ‰")
        
        return env_info
    
    def benchmark_model_loading(self) -> Dict[str, Any]:
        """ëª¨ë¸ ë¡œë”© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        self.logger.info("=== ëª¨ë¸ ë¡œë”© ë²¤ì¹˜ë§ˆí¬ ===")
        
        results = {
            'standard_loading': {},
            'qlora_loading': {},
            'memory_comparison': {}
        }
        
        # 1. í‘œì¤€ ëª¨ë¸ ë¡œë”© (QLoRA ë¹„í™œì„±í™”)
        self.logger.info("1. í‘œì¤€ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸...")
        
        # ì„ì‹œë¡œ QLoRA ë¹„í™œì„±í™”
        original_qlora = self.config['qlora']['use_qlora']
        self.config['qlora']['use_qlora'] = False
        
        tracemalloc.start()
        start_time = time.time()
        memory_before = self._get_memory_usage()
        
        try:
            # trainer ì´ˆê¸°í™” (ì‹¤ì œ ëª¨ë¸ ë¡œë”©)
            trainer = DialogueSummarizationTrainer(self.config)
            
            load_time = time.time() - start_time
            memory_after = self._get_memory_usage()
            current, peak = tracemalloc.get_traced_memory()
            
            results['standard_loading'] = {
                'load_time_seconds': load_time,
                'memory_before_mb': memory_before['cpu'] / 1024**2,
                'memory_after_mb': memory_after['cpu'] / 1024**2,
                'memory_increase_mb': (memory_after['cpu'] - memory_before['cpu']) / 1024**2,
                'peak_memory_mb': peak / 1024**2,
                'success': True
            }
            
            self.logger.info(f"í‘œì¤€ ë¡œë”©: {load_time:.2f}ì´ˆ, ë©”ëª¨ë¦¬ ì¦ê°€: {results['standard_loading']['memory_increase_mb']:.1f}MB")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del trainer
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
        except Exception as e:
            self.logger.error(f"í‘œì¤€ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            results['standard_loading'] = {'success': False, 'error': str(e)}
        
        tracemalloc.stop()
        
        # 2. QLoRA ëª¨ë¸ ë¡œë”©
        self.logger.info("2. QLoRA ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸...")
        
        # QLoRA ì¬í™œì„±í™”
        self.config['qlora']['use_qlora'] = original_qlora
        
        tracemalloc.start()
        start_time = time.time()
        memory_before = self._get_memory_usage()
        
        try:
            trainer = DialogueSummarizationTrainer(self.config)
            
            load_time = time.time() - start_time
            memory_after = self._get_memory_usage()
            current, peak = tracemalloc.get_traced_memory()
            
            results['qlora_loading'] = {
                'load_time_seconds': load_time,
                'memory_before_mb': memory_before['cpu'] / 1024**2,
                'memory_after_mb': memory_after['cpu'] / 1024**2,
                'memory_increase_mb': (memory_after['cpu'] - memory_before['cpu']) / 1024**2,
                'peak_memory_mb': peak / 1024**2,
                'success': True
            }
            
            self.logger.info(f"QLoRA ë¡œë”©: {load_time:.2f}ì´ˆ, ë©”ëª¨ë¦¬ ì¦ê°€: {results['qlora_loading']['memory_increase_mb']:.1f}MB")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del trainer
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
        except Exception as e:
            self.logger.error(f"QLoRA ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            results['qlora_loading'] = {'success': False, 'error': str(e)}
        
        tracemalloc.stop()
        
        # 3. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë¹„êµ
        if results['standard_loading'].get('success') and results['qlora_loading'].get('success'):
            standard_mem = results['standard_loading']['memory_increase_mb']
            qlora_mem = results['qlora_loading']['memory_increase_mb']
            
            memory_saving = ((standard_mem - qlora_mem) / standard_mem) * 100 if standard_mem > 0 else 0
            
            results['memory_comparison'] = {
                'standard_memory_mb': standard_mem,
                'qlora_memory_mb': qlora_mem,
                'memory_saving_percent': memory_saving,
                'memory_saving_mb': standard_mem - qlora_mem
            }
            
            self.logger.info(f"ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saving:.1f}% ({standard_mem - qlora_mem:.1f}MB ì ˆì•½)")
        
        return results
    
    def benchmark_inference_speed(self) -> Dict[str, Any]:
        """ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
        self.logger.info("=== ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬ ===")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_dialogues = [
            "ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì°¸ ì¢‹ë„¤ìš”. ì‚°ì±…í•˜ê¸° ë”± ì¢‹ì€ ë‚ ì”¨ì…ë‹ˆë‹¤.",
            "íšŒì˜ ì‹œê°„ì„ ì¡°ì •í•´ì•¼ í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì£¼ í™”ìš”ì¼ ì˜¤í›„ 2ì‹œëŠ” ì–´ë– ì„¸ìš”?",
            "í”„ë¡œì íŠ¸ ì§„í–‰ ìƒí™©ì„ ê³µìœ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. í˜„ì¬ 80% ì •ë„ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "ì ì‹¬ ë©”ë‰´ ì¶”ì²œ ë¶€íƒë“œë¦½ë‹ˆë‹¤. í•œì‹, ì¤‘ì‹, ì¼ì‹ ì¤‘ì—ì„œ ê³¨ë¼ì£¼ì„¸ìš”.",
            "ë‚´ì¼ ì¶œì¥ ì¼ì • í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤. ê¸°ì°¨í‘œ ì˜ˆì•½ë„ í•„ìš”í•©ë‹ˆë‹¤."
        ]
        
        results = {
            'single_inference': {},
            'batch_inference': {},
            'throughput_analysis': {}
        }
        
        try:
            # InferenceEngine ì´ˆê¸°í™”
            inference_engine = InferenceEngine(self.config)
            
            # 1. ë‹¨ì¼ ì¶”ë¡  ì„±ëŠ¥
            self.logger.info("1. ë‹¨ì¼ ì¶”ë¡  ì„±ëŠ¥ ì¸¡ì •...")
            
            single_times = []
            for dialogue in test_dialogues:
                start_time = time.time()
                summary = inference_engine.predict(dialogue)
                inference_time = time.time() - start_time
                single_times.append(inference_time)
            
            results['single_inference'] = {
                'avg_time_seconds': sum(single_times) / len(single_times),
                'min_time_seconds': min(single_times),
                'max_time_seconds': max(single_times),
                'total_samples': len(single_times)
            }
            
            # 2. ë°°ì¹˜ ì¶”ë¡  ì„±ëŠ¥
            self.logger.info("2. ë°°ì¹˜ ì¶”ë¡  ì„±ëŠ¥ ì¸¡ì •...")
            
            batch_sizes = [1, 2, 4]
            batch_results = {}
            
            for batch_size in batch_sizes:
                if len(test_dialogues) >= batch_size:
                    batch_data = test_dialogues[:batch_size]
                    
                    start_time = time.time()
                    summaries = inference_engine.predict_batch(batch_data, batch_size=batch_size)
                    batch_time = time.time() - start_time
                    
                    batch_results[f'batch_{batch_size}'] = {
                        'total_time_seconds': batch_time,
                        'time_per_sample': batch_time / batch_size,
                        'throughput_samples_per_sec': batch_size / batch_time
                    }
            
            results['batch_inference'] = batch_results
            
            # 3. ì²˜ë¦¬ëŸ‰ ë¶„ì„
            single_throughput = 1 / results['single_inference']['avg_time_seconds']
            best_batch_throughput = max([
                batch_results[key]['throughput_samples_per_sec'] 
                for key in batch_results.keys()
            ])
            
            results['throughput_analysis'] = {
                'single_throughput_samples_per_sec': single_throughput,
                'best_batch_throughput_samples_per_sec': best_batch_throughput,
                'throughput_improvement_percent': ((best_batch_throughput - single_throughput) / single_throughput) * 100
            }
            
            self.logger.info(f"ë‹¨ì¼ ì²˜ë¦¬ëŸ‰: {single_throughput:.2f} samples/sec")
            self.logger.info(f"ìµœì  ë°°ì¹˜ ì²˜ë¦¬ëŸ‰: {best_batch_throughput:.2f} samples/sec")
            
        except Exception as e:
            self.logger.error(f"ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
            results = {'success': False, 'error': str(e)}
        
        return results
    
    def benchmark_configuration_impact(self) -> Dict[str, Any]:
        """ì„¤ì • ìµœì í™” íš¨ê³¼ ë²¤ì¹˜ë§ˆí¬"""
        self.logger.info("=== ì„¤ì • ìµœì í™” íš¨ê³¼ ë¶„ì„ ===")
        
        results = {
            'configuration_analysis': {},
            'optimization_score': 0,
            'recommendations': []
        }
        
        # ì£¼ìš” ìµœì í™” ì„¤ì • í™•ì¸
        tokenizer_config = self.config.get('tokenizer', {})
        training_config = self.config.get('training', {})
        qlora_config = self.config.get('qlora', {})
        
        optimizations = {
            'decoder_max_len_200': tokenizer_config.get('decoder_max_len') == 200,
            'eval_strategy_steps': training_config.get('eval_strategy') == 'steps',
            'gradient_checkpointing': training_config.get('gradient_checkpointing') == True,
            'torch_empty_cache_steps': training_config.get('torch_empty_cache_steps') == 10,
            'qlora_enabled': qlora_config.get('use_qlora') == True,
            'lora_rank_optimized': qlora_config.get('lora_rank') == 16
        }
        
        # ìµœì í™” ì ìˆ˜ ê³„ì‚°
        score = sum(optimizations.values()) / len(optimizations) * 100
        results['optimization_score'] = score
        
        # ì„¤ì • ë¶„ì„
        results['configuration_analysis'] = {
            'applied_optimizations': optimizations,
            'total_optimizations': len(optimizations),
            'applied_count': sum(optimizations.values()),
            'score_percentage': score
        }
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = []
        if not optimizations['decoder_max_len_200']:
            recommendations.append("decoder_max_lenì„ 200ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë” ìƒì„¸í•œ ìš”ì•½ ìƒì„±")
        if not optimizations['eval_strategy_steps']:
            recommendations.append("eval_strategyë¥¼ 'steps'ë¡œ ë³€ê²½í•˜ì—¬ ì„¸ë°€í•œ ëª¨ë‹ˆí„°ë§")
        if not optimizations['gradient_checkpointing']:
            recommendations.append("gradient_checkpointing í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½")
        if not optimizations['qlora_enabled']:
            recommendations.append("QLoRA í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ")
        
        results['recommendations'] = recommendations
        
        self.logger.info(f"ìµœì í™” ì ìˆ˜: {score:.1f}% ({sum(optimizations.values())}/{len(optimizations)})")
        
        return results
    
    def run_full_benchmark(self) -> Dict[str, Any]:
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        self.logger.info("ğŸš€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        
        # ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        benchmark_results = {
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'config_path': self.config_path,
                'benchmark_version': '1.0'
            },
            'environment': self.benchmark_environment(),
            'model_loading': self.benchmark_model_loading(),
            'inference_speed': self.benchmark_inference_speed(),
            'configuration_impact': self.benchmark_configuration_impact()
        }
        
        # ê²°ê³¼ ìš”ì•½
        self._print_summary(benchmark_results)
        
        return benchmark_results
    
    def _print_summary(self, results: Dict[str, Any]):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ¯ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        # í™˜ê²½ ì •ë³´
        env = results['environment']
        print(f"ğŸ–¥ï¸  í™˜ê²½: {env['device_info']['type']}")
        print(f"ğŸ’¾ ë©”ëª¨ë¦¬: {env['memory_info']['total_gb']:.1f}GB")
        
        # ëª¨ë¸ ë¡œë”© ì„±ëŠ¥
        if 'memory_comparison' in results['model_loading']:
            mem_comp = results['model_loading']['memory_comparison']
            print(f"ğŸš€ ë©”ëª¨ë¦¬ ì ˆì•½: {mem_comp['memory_saving_percent']:.1f}%")
        
        # ìµœì í™” ì ìˆ˜
        config_score = results['configuration_impact']['optimization_score']
        print(f"âš™ï¸  ìµœì í™” ì ìˆ˜: {config_score:.1f}%")
        
        # ê¶Œì¥ì‚¬í•­
        recommendations = results['configuration_impact']['recommendations']
        if recommendations:
            print(f"ğŸ’¡ ê¶Œì¥ì‚¬í•­: {len(recommendations)}ê°œ")
            for rec in recommendations[:2]:  # ì²˜ìŒ 2ê°œë§Œ
                print(f"   - {rec}")
        else:
            print("âœ… ëª¨ë“  ìµœì í™” ì™„ë£Œ!")
        
        print("="*60)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    benchmark = PerformanceBenchmark()
    results = benchmark.run_full_benchmark()
    
    # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    output_file = f"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    import json
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“Š ìƒì„¸ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")


if __name__ == "__main__":
    main()
