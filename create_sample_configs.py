#!/usr/bin/env python3
"""ìƒ˜í”Œ ì‹¤í—˜ ì„¤ì • íŒŒì¼ ìƒì„± ìŠ¤í¬ë¦½íŠ¸"""

import os
import yaml
from pathlib import Path

def create_sample_configs():
    """ìƒ˜í”Œ ì‹¤í—˜ ì„¤ì • íŒŒì¼ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # config/experiments ë””ë ‰í† ë¦¬ í™•ì¸
    config_dir = Path("config/experiments")
    config_dir.mkdir(parents=True, exist_ok=True)
    
    # ìƒ˜í”Œ ì„¤ì •ë“¤
    sample_configs = {
        "01_baseline.yaml": {
            "experiment_name": "baseline",
            "description": "Baseline KoBART model experiment",
            "model": {
                "name": "digit82/kobart-summarization"
            },
            "training": {
                "learning_rate": 1e-5,
                "per_device_train_batch_size": 8,
                "num_train_epochs": 5
            },
            "wandb": {
                "name": "01_baseline_experiment"
            }
        },
        "02_high_lr.yaml": {
            "experiment_name": "high_learning_rate",
            "description": "Experiment with higher learning rate",
            "model": {
                "name": "digit82/kobart-summarization"
            },
            "training": {
                "learning_rate": 5e-5,
                "per_device_train_batch_size": 8,
                "num_train_epochs": 5
            },
            "wandb": {
                "name": "02_high_lr_experiment"
            }
        },
        "03_large_batch.yaml": {
            "experiment_name": "large_batch_size",
            "description": "Experiment with larger batch size",
            "model": {
                "name": "digit82/kobart-summarization"
            },
            "training": {
                "learning_rate": 1e-5,
                "per_device_train_batch_size": 16,
                "num_train_epochs": 5
            },
            "wandb": {
                "name": "03_large_batch_experiment"
            }
        },
        "04_longer_training.yaml": {
            "experiment_name": "longer_training",
            "description": "Experiment with more epochs",
            "model": {
                "name": "digit82/kobart-summarization"
            },
            "training": {
                "learning_rate": 1e-5,
                "per_device_train_batch_size": 8,
                "num_train_epochs": 10
            },
            "wandb": {
                "name": "04_longer_training_experiment"
            }
        }
    }
    
    # ì„¤ì • íŒŒì¼ ìƒì„±
    for filename, config in sample_configs.items():
        file_path = config_dir / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        print(f"âœ… ìƒ˜í”Œ ì„¤ì • ìƒì„±: {file_path}")
    
    print(f"\nğŸ“ ì´ {len(sample_configs)}ê°œ ìƒ˜í”Œ ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ")
    print(f"ğŸš€ ì‹¤í–‰ ë°©ë²•: bash run_auto_experiments.sh")

if __name__ == "__main__":
    create_sample_configs()
