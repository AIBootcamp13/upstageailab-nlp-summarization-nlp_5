#!/usr/bin/env python3
"""
Special tokens ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
baseline.pyì™€ trainer.pyì˜ special_tokens ì²˜ë¦¬ê°€ ë™ì¼í•œì§€ í™•ì¸
"""

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))
sys.path.insert(0, str(Path(__file__).parent / "code"))

try:
    from utils import load_config
    from trainer import DialogueSummarizationTrainer
except ImportError:
    # code ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰
    import os
    os.chdir(Path(__file__).parent)
    from utils import load_config
    from trainer import DialogueSummarizationTrainer
def test_special_tokens_loading():
    """special_tokens ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("ðŸ”§ Special tokens ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œìž‘")
    
    # eenzeenee T5 ì„¤ì • ë¡œë“œ (special_tokens ì •ì˜ë˜ì–´ ìžˆìŒ)
    config_path = "config/experiments/eenzeenee_t5_rtx3090.yaml"
    config = load_config(config_path)
    
    print(f"ì„¤ì • íŒŒì¼: {config_path}")
    print(f"Special tokens in config: {config.get('tokenizer', {}).get('special_tokens', [])}")
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± (í† í¬ë‚˜ì´ì €ë§Œ ë¡œë“œ)
    trainer = DialogueSummarizationTrainer(config=config, sweep_mode=False)
    
    print(f"ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸: {config.get('model', {}).get('checkpoint')}")
    
    # í† í¬ë‚˜ì´ì € ë¡œë”© (special_tokens ì²˜ë¦¬ í¬í•¨)
    trainer._load_tokenizer()
    
    # ê²°ê³¼ í™•ì¸
    special_tokens_added = getattr(trainer, '_special_tokens_added', False)
    new_vocab_size = getattr(trainer, '_new_vocab_size', None)
    
    print(f"\nðŸ“Š í† í¬ë‚˜ì´ì € ë¡œë”© ê²°ê³¼:")
    print(f"   Tokenizer type: {type(trainer.tokenizer).__name__}")
    print(f"   Vocab size: {len(trainer.tokenizer)}")
    print(f"   Special tokens added: {special_tokens_added}")
    print(f"   New vocab size: {new_vocab_size}")
    
    if special_tokens_added:
        # ì‹¤ì œ special tokens í™•ì¸
        print(f"\n   Special tokens map: {trainer.tokenizer.special_tokens_map}")
        
        # ê° special tokenì˜ ID í™•ì¸
        special_tokens_list = config.get('tokenizer', {}).get('special_tokens', [])
        for token in special_tokens_list[:3]:  # ì²˜ìŒ 3ê°œë§Œ í™•ì¸
            token_id = trainer.tokenizer.convert_tokens_to_ids(token)
            print(f"   '{token}' -> ID: {token_id}")
    
    print("\nâœ… Special tokens ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    return trainer

if __name__ == "__main__":
    test_special_tokens_loading()
