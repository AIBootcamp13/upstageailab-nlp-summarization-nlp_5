# 1에포크 테스트용 간단한 설정
experiment_name: simple_1epoch_test
description: "간단한 1에포크 테스트"

general:
  model_name: digit82/kobart-summarization
  data_path: ../data/
  train_path: ../data/train.csv
  val_path: ../data/dev.csv
  test_path: ../data/test.csv
  name: simple_1epoch_test

model:
  architecture: bart
  checkpoint: digit82/kobart-summarization

tokenizer:
  bos_token: <s>
  eos_token: </s>
  encoder_max_len: 256
  decoder_max_len: 64
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#PassportNumber#'

training:
  output_dir: ./outputs/test_1epoch
  overwrite_output_dir: true
  do_train: true
  do_eval: true
  
  # 1에포크 설정
  num_train_epochs: 1
  max_steps: 50  # 50 스텝만
  
  # 평가/저장 전략 일치
  evaluation_strategy: steps
  save_strategy: steps
  eval_steps: 25
  save_steps: 25
  
  # 배치 크기
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 1
  
  # 학습률
  learning_rate: 2.0e-05
  warmup_steps: 10
  weight_decay: 0.01
  
  # 최적화
  fp16: true
  gradient_checkpointing: false
  
  # 로깅
  logging_steps: 10
  save_total_limit: 2
  load_best_model_at_end: true
  metric_for_best_model: eval_loss
  greater_is_better: false
  
  # 생성 설정
  predict_with_generate: true
  generation_max_length: 64
  generation_num_beams: 2
  
  # WandB 비활성화
  report_to: []
  
  seed: 42

# generation 섹션 추가
generation:
  max_length: 64
  min_length: 5
  num_beams: 2
  no_repeat_ngram_size: 2
  early_stopping: true
  length_penalty: 1.0
  
inference:
  batch_size: 32
  generate_max_length: 64
  num_beams: 2
  no_repeat_ngram_size: 2
  early_stopping: true
  remove_tokens:
    - '<usr>'
    - '<s>'
    - '</s>'
    - '<pad>'

wandb:
  entity: lyjune37-juneictlab
  project: nlp-dialogue-summarization
  name: simple_1epoch_test
  tags: [test, 1epoch, kobart]
  notes: "간단한 1에포크 테스트"
