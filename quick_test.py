#!/usr/bin/env python3
"""
ë¹ ë¥¸ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ (Quick Test Runner)

1ì—í¬í¬ë§Œ ì‹¤í–‰í•˜ì—¬ ì „ì²´ íŒŒì´í”„ë¼ì¸ì´ ì—ëŸ¬ ì—†ì´ ë™ì‘í•˜ëŠ”ì§€ ë¹ ë¥´ê²Œ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import sys
import os
import yaml
import argparse
import logging
from pathlib import Path

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

from code.trainer import NMTTrainer, TrainingConfig
from code.utils import load_config

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_quick_test_config(base_config: dict) -> dict:
    """
    ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì„¤ì • ìƒì„±
    
    Args:
        base_config: ê¸°ë³¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        
    Returns:
        ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ìˆ˜ì •ëœ ì„¤ì •
    """
    # ê¸°ë³¸ ì„¤ì • ë³µì‚¬
    quick_config = base_config.copy()
    
    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì„¤ì • ì ìš©
    training_overrides = {
        'num_train_epochs': 1,  # 1ì—í¬í¬ë§Œ
        'per_device_train_batch_size': 2,  # ì‘ì€ ë°°ì¹˜ í¬ê¸°
        'per_device_eval_batch_size': 2,
        'logging_steps': 10,  # ìì£¼ ë¡œê¹…
        'eval_steps': 50,  # ìì£¼ í‰ê°€
        'save_steps': 50,  # ìì£¼ ì €ì¥
        'warmup_steps': 10,  # ì ì€ ì›Œë°ì—…
        'max_steps': 100,  # ìµœëŒ€ ìŠ¤í… ì œí•œ
        'load_best_model_at_end': False,  # ì‹œê°„ ë‹¨ì¶•
        'evaluation_strategy': 'steps',  # steps ê¸°ë°˜ í‰ê°€
        'save_strategy': 'steps',
        'report_to': [],  # WandB ë¹„í™œì„±í™” (ì„ íƒì )
    }
    
    # í† í¬ë‚˜ì´ì € ì„¤ì • ì¡°ì •
    tokenizer_overrides = {
        'encoder_max_len': 256,  # ì§§ì€ ì…ë ¥
        'decoder_max_len': 64,   # ì§§ì€ ì¶œë ¥
    }
    
    # ì¶”ë¡  ì„¤ì • ì¡°ì •
    inference_overrides = {
        'batch_size': 2,
        'generate_max_length': 64,
    }
    
    # ì„¤ì • ì ìš©
    if 'training' in quick_config:
        quick_config['training'].update(training_overrides)
    else:
        quick_config['training'] = training_overrides
        
    if 'tokenizer' in quick_config:
        quick_config['tokenizer'].update(tokenizer_overrides)
    else:
        quick_config['tokenizer'] = tokenizer_overrides
        
    if 'inference' in quick_config:
        quick_config['inference'].update(inference_overrides)
    else:
        quick_config['inference'] = inference_overrides
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ë¥¼ quick_testë¡œ ë³€ê²½
    if 'general' in quick_config:
        original_output = quick_config['general'].get('output_dir', './outputs')
        quick_config['general']['output_dir'] = f"{original_output}_quick_test"
    
    return quick_config


def limit_dataset_samples(trainer: NMTTrainer, max_samples: int = 100):
    """
    ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜ë¥¼ ì œí•œí•˜ì—¬ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
    
    Args:
        trainer: NMTTrainer ì¸ìŠ¤í„´ìŠ¤
        max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜
    """
    if hasattr(trainer, 'train_dataset') and trainer.train_dataset:
        original_size = len(trainer.train_dataset)
        if original_size > max_samples:
            # ë°ì´í„°ì…‹ì„ max_samplesë¡œ ì œí•œ
            trainer.train_dataset = trainer.train_dataset.select(range(max_samples))
            logger.info(f"ğŸ—‚ï¸ í›ˆë ¨ ë°ì´í„°ì…‹ ì œí•œ: {original_size} -> {max_samples} ìƒ˜í”Œ")
    
    if hasattr(trainer, 'valid_dataset') and trainer.valid_dataset:
        original_size = len(trainer.valid_dataset)
        eval_samples = min(max_samples // 4, 50)  # í‰ê°€ìš©ì€ ë” ì ê²Œ
        if original_size > eval_samples:
            trainer.valid_dataset = trainer.valid_dataset.select(range(eval_samples))
            logger.info(f"ğŸ—‚ï¸ ê²€ì¦ ë°ì´í„°ì…‹ ì œí•œ: {original_size} -> {eval_samples} ìƒ˜í”Œ")


def run_quick_test(config_path: str, 
                   model_section: str = None,
                   max_samples: int = 100,
                   disable_wandb: bool = True) -> dict:
    """
    ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    Args:
        config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        model_section: ì‚¬ìš©í•  ëª¨ë¸ ì„¹ì…˜ (ì˜ˆ: 'eenzeenee', 'xlsum_mt5')
        max_samples: ìµœëŒ€ í›ˆë ¨ ìƒ˜í”Œ ìˆ˜
        disable_wandb: WandB ë¹„í™œì„±í™” ì—¬ë¶€
        
    Returns:
        í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    try:
        logger.info(f"ğŸš€ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹œì‘: {config_path}")
        
        # ì„¤ì • ë¡œë“œ
        base_config = load_config(config_path)
        
        # íŠ¹ì • ëª¨ë¸ ì„¹ì…˜ ì‚¬ìš©
        if model_section and model_section in base_config:
            logger.info(f"ğŸ“‹ ëª¨ë¸ ì„¹ì…˜ ì‚¬ìš©: {model_section}")
            config = base_config[model_section]
        else:
            config = base_config
        
        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì„¤ì • ì ìš©
        quick_config = create_quick_test_config(config)
        
        # WandB ë¹„í™œì„±í™”
        if disable_wandb:
            quick_config.get('training', {})['report_to'] = []
            logger.info("ğŸ“´ WandB ë¹„í™œì„±í™”ë¨")
        
        # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        trainer = NMTTrainer(quick_config)
        
        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        logger.info("ğŸ¤– ëª¨ë¸ ë¡œë”© ì¤‘...")
        trainer.load_model_and_tokenizer()
        logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # ë°ì´í„° ë¡œë“œ
        logger.info("ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...")
        trainer.load_data()
        logger.info("âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ")
        
        # ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜ ì œí•œ
        limit_dataset_samples(trainer, max_samples)
        
        # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        logger.info("âš™ï¸ íŠ¸ë ˆì´ë„ˆ ì„¤ì • ì¤‘...")
        trainer.setup_trainer()
        logger.info("âœ… íŠ¸ë ˆì´ë„ˆ ì„¤ì • ì™„ë£Œ")
        
        # ë¹ ë¥¸ í›ˆë ¨ ì‹¤í–‰
        logger.info("ğŸƒ ë¹ ë¥¸ í›ˆë ¨ ì‹œì‘ (1 epoch)...")
        train_result = trainer.train()
        logger.info("âœ… í›ˆë ¨ ì™„ë£Œ")
        
        # ê°„ë‹¨í•œ í‰ê°€
        logger.info("ğŸ“ˆ í‰ê°€ ì‹¤í–‰ ì¤‘...")
        eval_result = trainer.evaluate()
        logger.info("âœ… í‰ê°€ ì™„ë£Œ")
        
        # ê²°ê³¼ ì •ë¦¬
        result = {
            'status': 'success',
            'model_name': quick_config.get('general', {}).get('model_name', 'unknown'),
            'train_samples': len(trainer.train_dataset) if trainer.train_dataset else 0,
            'eval_samples': len(trainer.valid_dataset) if trainer.valid_dataset else 0,
            'epochs_completed': 1,
            'train_metrics': train_result,
            'eval_metrics': eval_result,
            'config_used': quick_config
        }
        
        # ì„±ëŠ¥ ë¡œê¹…
        if 'eval_rouge_l' in eval_result:
            rouge_l = eval_result['eval_rouge_l']
            logger.info(f"ğŸ¯ ROUGE-L ì ìˆ˜: {rouge_l:.4f}")
        
        logger.info("ğŸ‰ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
        return result
        
    except Exception as e:
        logger.error(f"âŒ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return {
            'status': 'failed',
            'error': str(e),
            'model_name': model_section or 'unknown'
        }
    
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        if 'trainer' in locals():
            trainer.cleanup()


def main():
    """CLI ì¸í„°í˜ì´ìŠ¤"""
    parser = argparse.ArgumentParser(description="ë¹ ë¥¸ ê²€ì¦ í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ")
    parser.add_argument('--config', default='config.yaml',
                       help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--model-section', 
                       choices=['eenzeenee', 'xlsum_mt5', 'baseline'],
                       help='ì‚¬ìš©í•  ëª¨ë¸ ì„¹ì…˜')
    parser.add_argument('--max-samples', type=int, default=100,
                       help='ìµœëŒ€ í›ˆë ¨ ìƒ˜í”Œ ìˆ˜')
    parser.add_argument('--disable-wandb', action='store_true', default=True,
                       help='WandB ë¹„í™œì„±í™”')
    parser.add_argument('--verbose', action='store_true',
                       help='ìƒì„¸ ë¡œê¹… í™œì„±í™”')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # ì„¤ì • íŒŒì¼ í™•ì¸
    config_path = Path(args.config)
    if not config_path.exists():
        logger.error(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
        return 1
    
    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    result = run_quick_test(
        config_path=str(config_path),
        model_section=args.model_section,
        max_samples=args.max_samples,
        disable_wandb=args.disable_wandb
    )
    
    # ê²°ê³¼ ì¶œë ¥
    if result['status'] == 'success':
        print(f"\nâœ… ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print(f"ğŸ“Š ëª¨ë¸: {result['model_name']}")
        print(f"ğŸ“Š í›ˆë ¨ ìƒ˜í”Œ: {result['train_samples']}")
        print(f"ğŸ“Š í‰ê°€ ìƒ˜í”Œ: {result['eval_samples']}")
        
        eval_metrics = result.get('eval_metrics', {})
        if 'eval_rouge_l' in eval_metrics:
            print(f"ğŸ¯ ROUGE-L: {eval_metrics['eval_rouge_l']:.4f}")
        
        return 0
    else:
        print(f"\nâŒ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {result['error']}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
