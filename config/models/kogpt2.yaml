# KoGPT-2 모델 특화 설정  
# skt/kogpt2-base-v2를 대화 요약에 적용

# 모델 기본 정보
model:
  architecture: "kogpt2"
  checkpoint: "skt/kogpt2-base-v2"
  model_type: "gpt2"
  config_class: "GPT2Config"
  tokenizer_class: "PreTrainedTokenizerFast"
  
  # KoGPT-2 특화 설정
  config_overrides:
    vocab_size: 51200
    n_positions: 1024
    n_embd: 768
    n_layer: 12
    n_head: 12
    activation_function: "gelu_new"
    resid_pdrop: 0.1
    embd_pdrop: 0.1
    attn_pdrop: 0.1
    layer_norm_epsilon: 1e-5
    use_cache: true

# 토크나이저 설정 (GPT-2 특화)
tokenizer:
  model_max_length: 1024
  truncation_side: "left"  # GPT-2는 왼쪽 자르기
  padding_side: "left"     # GPT-2는 왼쪽 패딩
  
  # KoGPT-2 특수 토큰
  bos_token: "<|startoftext|>"
  eos_token: "<|endoftext|>"
  unk_token: "<|unk|>"
  pad_token: "<|pad|>"
  
  special_tokens:
    - '<|startoftext|>'
    - '<|endoftext|>'
    - '<|summary|>'  # 요약 시작 토큰
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#PassportNumber#'

# KoGPT-2 학습 설정 (Causal LM 특화)
training:
  # GPT-2 최적화 파라미터
  learning_rate: 5.0e-05  # GPT-2는 상대적으로 높은 LR
  per_device_train_batch_size: 16  # 메모리 사용량 고려
  per_device_eval_batch_size: 32
  warmup_ratio: 0.05  # GPT-2는 적은 워밍업
  weight_decay: 0.1   # 강한 정규화
  lr_scheduler_type: "cosine_with_restarts"
  num_train_epochs: 15
  
  # GPT-2 특화 최적화
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  gradient_accumulation_steps: 2  # 메모리 효율성
  
  # Causal LM 설정
  remove_unused_columns: false
  prediction_loss_only: false
  
  # 메모리 최적화
  fp16: true
  dataloader_num_workers: 2  # GPT-2는 상대적으로 적게

# 생성 설정 (GPT-2 특화)
generation:
  max_new_tokens: 128  # GPT-2는 max_new_tokens 사용
  min_length: 10
  do_sample: true      # GPT-2는 샘플링 생성
  temperature: 0.8     # 적절한 다양성
  top_k: 50
  top_p: 0.95
  num_beams: 1         # 샘플링 시에는 빔=1
  repetition_penalty: 1.1
  no_repeat_ngram_size: 2
  
  # GPT-2 특수 토큰 설정
  pad_token_id: 50256  # <|pad|>
  eos_token_id: 50257  # <|endoftext|>
  bos_token_id: 50258  # <|startoftext|>

# 데이터 전처리 (GPT-2 특화)
data_preprocessing:
  # GPT-2는 인과적 언어 모델이므로 특별한 형식 필요
  input_format: "dialogue: {dialogue} <|summary|> {summary} <|endoftext|>"
  max_source_length: 800   # 요약문 공간 확보
  max_target_length: 128
  
  # GPT-2 데이터 처리
  group_texts: false
  return_overflowing_tokens: false
  truncation_strategy: "longest_first"

# 평가 설정 (GPT-2 특화)
evaluation:
  # GPT-2 생성 평가
  prediction_loss_only: false
  eval_accumulation_steps: 2
  
  # 생성 기반 평가
  generate_during_eval: true
  eval_with_generate: true

# KoGPT-2 하이퍼파라미터 탐색 공간
sweep_parameters:
  learning_rate:
    distribution: log_uniform_values
    min: 1.0e-5
    max: 1.0e-4
  
  temperature:
    distribution: uniform
    min: 0.7
    max: 1.0
  
  top_p:
    values: [0.9, 0.95, 1.0]
  
  repetition_penalty:
    distribution: uniform
    min: 1.0
    max: 1.3
  
  per_device_train_batch_size:
    values: [8, 16, 24]
  
  gradient_accumulation_steps:
    values: [1, 2, 4]

# WandB 태그
wandb_tags:
  - "kogpt2"
  - "gpt2"
  - "korean"
  - "causal_lm"
  - "autoregressive"
  - "sampling"

# 추론 최적화
inference_optimization:
  batch_size: 32
  use_cache: true
  torch_dtype: "float16"
  device_map: "auto"
  generation_config:
    do_sample: true
    temperature: 0.8
    top_p: 0.95
