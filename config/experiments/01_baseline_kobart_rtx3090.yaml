# KoBART RTX 3090 24GB 최적화 실험
experiment_name: kobart_baseline_rtx3090_optimized
description: "KoBART 베이스라인 RTX 3090 최고 성능 최적화"

general:
  model_name: digit82/kobart-summarization
  data_path: data/
  train_path: data/train.csv
  val_path: data/dev.csv
  model_type: seq2seq
  name: kobart_baseline_rtx3090_optimized

model:
  architecture: bart
  checkpoint: digit82/kobart-summarization

# 🎯 KoBART용 prefix (빈 문자열)
input_prefix: ""

tokenizer:
  bos_token: <s>
  eos_token: </s>
  encoder_max_len: 768               # 긴 대화 처리
  decoder_max_len: 150               # 충분한 요약 길이
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#PassportNumber#'
    - '#DateOfBirth#'
    - '#SSN#'
    - '#CardNumber#'
    - '#CarNumber#'
    - '#Email#'

# 🚀 RTX 3090 KoBART 최적화
training:
  do_eval: true
  do_train: true
  evaluation_strategy: steps
  eval_steps: 200
  
  # 🔥 RTX 3090 KoBART 최적 배치 (BART는 메모리 더 많이 사용)
  per_device_train_batch_size: 8      # BART 최적 배치
  per_device_eval_batch_size: 16      # 평가시 더 큰 배치
  gradient_accumulation_steps: 3      # 유효 배치 24
  
  # 🎯 KoBART 최적화 학습률
  num_train_epochs: 4
  learning_rate: 3.0e-05              # KoBART 안정적 학습률
  lr_scheduler_type: linear
  warmup_ratio: 0.1                   # BART는 긴 웜업 선호
  weight_decay: 0.01
  
  # ⚡ RTX 3090 최적화
  fp16: true
  gradient_checkpointing: true        # BART 메모리 절약
  dataloader_num_workers: 8
  dataloader_pin_memory: true
  group_by_length: true
  
  # 📊 모니터링
  logging_steps: 50
  save_strategy: steps
  save_steps: 200
  save_total_limit: 5
  load_best_model_at_end: true
  early_stopping_patience: 5
  
  # 🎯 KoBART 최적 생성 설정
  predict_with_generate: true
  generation_num_beams: 8             # KoBART 고품질 생성
  generation_max_length: 150
  generation_min_length: 20
  generation_length_penalty: 1.0
  generation_no_repeat_ngram_size: 3
  generation_do_sample: false
  
  report_to: wandb
  seed: 42

# 🚀 KoBART QLoRA 최적화 (Unsloth 활성화)
qlora:
  use_unsloth: true
  use_qlora: true                     # BART는 QLoRA 효과적
  lora_rank: 64                       # 높은 표현력
  lora_alpha: 128
  lora_dropout: 0.1
  target_modules: ["q_proj", "k_proj", "v_proj", "out_proj"]  # BART 모듈
  load_in_4bit: true
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

wandb:
  entity: lyjune37-juneictlab
  project: nlp-5
  name: kobart_baseline_rtx3090_optimized
  tags: [KoBART, RTX3090, optimized, korean, qlora]
  notes: "KoBART RTX 3090 최고 성능 최적화 - QLoRA"
