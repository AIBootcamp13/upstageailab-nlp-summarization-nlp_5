# 고성능 학습률 RTX 3090 극한 최적화 실험
experiment_name: high_learning_rate_extreme_rtx3090
description: "고성능 학습률 RTX 3090 + Unsloth 극한 최적화 - 배치 14 + 학습률 8.0e-05"

general:
  model_name: digit82/kobart-summarization
  data_path: data/
  train_path: data/train.csv
  val_path: data/dev.csv
  model_type: seq2seq
  name: high_learning_rate_extreme_rtx3090

model:
  architecture: bart
  checkpoint: digit82/kobart-summarization

# 🔥 극한 최적화 prefix
input_prefix: ""

tokenizer:
  bos_token: <s>
  eos_token: </s>
  encoder_max_len: 1280              # 극한 최적화 1024→1280
  decoder_max_len: 230               # 긴 요약 지원 200→230
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#PassportNumber#'
    - '#DateOfBirth#'
    - '#SSN#'
    - '#CardNumber#'
    - '#CarNumber#'
    - '#Email#'
    - '<summary>'
    - '</summary>'
    - '<dialogue>'
    - '</dialogue>'

# 🔥 RTX 3090 + Unsloth 고학습률 극한 최적화
training:
  do_eval: true
  do_train: true
  evaluation_strategy: steps
  eval_steps: 100
  
  # 🔥 RTX 3090 고학습률 극한 배치
  per_device_train_batch_size: 14     # 극한 최적화 8→14
  per_device_eval_batch_size: 20      # 평가시 더 큰 배치
  gradient_accumulation_steps: 4      # 유효 배치 56
  
  # 🚀 극한 고학습률 실험
  num_train_epochs: 6
  learning_rate: 1.2e-04              # 극한 고학습률 4.0e-05→1.2e-04
  lr_scheduler_type: cosine_with_restarts
  warmup_ratio: 0.01                  # 매우 짧은 웜업
  weight_decay: 0.0001
  
  # 🔥 RTX 3090 + CUDA 12.2 + Unsloth 극한 최적화
  fp16: false
  bf16: true
  tf32: true
  gradient_checkpointing: false
  dataloader_num_workers: 36
  dataloader_pin_memory: true
  dataloader_persistent_workers: true
  group_by_length: true
  remove_unused_columns: false
  
  # 🔥 Unsloth 특화 옵티마이저
  optim: adamw_torch
  adam_beta1: 0.9
  adam_beta2: 0.95
  max_grad_norm: 0.3
  
  # 📊 세밀한 모니터링
  logging_steps: 25
  save_strategy: steps
  save_steps: 100
  save_total_limit: 10
  load_best_model_at_end: true
  early_stopping_patience: 10
  
  # 🏆 고학습률 극한 생성
  predict_with_generate: true
  generation_num_beams: 12
  generation_max_length: 230
  generation_min_length: 30
  generation_length_penalty: 0.8
  generation_no_repeat_ngram_size: 3
  generation_do_sample: false
  generation_early_stopping: true
  
  report_to: wandb
  seed: 42

# 🔥 고학습률 + Unsloth 극한 QLoRA
qlora:
  use_unsloth: true
  use_qlora: true
  lora_rank: 160
  lora_alpha: 320
  lora_dropout: 0.01
  target_modules: ["q", "k", "v", "o", "fc1", "fc2", "lm_head", "embed_tokens"]
  load_in_4bit: true
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

wandb:
  entity: lyjune37-juneictlab
  project: nlp-5
  name: high_learning_rate_extreme_rtx3090
  tags: [KoBART, RTX3090, extreme, high_lr, Unsloth, batch14, lr1.2e-04]
  notes: "고학습률 RTX 3090 + Unsloth 극한 최적화 - 배치 14, 학습률 1.2e-04, LoRA 160"

# 🏆 고학습률 극한 생성 설정
generation:
  max_length: 230
  min_length: 30
  num_beams: 12
  length_penalty: 0.8
  no_repeat_ngram_size: 3
  early_stopping: true
  do_sample: false

# baseline.py 호환 - 추론 설정
inference:
  ckt_path: "model_ckt_path"
  result_path: "./prediction/"
  batch_size: 32
  
  # baseline.py 호환 - 생성 설정
  no_repeat_ngram_size: 2
  early_stopping: true
  generate_max_length: 100
  num_beams: 4
  
  # baseline.py 호환 - 특수 토큰 제거
  remove_tokens: 
    - "<usr>"
    - "<s>"
    - "</s>"
    - "<pad>"
    - "#Person1#"
    - "#Person2#"
    - "#Person3#"
    - "#PhoneNumber#"
    - "#Address#"
    - "#PassportNumber#"
  
  output_format: "csv"
