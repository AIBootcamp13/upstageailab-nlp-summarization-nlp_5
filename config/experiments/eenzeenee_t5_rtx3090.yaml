# eenzeenee T5 RTX 3090 극한 최적화 실험
experiment_name: eenzeenee_t5_extreme_rtx3090
description: "eenzeenee T5 한국어 요약 모델 RTX 3090 + Unsloth 극한 최적화 - 배치 20 + Full Finetune"

general:
  model_name: eenzeenee/t5-base-korean-summarization
  data_path: ./data/
  train_path: ./data/train.csv
  val_path: ./data/dev.csv
  test_path: ./data/test.csv  # 🔥 추론용 테스트 데이터 경로 추가
  model_type: seq2seq
  name: eenzeenee_t5_extreme_rtx3090

model:
  architecture: t5
  checkpoint: eenzeenee/t5-base-korean-summarization

# 🔥 T5 극한 최적화 한국어 요약 prefix (정확한 eenzeenee 모델 prefix 사용)
input_prefix: "summarize: "
tokenizer:
  eos_token: "</s>"       # T5 종료 토큰
  unk_token: "<unk>"      # T5 unknown 토큰
  pad_token: "<pad>"      # T5 패딩 토큰
  bos_token: ""            # T5는 BOS 토큰 사용하지 않음 (빈 문자열)
  encoder_max_len: 1024              # T5-base 극한 최적화 768→1024
  decoder_max_len: 200               # 충분한 요약 길이 150→200
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#PassportNumber#'
    - '#DateOfBirth#'
    - '#SSN#'
    - '#CardNumber#'
    - '#CarNumber#'
    - '#Email#'
    - '<summary>'
    - '</summary>'
    - '<dialogue>'
    - '</dialogue>'

# 🔥 RTX 3090 + Unsloth T5 극한 최적화 학습 설정
training:
  do_eval: true
  do_train: true
  evaluation_strategy: steps
  eval_steps: 150                     # 더 자주 평가
  
  # 🔥 RTX 3090 T5-base 극한 최적화 배치
  per_device_train_batch_size: 8      # 메모리 부족 문제 해결을 위해 20→8로 안전하게 조정
  per_device_eval_batch_size: 16       # 평가시 메모리 절약을 위해 32→16으로 조정
  gradient_accumulation_steps: 2      # 유효 배치 40
  
  # 🎯 T5 극한 최적화 학습 파라미터
  num_train_epochs: 5                 # 충분한 학습
  learning_rate: 8.0e-05              # T5-base 극한 최적화 학습률 5.0e-05→8.0e-05
  lr_scheduler_type: cosine_with_restarts
  warmup_ratio: 0.03                  # 짧은 웜업
  weight_decay: 0.001                 # 세밀한 정규화
  
  # 🔥 RTX 3090 + CUDA 12.2 + Unsloth 극한 최적화
  fp16: false                         # bf16 사용으로 변경
  bf16: true                          # RTX 3090 + CUDA 12.2 최적
  tf32: true                          # Ampere 아키텍처 최적화
  gradient_checkpointing: false       # T5-base는 메모리 여유 + Unsloth 자체 최적화
  dataloader_num_workers: 36          # RTX 3090 극한 최적화 8→36 (48코어 75%)
  dataloader_pin_memory: true         # 251GB RAM 활용
  dataloader_persistent_workers: true # 워커 재사용
  group_by_length: true
  remove_unused_columns: false        # 전체 정보 활용
  
  # 🔥 Unsloth 특화 옵티마이저 설정
  optim: adamw_torch                   # Unsloth 호환 옵티마이저
  adam_beta1: 0.9
  adam_beta2: 0.95                    # Unsloth 추천값
  max_grad_norm: 0.3                  # Unsloth와 함께 낮은 값
  
  # 📊 모니터링
  logging_steps: 50
  save_strategy: steps
  save_steps: 200
  save_total_limit: 8                 # 더 많은 체크포인트
  load_best_model_at_end: true
  early_stopping_patience: 8          # 충분한 학습 시간
  
  # 🏆 T5 극한 최적화 생성 설정
  predict_with_generate: true
  generation_num_beams: 3             # eenzeenee 모델 권장값: 3
  generation_max_length: 64           # eenzeenee 모델 권장값: 64
  generation_min_length: 10           # eenzeenee 모델 권장값: 10
  generation_length_penalty: 1.0      # 기본값
  generation_no_repeat_ngram_size: 2  # 기본값
  generation_do_sample: true          # ✅ eenzeenee 모델 필수: true
  generation_early_stopping: true     # 조기 중단
  
  report_to: wandb
  seed: 42

# 🔥 T5 + Unsloth 극한 최적화 QLoRA (Full Fine-tuning 고려)
qlora:
  use_unsloth: true
  use_qlora: true                     # T5-base는 Full Fine-tuning 가능하지만 QLoRA 사용
  lora_rank: 128                     # 64→128 2배 증가
  lora_alpha: 256                    # 128→256 2배 증가
  lora_dropout: 0.05                 # 0.1→0.05 드롭아웃 감소
  target_modules: ["q", "k", "v", "o", "wi", "wo", "lm_head", "embed_tokens"]
  load_in_4bit: true                 # Unsloth 사용
  bnb_4bit_compute_dtype: "bfloat16"  # RTX 3090 + CUDA 12.2 최적
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true
  load_in_8bit: false

wandb:
  entity: lyjune37-juneictlab
  project: nlp-5
  name: eenzeenee_t5_extreme_rtx3090
  tags: [eenzeenee, T5-base, RTX3090, extreme, korean, Unsloth, batch20]
  notes: "eenzeenee T5 RTX 3090 + Unsloth 극한 최적화 - 배치 20, 시퀀스 1024, LoRA 128"

# 🏆 T5 극한 최적화 생성 설정
generation:
  max_length: 64                      # eenzeenee 모델 권장값
  min_length: 10                      # eenzeenee 모델 권장값
  num_beams: 3                        # eenzeenee 모델 권장값
  length_penalty: 1.0                 # 기본값
  no_repeat_ngram_size: 2             # 기본값
  early_stopping: true                # 조기 중단
  do_sample: true                     # ✅ eenzeenee 모델 필수 설정

# baseline.py 호환 - 추론 설정
inference:
  ckt_path: "model_ckt_path"
  result_path: "./prediction/"
  batch_size: 32
  
  # generation 설정과 동일하게 수정
  no_repeat_ngram_size: 2             # generation과 동일
  early_stopping: true               # generation과 동일
  generate_max_length: 64             # generation과 동일: 64
  num_beams: 3                        # generation과 동일: 3
  
  # baseline.py 호환 - 특수 토큰 제거
  remove_tokens: 
    - "<usr>"        # baseline.py 호환
    - "</s>"         # T5 종료 토큰
    - "<pad>"        # T5 패딩 토큰
    - "<unk>"        # unknown 토큰
    - "#Person1#"    # 특수 토큰들
    - "#Person2#" 
    - "#Person3#"
    - "#PhoneNumber#"
    - "#Address#"
    - "#PassportNumber#"
  
  output_format: "csv"
