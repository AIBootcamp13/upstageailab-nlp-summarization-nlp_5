# 학습률 스케줄링 실험

## 개요
이 디렉토리는 대화 요약을 위한 최적의 수렴 패턴을 찾기 위한 다양한 학습률 스케줄링 실험을 포함합니다.

## 실험 구성

### 1. 코사인 어닐링 (03a)
- **설명**: 초기 학습률에서 거의 0까지 부드러운 코사인 감소
- **초기 학습률**: 3e-5 (부드러운 감소를 위해 약간 높게 설정)
- **워밍업**: 전체 학습 단계의 10%
- **장점**: 부드러운 감소, 트랜스포머에서 검증된 효과
- **최적 용도**: 점진적인 성능 개선을 원할 때

### 2. 선형 워밍업 + 감소 (03b)
- **설명**: 최고 학습률까지 선형 증가 후 선형 감소
- **초기 학습률**: 1e-5 (표준)
- **워밍업**: 500 스텝 고정
- **장점**: 단순하고 예측 가능
- **최적 용도**: 표준 파인튜닝 시나리오

### 3. 지수 감소 (03c)
- **설명**: 다항식 스케줄러를 사용한 공격적인 지수형 감소
- **초기 학습률**: 5e-5 (공격적 감소를 위해 높은 초기값)
- **거듭제곱**: 2.0 (2차 감소)
- **장점**: 빠른 초기 학습, 후반부 세밀한 조정
- **최적 용도**: 모델이 빠른 적응이 필요할 때

### 4. 순환 학습률 (03d)
- **설명**: 코사인 재시작을 포함한 진동 학습률
- **최대 학습률**: 3e-5
- **사이클**: 2회 완전 순환
- **장점**: 지역 최솟값 탈출 가능, 더 나은 해 탐색
- **최적 용도**: 표준 스케줄이 일찍 정체될 때

### 5. 워밍업 후 상수 (03e)
- **설명**: 워밍업 후 상수 학습률 (감소 없음)
- **학습률**: 1e-5 (워밍업 후)
- **워밍업**: 300 스텝
- **장점**: 워밍업 후 안정적인 학습
- **최적 용도**: 모델이 쉽게 과적합되지 않을 때

### 6. 역제곱근 (03f)
- **설명**: 트랜스포머에서 인기 있는 1/√t 감소
- **초기 학습률**: 2e-5
- **거듭제곱**: 0.5 (제곱근 감소)
- **장점**: 빠른 초기 학습과 안정적인 후반 학습의 균형
- **최적 용도**: 트랜스포머 모델, 긴 학습 실행

## 예상 결과

각 스케줄러는 서로 다른 수렴 패턴을 보일 것으로 예상됩니다:

1. **코사인 어닐링**: 부드러운 수렴, 좋은 최종 성능
2. **선형**: 예측 가능한 개선, 표준 베이스라인
3. **지수**: 빠른 초기 향상, 잠재적 조기 정체
4. **순환**: 여러 성능 피크, 어떤 사이클에서든 최고 모델
5. **상수**: 꾸준한 개선, 수동 중단 필요할 수 있음
6. **역제곱근**: 균형잡힌 수렴, 긴 학습에 적합

## 선택 기준

다음 기준에 따라 최적의 스케줄러를 선택하세요:
- **수렴 속도**: 좋은 성능에 도달하는 속도
- **최종 성능**: 달성한 최고 ROUGE 점수
- **안정성**: 손실 급증이나 불규칙한 동작의 부재
- **과적합**: 과적합 방지에 도움이 되는지 여부

## 실험 실행

모든 실험은 5 에포크 빠른 테스트용으로 설계되었습니다. 실행 방법:

```bash
# 모든 스케줄링 실험 실행
for config in config/experiments/03_lr_scheduling/*.yaml; do
    python code/trainer.py --config "$config"
done
```

## 분석

모든 실험 실행 후 다음을 비교하세요:
1. 학습 곡선 (시간에 따른 손실)
2. ROUGE 점수 진행
3. 각 스케줄러의 최적 에포크
4. 학습 안정성

최적의 스케줄러는 베이스라인 상수 학습률 대비 +1-2% 개선을 제공해야 합니다.
