experiment_name: postprocessing_aggressive
description: More aggressive postprocessing settings for maximum quality improvement

# Model configuration - Same as baseline
model:
  name: digit82/kobart-summarization
  architecture: kobart

# Tokenizer configuration  
tokenizer:
  encoder_max_len: 512
  decoder_max_len: 200
  bos_token: "<s>"
  eos_token: "</s>"
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#DateOfBirth#'
    - '#PassportNumber#'
    - '#SSN#'
    - '#CardNumber#'
    - '#CarNumber#'
    - '#Email#'

# Aggressive postprocessing configuration
postprocessing:
  enabled: true
  
  # More aggressive duplicate removal
  duplicate_removal:
    enabled: true
    min_phrase_length: 2  # Check shorter phrases too
    similarity_threshold: 0.75  # Lower threshold to catch more duplicates
  
  # Stricter length optimization
  length_optimization:
    enabled: true
    min_tokens: 60   # Higher minimum
    max_tokens: 90   # Lower maximum
    target_tokens: 75  # Target middle range
  
  # Enhanced token validation
  token_validation:
    enabled: true
    copy_missing_tokens: true
    add_context: true  # Add contextual phrases with tokens
    special_tokens:  # All PII tokens
      - '#PhoneNumber#'
      - '#Address#'
      - '#Email#'
      - '#DateOfBirth#'
      - '#PassportNumber#'
      - '#SSN#'
      - '#CardNumber#'
      - '#CarNumber#'

# Training configuration - Same as baseline but shorter
training:
  num_train_epochs: 15  # Fewer epochs since postprocessing helps
  learning_rate: 1.0e-05
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 4
  gradient_checkpointing: true
  fp16: true
  warmup_steps: 10
  weight_decay: 0.01
  seed: 42
  
  # Evaluation settings
  evaluation_strategy: steps
  eval_steps: 400
  save_steps: 400
  save_total_limit: 1
  logging_steps: 100
  
  # Early stopping
  early_stopping_patience: 3
  early_stopping_threshold: 0.001
  load_best_model_at_end: true

# Generation configuration - Adjusted for postprocessing
generation:
  max_length: 250  # Generate longer, then trim in postprocessing
  num_beams: 5     # More beams for better quality
  no_repeat_ngram_size: 3  # Stricter repetition control
  early_stopping: true
  length_penalty: 0.8  # Prefer shorter outputs

# Inference configuration
inference:
  batch_size: 16
  generate_max_length: 250
  apply_postprocessing: true
  remove_tokens:
    - '<usr>'
    - '<s>'
    - '</s>'
    - '<pad>'

# WandB configuration
wandb:
  name: "02_postprocessing_aggressive"
  notes: "Aggressive postprocessing for maximum quality improvement"
  tags: ["postprocessing", "aggressive", "quality_focus", "kobart"]
  
# Data configuration
data:
  train_file: "train.csv"
  dev_file: "dev.csv"
  test_file: "test.csv"
