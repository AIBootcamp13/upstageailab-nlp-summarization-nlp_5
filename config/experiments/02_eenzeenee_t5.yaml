# eenzeenee T5 모델 실험
experiment_name: eenzeenee_t5
description: "eenzeenee T5 한국어 요약 모델 실험"

general:
  model_name: t5-base-korean-summarization
  data_path: data/
  train_path: data/train.csv
  val_path: data/dev.csv
  model_type: seq2seq
  name: eenzeenee_korean_summarization

model:
  architecture: t5
  checkpoint: eenzeenee/t5-base-korean-summarization

input_prefix: "summarize: "

tokenizer:
  bos_token: <pad>
  eos_token: </s>
  encoder_max_len: 512
  decoder_max_len: 64
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#PassportNumber#'

training:
  do_eval: true
  do_train: true
  eval_strategy: steps
  eval_steps: 400
  num_train_epochs: 5
  per_device_train_batch_size: 12     # RTX 3090 24GB 최적화
  per_device_eval_batch_size: 24      # 평가는 더 큰 배치
  learning_rate: 3.0e-05
  fp16: true
  gradient_checkpointing: false       # 메모리 여유로우므로 비활성화
  dataloader_num_workers: 8            # 48코어 활용
  dataloader_pin_memory: true          # 251GB RAM 활용
  group_by_length: true                # 효율성 증대
  logging_steps: 100
  save_strategy: steps
  save_steps: 400
  save_total_limit: 3
  load_best_model_at_end: true
  early_stopping_patience: 3

wandb:
  entity: lyjune37-juneictlab
  project: nlp-5
  name: eenzeenee_t5_rtx3090_lyj
  tags: [eenzeenee, T5, Korean, RTX3090, lyj]
  notes: "eenzeenee T5 한국어 모델 RTX 3090 최적화"
