experiment_name: "04_batch_optimization_safe"
base_config: "config.yaml"

general:
  experiment_name: "dialogue_summarization"
  model_name: "digit82/kobart-summarization"
  seed: 42
  device: "auto"

data:
  train_path: "data/train.csv"
  val_path: "data/dev.csv"
  test_path: "data/test.csv"
  # 특수 토큰 설정 - 안전 모드
  special_tokens:
    - "#PhoneNumber#"
    - "#Address#"
    - "#PassportNumber#"

tokenizer:
  encoder_max_len: 512
  decoder_max_len: 128

training:
  output_dir: "outputs/"
  num_train_epochs: 5
  per_device_train_batch_size: 6
  per_device_eval_batch_size: 12
  gradient_accumulation_steps: 3
  warmup_ratio: 0.1
  learning_rate: 3.0e-05
  weight_decay: 0.01
  gradient_checkpointing: true
  fp16: true
  fp16_opt_level: "O2"  # 더 공격적인 mixed precision
  save_steps: 500
  eval_steps: 500
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "rouge_combined_f1"
  logging_steps: 100
  dataloader_num_workers: 8  # 더 많은 워커
  dataloader_pin_memory: true

generation:
  max_length: 200
  min_length: 30
  num_beams: 4
  length_penalty: 1.0
  no_repeat_ngram_size: 3

evaluation:
  rouge_types: ["rouge1", "rouge2", "rougeL", "rougeLsum"]
  rouge_tokenize_korean: true

callbacks:
  early_stopping:
    enabled: true
    patience: 3

wandb:
  enabled: true
  entity: "lyjune37-juneictlab"
  project: "nlp-5"
  name: "batch_opt_kobart_safe"
  tags: ["batch_optimization", "kobart", "safe_mode"]

# Generation settings
generation:
  max_length: 256
  min_length: 5
  num_beams: 4
  no_repeat_ngram_size: 2
\ntraining:\n  eval_strategy: no
