# 고성능 학습률 실험
experiment_name: high_learning_rate
description: "높은 학습률로 빠른 수렴 실험"

general:
  model_name: digit82/kobart-summarization
  data_path: ../data/
  name: high_lr_experiment

model:
  architecture: BART
  checkpoint: digit82/kobart-summarization

tokenizer:
  bos_token: <s>
  eos_token: </s>
  encoder_max_len: 512
  decoder_max_len: 100

training:
  do_eval: true
  do_train: true
  evaluation_strategy: epoch
  num_train_epochs: 3  # 높은 학습률이므로 epoch 줄임
  per_device_train_batch_size: 16     # RTX 3090 24GB 최적화
  per_device_eval_batch_size: 32      # 높은 학습률에 맞는 배치
  learning_rate: 8.0e-05              # RTX 3090에서 더 높은 학습률 가능
  fp16: true
  warmup_ratio: 0.15                  # 높은 학습률에 맞는 웜업
  weight_decay: 0.01                   # 정규화 추가
  
  # RTX 3090 성능 최적화
  gradient_checkpointing: false        # 메모리 여유로우므로 비활성화
  dataloader_num_workers: 10           # 48코어 활용
  dataloader_pin_memory: true          # 251GB RAM 활용
  group_by_length: true                # 효율성 증대
  logging_steps: 50
  save_strategy: epoch
  save_total_limit: 2
  load_best_model_at_end: true
  early_stopping_patience: 2

wandb:
  entity: lyjune37-juneictlab
  project: nlp-5
  name: high_lr_rtx3090_lyj
  tags: [high_lr, fast_convergence, RTX3090, optimized, lyj]
  notes: "높은 학습률 RTX 3090 최적화 실험"
