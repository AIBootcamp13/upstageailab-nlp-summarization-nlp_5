experiment_name: "01_baseline_kobart_safe"
base_config: "config.yaml"

general:
  experiment_name: "dialogue_summarization"
  model_name: "digit82/kobart-summarization"
  seed: 42
  device: "auto"

data:
  train_path: "data/train.csv"
  val_path: "data/dev.csv"
  test_path: "data/test.csv"
  max_train_samples: null
  max_val_samples: null
  max_test_samples: null
  # 특수 토큰 설정 - KoBART를 위해 최소화
  special_tokens:
    - "#PhoneNumber#"
    - "#Address#"
    - "#PassportNumber#"
  # Person 토큰은 사용하지 않음

tokenizer:
  encoder_max_len: 512
  decoder_max_len: 128
  truncation: true
  padding: true

training:
  output_dir: "outputs/"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 4
  eval_accumulation_steps: 4
  warmup_ratio: 0.1
  warmup_steps: 0
  max_steps: -1
  learning_rate: 3.0e-05
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-08
  max_grad_norm: 1.0
  lr_scheduler_type: "cosine"
  gradient_checkpointing: true
  fp16: true
  fp16_opt_level: "O1"
  fp16_backend: "auto"
  dataloader_num_workers: 4
  save_steps: 500
  eval_steps: 500
  save_total_limit: 3
  no_cuda: false
  seed: 42
  local_rank: -1
  dataloader_drop_last: false
  eval_delay: 0
  dataloader_pin_memory: true
  remove_unused_columns: true
  label_names: null
  load_best_model_at_end: true
  metric_for_best_model: "rouge_combined_f1"
  greater_is_better: true
  ignore_data_skip: false
  sharded_ddp: null
  deepspeed: null
  label_smoothing_factor: 0.0
  adafactor: false
  length_column_name: "length"
  ddp_find_unused_parameters: false
  ddp_bucket_cap_mb: null
  skip_memory_metrics: true
  use_legacy_prediction_loop: false
  push_to_hub: false
  resume_from_checkpoint: null
  hub_model_id: null
  hub_strategy: "every_save"
  hub_token: null
  hub_private_repo: false
  save_on_each_node: false
  save_safetensors: true
  logging_dir: "logs/"
  logging_strategy: "steps"
  logging_first_step: false
  logging_steps: 100
  logging_nan_inf_filter: true
  evaluation_strategy: "steps"
  prediction_loss_only: false
  do_train: true
  do_eval: true
  do_predict: false
  jit_mode_eval: false

generation:
  max_length: 200
  min_length: 30
  do_sample: false
  early_stopping: true
  num_beams: 4
  num_beam_groups: 1
  diversity_penalty: 0.0
  temperature: 1.0
  top_k: 50
  top_p: 1.0
  repetition_penalty: 1.0
  length_penalty: 1.0
  no_repeat_ngram_size: 3
  encoder_repetition_penalty: 1.0
  bad_words_ids: null
  num_return_sequences: 1
  output_scores: false
  return_dict_in_generate: false
  forced_bos_token_id: null
  forced_eos_token_id: null
  remove_invalid_values: false

evaluation:
  rouge_types: ["rouge1", "rouge2", "rougeL", "rougeLsum"]
  rouge_use_stemmer: true
  rouge_use_aggregator: true
  rouge_tokenize_korean: true
  prediction_save_steps: 1000

qlora:
  use_qlora: false
  use_unsloth: false

callbacks:
  early_stopping:
    enabled: true
    patience: 3
    threshold: 0.001
    mode: "max"
    min_delta: 0.0

wandb:
  enabled: true
  entity: "lyjune37-juneictlab"
  project: "nlp-5"
  name: "baseline_kobart_safe"
  tags: ["baseline", "kobart", "safe_tokens"]
  group: "experiment_01"
  notes: "Safe KoBART baseline with minimal special tokens"

logging:
  log_level: "info"
  log_level_replica: "warning"
  log_on_each_node: true
  logging_nan_inf_filter: true
  save_log: true

# Generation settings
generation:
  max_length: 256
  min_length: 5
  num_beams: 4
  no_repeat_ngram_size: 2
\ntraining:\n  eval_strategy: no
