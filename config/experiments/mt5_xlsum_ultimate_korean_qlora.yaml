# mT5 XL-Sum RTX 3090 극한 최적화 실험 1단계 (수정된 버전)
experiment_name: mt5_xlsum_ultimate_korean_qlora
description: "mT5 XL-Sum RTX 3090 + Unsloth 최적화 실험 - 배치 16/24 + 시퀀스 1024"
general:
  model_name: csebuetnlp/mT5_multilingual_XLSum
  data_path: ./data/
  train_path: ./data/train.csv
  val_path: ./data/dev.csv
  test_path: ./data/test.csv  # 🔥 추론용 테스트 데이터 경로 추가
  model_type: seq2seq
  name: mt5_xlsum_ultimate_korean_qlora

model:
  architecture: mt5
  checkpoint: csebuetnlp/mT5_multilingual_XLSum

# 🔥 극한 최적화 한국어 대화 요약 특화 prefix (mT5는 single-task에서 prefix 불필요)
input_prefix: ""
tokenizer:
  eos_token: "</s>"       # mT5 종료 토큰
  unk_token: "<unk>"      # mT5 unknown 토큰  
  pad_token: "<pad>"      # mT5 패딩 토큰
  bos_token: ""            # mT5는 BOS 토큰 사용하지 않음 (빈 문자열)
  # mT5는 BOS 토큰 사용하지 않음
  encoder_max_len: 1024              # 1단계: 512→1024 확장
  decoder_max_len: 200               # 1단계: 128→200 확장
  
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#PassportNumber#'
    - '#DateOfBirth#'
    - '#SSN#'
    - '#CardNumber#'
    - '#CarNumber#'
    - '#Email#'
    - '<summary>'
    - '</summary>'
    - '<dialogue>'
    - '</dialogue>'

# 🔥 RTX 3090 + Unsloth 극한 최적화 1단계 학습 설정
training:
  do_eval: true
  do_train: true
  evaluation_strategy: steps
  eval_steps: 100                     # 더 자주 평가
  # 🔥 Unsloth 활성화로 메모리 효율성 극대화된 배치 설정
  per_device_train_batch_size: 16      # Unsloth로 12→16 안전하게 증가
  per_device_eval_batch_size: 24       # Unsloth로 16→24 안전하게 증가
  gradient_accumulation_steps: 2       # 유효 배치 32 달성
  
  
  # ⚡ 1단계 학습 파라미터
  num_train_epochs: 6                 # 최적화로 더 긴 학습
  learning_rate: 5.0e-05              # 8e-05→5e-05 안정성 강화
  lr_scheduler_type: cosine_with_restarts
  warmup_ratio: 0.02                  # 짧은 웜업
  weight_decay: 0.0001                # 세밀한 정규화
  
  fp16: false                         # bf16 사용으로 변경
  bf16: true                          # RTX 3090 + CUDA 12.2 최적
  tf32: true                          # Ampere 아키텍처 최적화
  gradient_checkpointing: true        # 메모리 절약을 위해 활성화
  dataloader_num_workers: 12           # Unsloth로 8→12 안전한 증가
  dataloader_pin_memory: true         # 251GB RAM 활용
  dataloader_persistent_workers: true # 워커 재사용
  group_by_length: true
  remove_unused_columns: false        # 전체 정보 활용
  remove_unused_columns: false        # 전체 정보 활용
  remove_unused_columns: false        # 전체 정보 활용
  
  # 🔥 Unsloth 특화 옵티마이저 설정
  optim: adamw_torch                   # Unsloth 호환 옵티마이저
  adam_beta1: 0.9
  adam_beta2: 0.95                    # Unsloth 추천값
  max_grad_norm: 0.3                  # Unsloth와 함께 낮은 값
  
  # 📊 모니터링
  logging_steps: 50
  save_strategy: steps
  save_steps: 200
  save_total_limit: 5
  load_best_model_at_end: true
  early_stopping_patience: 5
  
  # 🏆 1단계 극한 최적화 생성 설정
  predict_with_generate: true
  generation_num_beams: 4             # mT5 원본 모델 권장값: 4
  generation_max_length: 84           # mT5 원본 모델 권장값: 84
  generation_min_length: 10           # 기본값
  generation_length_penalty: 1.0      # 기본값
  generation_no_repeat_ngram_size: 2  # mT5 원본 모델 권장값: 2
  generation_do_sample: false         # 결정론적 생성
  generation_early_stopping: true     # 조기 중단 활성화
  
  report_to: wandb
  seed: 42

qlora:
  use_unsloth: true   # ✅ CRITICAL: Unsloth 활성화로 30-70% 메모리 절약
  use_qlora: true
  lora_rank: 64                       # 128→64 메모리 안전성 강화
  lora_alpha: 128                     # 256→128 메모리 안전성 강화
  lora_dropout: 0.01                  # 0.05→0.01 드롭아웃 감소
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "wi_0", "wi_1", "wo", "lm_head"]  # mT5 올바른 모듈명
  load_in_4bit: true
  bnb_4bit_compute_dtype: "bfloat16"   # RTX 3090 + CUDA 12.2 최적
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true
  
  wandb:
  entity: lyjune37-juneictlab
  project: nlp-5
  name: mt5_xlsum_unsloth_optimized_rtx3090
  tags: [mT5, XL-Sum, RTX3090, Unsloth, QLoRA, batch16, seq1024, optimized]
  notes: "mT5 XL-Sum RTX 3090 + Unsloth 최적화 - 배치 16/24, 시퀀스 1024, Unsloth QLoRA"
# 🏆 mT5 원본 모델 최적화 생성 설정
generation:
  max_length: 84                      # mT5 원본 모델 권장값
  min_length: 10                      # 기본값
  num_beams: 4                        # mT5 원본 모델 권장값
  length_penalty: 1.0                 # 기본값
  no_repeat_ngram_size: 2             # mT5 원본 모델 권장값
  early_stopping: true                # 조기 중단
  do_sample: false                    # 결정론적 생성

# 🚀 추가된 inference 섹션 (baseline.py 참조)
inference:
  no_repeat_ngram_size: 2             # generation과 동일값
  early_stopping: true               # generation과 동일값
  generate_max_length: 84             # generation의 max_length와 동일
  num_beams: 4                        # generation과 동일값
  batch_size: 16                       # 추론 배치 크기 (training과 일치)
  result_path: "./prediction/"        # baseline.py와 동일
  remove_tokens: ['<pad>', '</s>', '<unk>']  # mT5 올바른 제거 토큰들 (<s>는 유지)
  remove_tokens: ['<pad>', '</s>', '<unk>']  # mT5 올바른 제거 토큰들 (<s>는 유지)
