# mT5 XL-Sum RTX 3090 ê·¹í•œ ìµœì í™” ì‹¤í—˜ 1ë‹¨ê³„ (ìˆ˜ì •ëœ ë²„ì „)
experiment_name: mt5_xlsum_ultimate_korean_qlora
description: "mT5 XL-Sum RTX 3090 + Unsloth ìµœì í™” ì‹¤í—˜ - ë°°ì¹˜ 16/24 + ì‹œí€€ìŠ¤ 1024"
general:
  model_name: csebuetnlp/mT5_multilingual_XLSum
  data_path: ./data/
  train_path: ./data/train.csv
  val_path: ./data/dev.csv
  test_path: ./data/test.csv  # ğŸ”¥ ì¶”ë¡ ìš© í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ ì¶”ê°€
  model_type: seq2seq
  name: mt5_xlsum_ultimate_korean_qlora

model:
  architecture: mt5
  checkpoint: csebuetnlp/mT5_multilingual_XLSum

# ğŸ”¥ ê·¹í•œ ìµœì í™” í•œêµ­ì–´ ëŒ€í™” ìš”ì•½ íŠ¹í™” prefix (mT5ëŠ” single-taskì—ì„œ prefix ë¶ˆí•„ìš”)
input_prefix: ""
tokenizer:
  eos_token: "</s>"       # mT5 ì¢…ë£Œ í† í°
  unk_token: "<unk>"      # mT5 unknown í† í°  
  pad_token: "<pad>"      # mT5 íŒ¨ë”© í† í°
  bos_token: ""            # mT5ëŠ” BOS í† í° ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ë¹ˆ ë¬¸ìì—´)
  # mT5ëŠ” BOS í† í° ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
  encoder_max_len: 1024              # 1ë‹¨ê³„: 512â†’1024 í™•ì¥
  decoder_max_len: 200               # 1ë‹¨ê³„: 128â†’200 í™•ì¥
  
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#PassportNumber#'
    - '#DateOfBirth#'
    - '#SSN#'
    - '#CardNumber#'
    - '#CarNumber#'
    - '#Email#'
    - '<summary>'
    - '</summary>'
    - '<dialogue>'
    - '</dialogue>'

# ğŸ”¥ RTX 3090 + Unsloth ê·¹í•œ ìµœì í™” 1ë‹¨ê³„ í•™ìŠµ ì„¤ì •
training:
  do_eval: true
  do_train: true
  evaluation_strategy: steps
  eval_steps: 100                     # ë” ìì£¼ í‰ê°€
  # ğŸ”¥ Unsloth í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê·¹ëŒ€í™”ëœ ë°°ì¹˜ ì„¤ì •
  per_device_train_batch_size: 16      # Unslothë¡œ 12â†’16 ì•ˆì „í•˜ê²Œ ì¦ê°€
  per_device_eval_batch_size: 24       # Unslothë¡œ 16â†’24 ì•ˆì „í•˜ê²Œ ì¦ê°€
  gradient_accumulation_steps: 2       # ìœ íš¨ ë°°ì¹˜ 32 ë‹¬ì„±
  
  
  # âš¡ 1ë‹¨ê³„ í•™ìŠµ íŒŒë¼ë¯¸í„°
  num_train_epochs: 6                 # ìµœì í™”ë¡œ ë” ê¸´ í•™ìŠµ
  learning_rate: 5.0e-05              # 8e-05â†’5e-05 ì•ˆì •ì„± ê°•í™”
  lr_scheduler_type: cosine_with_restarts
  warmup_ratio: 0.02                  # ì§§ì€ ì›œì—…
  weight_decay: 0.0001                # ì„¸ë°€í•œ ì •ê·œí™”
  
  fp16: false                         # bf16 ì‚¬ìš©ìœ¼ë¡œ ë³€ê²½
  bf16: true                          # RTX 3090 + CUDA 12.2 ìµœì 
  tf32: true                          # Ampere ì•„í‚¤í…ì²˜ ìµœì í™”
  gradient_checkpointing: true        # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ í™œì„±í™”
  dataloader_num_workers: 12           # Unslothë¡œ 8â†’12 ì•ˆì „í•œ ì¦ê°€
  dataloader_pin_memory: true         # 251GB RAM í™œìš©
  dataloader_persistent_workers: true # ì›Œì»¤ ì¬ì‚¬ìš©
  group_by_length: true
  remove_unused_columns: false        # ì „ì²´ ì •ë³´ í™œìš©
  remove_unused_columns: false        # ì „ì²´ ì •ë³´ í™œìš©
  remove_unused_columns: false        # ì „ì²´ ì •ë³´ í™œìš©
  
  # ğŸ”¥ Unsloth íŠ¹í™” ì˜µí‹°ë§ˆì´ì € ì„¤ì •
  optim: adamw_torch                   # Unsloth í˜¸í™˜ ì˜µí‹°ë§ˆì´ì €
  adam_beta1: 0.9
  adam_beta2: 0.95                    # Unsloth ì¶”ì²œê°’
  max_grad_norm: 0.3                  # Unslothì™€ í•¨ê»˜ ë‚®ì€ ê°’
  
  # ğŸ“Š ëª¨ë‹ˆí„°ë§
  logging_steps: 50
  save_strategy: steps
  save_steps: 200
  save_total_limit: 5
  load_best_model_at_end: true
  early_stopping_patience: 5
  
  # ğŸ† 1ë‹¨ê³„ ê·¹í•œ ìµœì í™” ìƒì„± ì„¤ì •
  predict_with_generate: true
  generation_num_beams: 4             # mT5 ì›ë³¸ ëª¨ë¸ ê¶Œì¥ê°’: 4
  generation_max_length: 84           # mT5 ì›ë³¸ ëª¨ë¸ ê¶Œì¥ê°’: 84
  generation_min_length: 10           # ê¸°ë³¸ê°’
  generation_length_penalty: 1.0      # ê¸°ë³¸ê°’
  generation_no_repeat_ngram_size: 2  # mT5 ì›ë³¸ ëª¨ë¸ ê¶Œì¥ê°’: 2
  generation_do_sample: false         # ê²°ì •ë¡ ì  ìƒì„±
  generation_early_stopping: true     # ì¡°ê¸° ì¤‘ë‹¨ í™œì„±í™”
  
  report_to: wandb
  seed: 42

qlora:
  use_unsloth: true   # âœ… CRITICAL: Unsloth í™œì„±í™”ë¡œ 30-70% ë©”ëª¨ë¦¬ ì ˆì•½
  use_qlora: true
  lora_rank: 64                       # 128â†’64 ë©”ëª¨ë¦¬ ì•ˆì „ì„± ê°•í™”
  lora_alpha: 128                     # 256â†’128 ë©”ëª¨ë¦¬ ì•ˆì „ì„± ê°•í™”
  lora_dropout: 0.01                  # 0.05â†’0.01 ë“œë¡­ì•„ì›ƒ ê°ì†Œ
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "wi_0", "wi_1", "wo", "lm_head"]  # mT5 ì˜¬ë°”ë¥¸ ëª¨ë“ˆëª…
  load_in_4bit: true
  bnb_4bit_compute_dtype: "bfloat16"   # RTX 3090 + CUDA 12.2 ìµœì 
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true
  
  wandb:
  entity: lyjune37-juneictlab
  project: nlp-5
  name: mt5_xlsum_unsloth_optimized_rtx3090
  tags: [mT5, XL-Sum, RTX3090, Unsloth, QLoRA, batch16, seq1024, optimized]
  notes: "mT5 XL-Sum RTX 3090 + Unsloth ìµœì í™” - ë°°ì¹˜ 16/24, ì‹œí€€ìŠ¤ 1024, Unsloth QLoRA"
# ğŸ† mT5 ì›ë³¸ ëª¨ë¸ ìµœì í™” ìƒì„± ì„¤ì •
generation:
  max_length: 84                      # mT5 ì›ë³¸ ëª¨ë¸ ê¶Œì¥ê°’
  min_length: 10                      # ê¸°ë³¸ê°’
  num_beams: 4                        # mT5 ì›ë³¸ ëª¨ë¸ ê¶Œì¥ê°’
  length_penalty: 1.0                 # ê¸°ë³¸ê°’
  no_repeat_ngram_size: 2             # mT5 ì›ë³¸ ëª¨ë¸ ê¶Œì¥ê°’
  early_stopping: true                # ì¡°ê¸° ì¤‘ë‹¨
  do_sample: false                    # ê²°ì •ë¡ ì  ìƒì„±

# ğŸš€ ì¶”ê°€ëœ inference ì„¹ì…˜ (baseline.py ì°¸ì¡°)
inference:
  no_repeat_ngram_size: 2             # generationê³¼ ë™ì¼ê°’
  early_stopping: true               # generationê³¼ ë™ì¼ê°’
  generate_max_length: 84             # generationì˜ max_lengthì™€ ë™ì¼
  num_beams: 4                        # generationê³¼ ë™ì¼ê°’
  batch_size: 16                       # ì¶”ë¡  ë°°ì¹˜ í¬ê¸° (trainingê³¼ ì¼ì¹˜)
  result_path: "./prediction/"        # baseline.pyì™€ ë™ì¼
  remove_tokens: ['<pad>', '</s>', '<unk>']  # mT5 ì˜¬ë°”ë¥¸ ì œê±° í† í°ë“¤ (<s>ëŠ” ìœ ì§€)
  remove_tokens: ['<pad>', '</s>', '<unk>']  # mT5 ì˜¬ë°”ë¥¸ ì œê±° í† í°ë“¤ (<s>ëŠ” ìœ ì§€)
