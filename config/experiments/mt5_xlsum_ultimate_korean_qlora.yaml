# mT5 XL-Sum Unsloth QLoRA ìµœì í™” ì‹¤í—˜
experiment_name: mt5_xlsum_unsloth_qlora
description: "mT5 XL-Sum Unsloth + QLoRA ìµœì í™” ì‹¤í—˜ - ë°°ì¹˜ 8/12"
general:
  model_name: csebuetnlp/mT5_multilingual_XLSum
  data_path: ./data/
  train_path: ./data/train.csv
  val_path: ./data/dev.csv
  test_path: ./data/test.csv
  model_type: seq2seq
  name: mt5_xlsum_unsloth_qlora

model:
  architecture: mt5
  checkpoint: csebuetnlp/mT5_multilingual_XLSum
  # ğŸ”§ Unsloth í˜¸í™˜ì„± ì„¤ì •
  torch_dtype: bfloat16
  trust_remote_code: true
  use_safetensors: true              # safetensors ê°•ì œ ì‚¬ìš©

# í•œêµ­ì–´ ëŒ€í™” ìš”ì•½ íŠ¹í™” prefix
input_prefix: ""
tokenizer:
  eos_token: "</s>"       # mT5 ì¢…ë£Œ í† í°
  unk_token: "<unk>"      # mT5 unknown í† í°  
  pad_token: "<pad>"      # mT5 íŒ¨ë”© í† í°
  bos_token: ""           # mT5ëŠ” BOS í† í° ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
  encoder_max_len: 512    # ì•ˆì „ì„±ì„ ìœ„í•´ 512ë¡œ ì‹œì‘
  decoder_max_len: 128    # ì•ˆì „ì„±ì„ ìœ„í•´ 128ë¡œ ì‹œì‘
  
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#PassportNumber#'
    - '#DateOfBirth#'
    - '#SSN#'
    - '#CardNumber#'
    - '#CarNumber#'
    - '#Email#'
    - '<summary>'
    - '</summary>'
    - '<dialogue>'
    - '</dialogue>'

# ğŸ”¥ Unsloth + QLoRA ìµœì í™” í•™ìŠµ ì„¤ì •
training:
  do_eval: true
  do_train: true
  evaluation_strategy: steps
  eval_steps: 100
  
  # Unsloth ìµœì í™” ë°°ì¹˜ ì„¤ì •
  per_device_train_batch_size: 8       # ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸°
  per_device_eval_batch_size: 12       # ì•ˆì „í•œ í‰ê°€ ë°°ì¹˜
  gradient_accumulation_steps: 4       # ìœ íš¨ ë°°ì¹˜ 32 ìœ ì§€
  
  # í•™ìŠµ íŒŒë¼ë¯¸í„°
  num_train_epochs: 3                  # ì•ˆì „ì„±ì„ ìœ„í•´ 3 ì—í¬í¬
  learning_rate: 5.0e-05               # ì•ˆì •ì ì¸ í•™ìŠµë¥ 
  lr_scheduler_type: cosine_with_restarts
  warmup_ratio: 0.02
  weight_decay: 0.0001
  
  # RTX 3090 ìµœì í™” ì„¤ì •
  fp16: false                          # bf16 ì‚¬ìš©
  bf16: true                           # RTX 3090 ìµœì 
  tf32: true                           # Ampere ìµœì í™”
  gradient_checkpointing: true         # ë©”ëª¨ë¦¬ ì ˆì•½
  dataloader_num_workers: 8            # ì•ˆì „í•œ ì›Œì»¤ ìˆ˜
  dataloader_pin_memory: true
  dataloader_persistent_workers: true
  group_by_length: true
  remove_unused_columns: false
  
  # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
  optim: adamw_torch
  adam_beta1: 0.9
  adam_beta2: 0.95
  max_grad_norm: 0.3
  
  # ëª¨ë‹ˆí„°ë§
  logging_steps: 50
  save_strategy: steps
  save_steps: 200
  save_total_limit: 3
  load_best_model_at_end: true
  early_stopping_patience: 5
  
  # ìƒì„± ì„¤ì •
  predict_with_generate: true
  generation_num_beams: 4
  generation_max_length: 84
  generation_min_length: 10
  generation_length_penalty: 1.0
  generation_no_repeat_ngram_size: 2
  generation_do_sample: false
  generation_early_stopping: true
  
  report_to: wandb
  seed: 42

# ğŸ”¥ RTX 3090 + Unsloth ìµœì í™” QLoRA
qlora:
  use_unsloth: true   # âœ… CRITICAL: Unsloth í™œì„±í™”ë¡œ 30-70% ë©”ëª¨ë¦¬ ì ˆì•½
  use_qlora: true     # âœ… CRITICAL: QLoRA í™œì„±í™”
  lora_rank: 32                       # 64â†’32 ë©”ëª¨ë¦¬ ì•ˆì „ì„± ê°•í™”
  lora_alpha: 64                      # 128â†’64 ë©”ëª¨ë¦¬ ì•ˆì „ì„± ê°•í™”
  lora_dropout: 0.05                  # 0.01â†’0.05 ë“œë¡­ì•„ì›ƒ ì¦ê°€
  target_modules: ["q", "k", "v", "o", "wi_0", "wi_1", "wo", "lm_head"]  # mT5 ì •í™•í•œ ëª¨ë“ˆëª… (_proj ì—†ìŒ!)
  load_in_4bit: true                  # 4ë¹„íŠ¸ ì–‘ìí™” í™œì„±í™”
  bnb_4bit_compute_dtype: "bfloat16"   # RTX 3090 + CUDA 12.2 ìµœì 
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true     # ì´ì¤‘ ì–‘ìí™” í™œì„±í™”

wandb:
  entity: lyjune37-juneictlab
  project: nlp-5
  name: mt5_xlsum_unsloth_qlora
  tags: [mT5, XL-Sum, RTX3090, Unsloth, QLoRA, batch8]
  notes: "mT5 XL-Sum Unsloth + QLoRA ìµœì í™” ì‹¤í—˜"

# ìƒì„± ì„¤ì •
generation:
  max_length: 84
  min_length: 10
  num_beams: 4
  length_penalty: 1.0
  no_repeat_ngram_size: 2
  early_stopping: true
  do_sample: false

# ì¶”ë¡  ì„¤ì •
inference:
  no_repeat_ngram_size: 2
  early_stopping: true
  generate_max_length: 84
  num_beams: 4
  batch_size: 8
  result_path: "./prediction/"
  remove_tokens: ['<pad>', '</s>', '<unk>']
