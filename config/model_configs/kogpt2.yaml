# KoGPT2 Configuration
meta:
  experiment_name: kogpt2
  description: KoGPT2 model for Korean dialogue summarization

model:
  architecture: kogpt2
  checkpoint: skt/kogpt2-base-v2
  size: base

tokenizer:
  encoder_max_len: 512
  decoder_max_len: 128
  
data:
  train_path: data/train.csv
  val_path: data/dev.csv
  test_path: data/test.csv
  min_source_length: 10
  max_source_length: 1024
  min_target_length: 5
  max_target_length: 256

training:
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 4
  learning_rate: 5e-5
  num_train_epochs: 3
  warmup_ratio: 0.1
  weight_decay: 0.01
  lr_scheduler_type: cosine
  evaluation_strategy: steps
  eval_steps: 500
  save_strategy: steps
  save_steps: 500
  save_total_limit: 3
  load_best_model_at_end: true
  fp16: true
  gradient_checkpointing: true
  early_stopping_patience: 3
  dataloader_num_workers: 4

generation:
  max_length: 640  # encoder + decoder length for GPT
  num_beams: 4
  length_penalty: 1.0
  no_repeat_ngram_size: 3
  early_stopping: false
  do_sample: true
  temperature: 0.9
  top_k: 50
  top_p: 0.95

evaluation:
  rouge_use_stemmer: true
  rouge_tokenize_korean: true

general:
  seed: 42
  device: auto

logging:
  level: INFO
  
qlora:
  use_unsloth: false
  use_qlora: false
