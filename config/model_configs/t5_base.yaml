# T5 Base Configuration
meta:
  experiment_name: t5_base
  description: T5 base model for dialogue summarization

model:
  architecture: t5
  checkpoint: t5-base
  size: base

tokenizer:
  encoder_max_len: 512
  decoder_max_len: 128
  
data:
  train_path: data/train.csv
  val_path: data/dev.csv
  test_path: data/test.csv
  min_source_length: 10
  max_source_length: 1024
  min_target_length: 5
  max_target_length: 256

training:
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 2
  learning_rate: 3e-4
  num_train_epochs: 3
  warmup_ratio: 0.1
  weight_decay: 0.01
  lr_scheduler_type: linear
  evaluation_strategy: steps
  eval_steps: 500
  save_strategy: steps
  save_steps: 500
  save_total_limit: 3
  load_best_model_at_end: true
  fp16: true
  gradient_checkpointing: false
  early_stopping_patience: 3
  dataloader_num_workers: 4

generation:
  max_length: 128
  num_beams: 4
  length_penalty: 1.0
  no_repeat_ngram_size: 2
  early_stopping: true
  do_sample: false

evaluation:
  rouge_use_stemmer: true
  rouge_tokenize_korean: true

general:
  seed: 42
  device: auto

logging:
  level: INFO
  
qlora:
  use_unsloth: true  # AIStages RTX 3090 환경에서 활성화
  use_qlora: true
