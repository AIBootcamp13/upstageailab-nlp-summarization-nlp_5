# 배치 크기 최적화 실험
experiment_name: optimized_batch_size
description: "큰 배치 크기와 gradient accumulation 실험"

general:
  model_name: digit82/kobart-summarization
  data_path: ../data/
  name: batch_optimization

tokenizer:
  bos_token: <s>
  eos_token: </s>
  encoder_max_len: 512
  decoder_max_len: 100

training:
  do_eval: true
  do_train: true
  evaluation_strategy: steps
  eval_steps: 200
  num_train_epochs: 5
  per_device_train_batch_size: 4     # 메모리 고려하여 줄임
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 4     # 유효 배치 크기 16
  learning_rate: 2.0e-05
  fp16: true
  gradient_checkpointing: true       # 메모리 최적화
  dataloader_num_workers: 4
  logging_steps: 100
  save_strategy: steps
  save_steps: 200
  save_total_limit: 3
  load_best_model_at_end: true

wandb:
  entity: lyjune37-juneictlab
  project: nlp-5
  name: batch_optimization_lyj
  tags: [batch_optimization, memory_efficient, lyj]
