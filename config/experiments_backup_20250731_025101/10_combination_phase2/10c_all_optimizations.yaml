experiment_name: combination_phase2_all_optimizations
description: 2차 조합 실험 - 모든 최적화 기법 통합 (최종 성능 극대화)

# 모델 구성
model:
  name: digit82/kobart-summarization
  architecture: kobart

# 토크나이저 구성  
tokenizer:
  encoder_max_len: 512
  decoder_max_len: 200
  bos_token: "<s>"
  eos_token: "</s>"
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#DateOfBirth#'
    - '#PassportNumber#'
    - '#SSN#'
    - '#CardNumber#'
    - '#CarNumber#'
    - '#Email#'

# 학습 구성 (모든 최적화 통합)
training:
  num_train_epochs: 25  # 더 긴 학습
  learning_rate: 3.0e-05
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 4
  
  # Cosine Annealing with Restarts
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  
  weight_decay: 0.01
  seed: 42
  evaluation_strategy: epoch
  save_strategy: epoch
  save_total_limit: 3
  logging_steps: 50
  load_best_model_at_end: true
  metric_for_best_model: "eval_rouge_combined_f1"
  greater_is_better: true
  fp16: true
  gradient_checkpointing: true
  
  # 조기 종료
  early_stopping_patience: 5  # 더 많은 인내심
  early_stopping_threshold: 0.0005  # 더 엄격한 임계값
  
  # 데이터로더 설정
  dataloader_num_workers: 4
  dataloader_drop_last: false
  group_by_length: true
  predict_with_generate: true
  remove_unused_columns: true

# 특수 토큰 가중치 설정
token_weighting:
  enabled: true
  use_dynamic_weights: true  # 동적 가중치 사용
  weights:
    default: 1.0
    special_token_weight: 2.0
    pii_token_weight: 2.5
    speaker_token_weight: 2.0
  dynamic_config:
    start_epoch: 5
    warmup_epochs: 3
    max_weight_multiplier: 3.0
    schedule: "linear"  # linear, exponential, cosine
  pii_tokens:
    - '#PhoneNumber#'
    - '#Address#'
    - '#DateOfBirth#'
    - '#PassportNumber#'
    - '#SSN#'
    - '#CardNumber#'
    - '#CarNumber#'
    - '#Email#'
  speaker_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
  label_smoothing: 0.1
  temperature_scaling: 0.9

# 전처리 설정 (강화된 정규화)
preprocessing:
  enabled: true
  normalizer:
    normalize_spacing: true
    normalize_punctuation: true
    expand_abbreviations: true
    reduce_repetition: true
    max_repeat: 3
    normalize_emoticons: true
    preserve_special_tokens: true
    remove_filler_words: true  # 추가
    correct_typos: true  # 추가

# 데이터 증강 설정 (모든 증강 기법)
data_augmentation:
  enabled: true
  augmenters:
    # 간단한 증강
    - type: "SynonymReplacement"
      params:
        replace_ratio: 0.15
        preserve_special_tokens: true
        preserve_speakers: true
    - type: "SentenceReorder"
      params:
        reorder_ratio: 0.2
        preserve_speaker_order: true
        preserve_context_window: 3
    # 백트랜슬레이션
    - type: "BackTranslation"
      params:
        model_type: "google"
        source_lang: "ko"
        target_langs: ["en", "ja"]  # 다중 언어
        temperature: 0.9
        preserve_special_tokens: true
        preserve_structure: true
        batch_size: 32
        cache_translations: true
        cache_dir: "cache/backtranslation"
        quality_threshold: 0.7
    # 추가 증강 (선택적)
    - type: "ParaphraseGeneration"
      params:
        model: "kobart"
        temperature: 0.8
        max_paraphrases: 2
  augmentation_ratio: 0.5  # 50% 증강
  mixing_strategy: "balanced"  # 균형잡힌 혼합

# 후처리 설정 (강화된 후처리)
postprocessing:
  enabled: true
  processors:
    - type: "DuplicateRemover"
      params:
        similarity_threshold: 0.85  # 더 엄격
        preserve_important: true
        use_semantic_similarity: true
    - type: "LengthOptimizer"
      params:
        target_length_ratio: 0.9
        min_length: 30
        max_length: 180
        preserve_key_sentences: true
        adaptive_length: true  # 대화 길이에 따라 조정
    - type: "SpecialTokenValidator"
      params:
        ensure_all_tokens: true
        correct_format: true
        validate_consistency: true
    - type: "CoherenceChecker"  # 추가
      params:
        check_transitions: true
        fix_pronouns: true
  apply_to: ["validation", "test", "inference"]

# 생성 구성 (최적화된 빔 서치)
generation:
  max_length: 200
  min_length: 30
  num_beams: 5  # 증가
  num_beam_groups: 5  # Diverse Beam Search
  diversity_penalty: 0.5
  no_repeat_ngram_size: 3
  repetition_penalty: 1.2
  early_stopping: true
  length_penalty: 1.2
  temperature: 0.9
  do_sample: false
  top_k: 50
  top_p: 0.95

# 추론 구성
inference:
  batch_size: 16
  generate_max_length: 200
  ensemble_predictions: true  # 앙상블 예측
  num_return_sequences: 3  # 여러 후보 생성
  remove_tokens:
    - '<usr>'
    - '<s>'
    - '</s>'
    - '<pad>'

# WandB 구성
wandb:
  name: "10c_phase2_all_optimizations"
  notes: "조합 실험 2차: 모든 최적화 기법 통합 - 최종 성능 극대화"
  tags: ["combination", "phase2", "all_optimizations", "final", "kobart"]
  
# 데이터 구성
data:
  train_file: "train.csv"
  dev_file: "dev.csv"
  test_file: "test.csv"

# 모니터링 설정
monitoring:
  log_memory_usage: true
  log_training_speed: true
  log_augmentation_stats: true
  log_postprocessing_stats: true
  log_preprocessing_stats: true
  log_lr_curve: true
  log_token_weight_stats: true
  log_backtranslation_stats: true
  log_generation_diversity: true  # 생성 다양성 추적
  save_prediction_samples: true  # 예측 샘플 저장
