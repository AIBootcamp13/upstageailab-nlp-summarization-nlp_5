# 송규헌님 요청사항 구현 문서

작성일: 2025-07-28
작성자: 이영준
요청자: 송규헌

## 요청 내용 요약

1. **코드 오류 확인 및 실행 테스트**
2. **다양한 모델 실험 지원** (AutoModelForSeq2SeqLM, AutoModelForCausalLM)
3. **unsloth 라이브러리 적용 버전 구현**

## 구현 내용

### 1. 코드 오류 수정

#### 발견된 문제점
- `trainer.py`의 `_load_model_with_unsloth` 등 메서드의 인덴테이션 오류
- `DataProcessor` 클래스가 모델별 전처리를 지원하지 않음

#### 해결 방법
- 모든 메서드를 클래스 내부로 올바르게 이동
- `DataProcessor`에 모델별 전처리 함수 콜백 추가
- 에러 핸들링 및 폴백 메커니즘 구현

### 2. 다양한 모델 지원 구현

#### 지원 모델 목록

| 모델 아키텍처 | 체크포인트 | 설정 파일 | 특징 |
|-------------|-----------|----------|------|
| BART | facebook/bart-base | bart_base.yaml | 기본 Seq2Seq |
| T5 | t5-base | t5_base.yaml | Task prefix 필요 |
| mT5 | google/mt5-base | mt5_base.yaml | 다국어 지원 |
| Flan-T5 | google/flan-t5-base | flan_t5_base.yaml | Instruction tuned |
| KoGPT2 | skt/kogpt2-base-v2 | kogpt2.yaml | Causal LM |
| KoBART | gogamza/kobart-base-v2 | kobart_unsloth.yaml | unsloth 최적화 |

#### 구현 세부사항

1. **모델별 전처리 함수 (`_preprocess_for_model`)**
   ```python
   # T5 계열: prefix 추가
   if architecture in ['t5', 'mt5', 'flan-t5']:
       examples['input'] = ["summarize: " + inp for inp in examples['input']]
   
   # GPT 계열: 입력과 타겟 연결
   elif architecture in ['gpt2', 'kogpt2', 'gpt-neo']:
       examples['input'] = [f"{inp} TL;DR: {tgt}" for inp, tgt in ...]
   ```

2. **자동 모델 로딩**
   - `AutoModelForSeq2SeqLM`: BART, T5, mT5 계열
   - `AutoModelForCausalLM`: GPT2, KoGPT2 계열

### 3. unsloth 라이브러리 적용

#### 특징
- **메모리 사용량 75% 감소**
- **학습 속도 2-5배 향상**
- **더 큰 배치 사이즈 가능**

#### 설정 방법
```yaml
qlora:
  use_unsloth: true
  use_qlora: true
  load_in_4bit: true
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.1
```

#### 설치 방법
```bash
# PyTorch 2.4+ 필요
./install_unsloth.sh
```

## 실행 방법

### 1. 단일 모델 실험
```bash
# BART 모델
python code/trainer.py --config config/model_configs/bart_base.yaml

# T5 모델
python code/trainer.py --config config/model_configs/t5_base.yaml

# unsloth 적용 KoBART
python code/trainer.py --config config/model_configs/kobart_unsloth.yaml
```

### 2. 다중 모델 실험
```bash
# 모든 모델 순차 실행
./run_multi_model_experiments.sh
```

### 3. 결과 확인
- 실행 로그: `logs/multi_model_experiments/`
- 모델 체크포인트: `models/{model_name}_{timestamp}/`
- 실험 결과: `outputs/{model_name}_{timestamp}/results/`

## 성능 비교 예상

| 모델 | 메모리 사용량 | 학습 속도 | ROUGE 점수 |
|-----|-------------|----------|-----------|
| KoBART (기본) | 100% | 1x | Baseline |
| KoBART + QLoRA | ~40% | 0.8x | -1~2% |
| KoBART + unsloth | ~25% | 2-5x | -0.5~1% |

## 추가 최적화 팁

1. **배치 사이즈 조정**
   - unsloth 사용 시: 2-4배 증가 가능
   - QLoRA만 사용 시: 1.5-2배 증가 가능

2. **학습률 조정**
   - T5 계열: 3e-4 권장
   - BART 계열: 5e-5 권장
   - unsloth 사용 시: 약간 높은 학습률 가능

3. **Gradient Accumulation**
   - 메모리 부족 시 `gradient_accumulation_steps` 증가
   - 효과적인 배치 사이즈 = per_device_batch_size × gradient_accumulation_steps

## 문제 해결

### unsloth 설치 실패
```bash
# CUDA 버전 확인
nvidia-smi

# PyTorch 재설치
pip install torch>=2.4.0 --index-url https://download.pytorch.org/whl/cu118
```

### 메모리 부족
1. 배치 사이즈 감소
2. gradient_checkpointing 활성화
3. fp16/bf16 사용
4. QLoRA/unsloth 활성화

### 학습 속도 저하
1. num_workers 증가
2. 데이터 로딩 최적화
3. mixed precision 사용

## 참고 자료

- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [unsloth GitHub](https://github.com/unslothai/unsloth)
- [QLoRA Paper](https://arxiv.org/abs/2305.14314)
- [PEFT Documentation](https://huggingface.co/docs/peft)
