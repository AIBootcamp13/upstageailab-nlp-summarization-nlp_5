# 다양한 모델 실험 가이드

## 개요
이 문서는 NLP 대화 요약 프로젝트에서 다양한 모델을 실험하는 방법을 설명합니다.

## 지원 모델

### 1. Seq2Seq 모델 (Encoder-Decoder)

#### BART (Bidirectional and Auto-Regressive Transformers)
- **체크포인트**: `facebook/bart-base`, `facebook/bart-large`
- **특징**: 텍스트 생성에 강점, 노이즈 제거 사전학습
- **사용 예시**:
  ```yaml
  model:
    architecture: bart
    checkpoint: facebook/bart-base
  ```

#### T5 (Text-to-Text Transfer Transformer)
- **체크포인트**: `t5-small`, `t5-base`, `t5-large`
- **특징**: 모든 NLP 태스크를 text-to-text로 처리
- **주의사항**: Task prefix 필요 (`summarize:`)
- **사용 예시**:
  ```yaml
  model:
    architecture: t5
    checkpoint: t5-base
  ```

#### mT5 (Multilingual T5)
- **체크포인트**: `google/mt5-small`, `google/mt5-base`
- **특징**: 101개 언어 지원, 한국어 성능 우수
- **사용 예시**:
  ```yaml
  model:
    architecture: mt5
    checkpoint: google/mt5-base
  ```

#### Flan-T5
- **체크포인트**: `google/flan-t5-base`, `google/flan-t5-large`
- **특징**: Instruction tuning으로 성능 향상
- **사용 예시**:
  ```yaml
  model:
    architecture: flan-t5
    checkpoint: google/flan-t5-base
  ```

### 2. Causal LM 모델 (Decoder-only)

#### GPT-2
- **체크포인트**: `gpt2`, `gpt2-medium`, `gpt2-large`
- **특징**: 자동회귀 언어 모델
- **주의사항**: 입력과 출력을 연결하여 학습

#### KoGPT2
- **체크포인트**: `skt/kogpt2-base-v2`
- **특징**: 한국어 특화 GPT-2
- **사용 예시**:
  ```yaml
  model:
    architecture: kogpt2
    checkpoint: skt/kogpt2-base-v2
  ```

## 모델별 최적 설정

### 배치 사이즈 권장값

| 모델 | GPU 메모리 | 배치 사이즈 | Gradient Accumulation |
|-----|-----------|------------|---------------------|
| BART-base | 16GB | 8 | 2 |
| T5-base | 16GB | 8 | 2 |
| mT5-base | 16GB | 6 | 2 |
| Flan-T5-base | 16GB | 8 | 2 |
| KoGPT2 | 16GB | 4 | 4 |

### 학습률 권장값

| 모델 계열 | 학습률 | 스케줄러 |
|----------|--------|----------|
| BART | 5e-5 | linear |
| T5 | 3e-4 | linear |
| GPT | 5e-5 | cosine |

## 실험 실행 방법

### 1. 개별 모델 실험
```bash
# BART 실험
python code/trainer.py \
    --config config/model_configs/bart_base.yaml \
    --train-data data/train.csv \
    --val-data data/dev.csv

# T5 실험 (prefix 자동 추가됨)
python code/trainer.py \
    --config config/model_configs/t5_base.yaml \
    --train-data data/train.csv \
    --val-data data/dev.csv

# KoGPT2 실험 (Causal LM)
python code/trainer.py \
    --config config/model_configs/kogpt2.yaml \
    --train-data data/train.csv \
    --val-data data/dev.csv
```

### 2. 일괄 실험
```bash
# 모든 모델 순차 실행
./run_multi_model_experiments.sh
```

### 3. 커스텀 설정으로 실험
```bash
# 기존 설정 복사
cp config/model_configs/bart_base.yaml config/model_configs/my_bart.yaml

# 설정 수정 (vim, nano 등)
vim config/model_configs/my_bart.yaml

# 실행
python code/trainer.py --config config/model_configs/my_bart.yaml
```

## 메모리 최적화

### QLoRA 적용
```yaml
qlora:
  use_qlora: true
  load_in_4bit: true
  lora_rank: 16
  lora_alpha: 32
```

### unsloth 적용 (BART/T5 계열만)
```yaml
qlora:
  use_unsloth: true
  use_qlora: true
  load_in_4bit: true
```

### Gradient Checkpointing
```yaml
training:
  gradient_checkpointing: true
```

## 결과 분석

### 로그 위치
- 학습 로그: `logs/multi_model_experiments/`
- TensorBoard: `outputs/{model_name}/logs/`
- WandB: 자동 업로드 (설정 시)

### 메트릭 비교
```bash
# 결과 요약 확인
cat outputs/*/results/summary.txt | grep -A 5 "Best Metrics"

# CSV로 결과 추출
python scripts/collect_results.py --output results_comparison.csv
```

## 트러블슈팅

### 1. CUDA Out of Memory
- 배치 사이즈 감소
- Gradient accumulation 증가
- fp16 활성화
- QLoRA/unsloth 사용

### 2. 느린 학습 속도
- num_workers 증가 (CPU 코어 수에 맞게)
- 데이터 전처리 최적화
- Mixed precision training 사용

### 3. 모델별 특수 처리
- T5: "summarize:" prefix 자동 추가
- GPT: 입력과 출력 연결 처리
- mT5: 한국어 토크나이저 최적화

## 성능 향상 팁

1. **데이터 증강**
   - 패러프레이징
   - 역번역
   - 노이즈 추가

2. **하이퍼파라미터 튜닝**
   - WandB Sweep 사용
   - Optuna 통합
   - Grid/Random search

3. **앙상블**
   - 다중 모델 예측 결합
   - 가중 평균
   - Voting

## 참고 자료
- [Hugging Face Model Hub](https://huggingface.co/models)
- [Transformers Documentation](https://huggingface.co/docs/transformers)
- [모델별 Paper 링크](docs/references.md)
