"""
í† í° ë²”ìœ„ ê²€ì¦ ëª¨ë“ˆ

í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ê°„ì˜ vocabulary í˜¸í™˜ì„±ì„ ê²€ì¦í•˜ê³ 
í† í° ID ë²”ìœ„ ì´ˆê³¼ ë¬¸ì œë¥¼ ì‚¬ì „ì— ê°ì§€í•©ë‹ˆë‹¤.
"""

import logging
import torch
from typing import Dict, Any, List, Tuple, Optional
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import pandas as pd
from pathlib import Path

logger = logging.getLogger(__name__)


class TokenValidator:
    """í† í° ë²”ìœ„ ë° í˜¸í™˜ì„± ê²€ì¦ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str, config: Dict[str, Any]):
        """
        TokenValidator ì´ˆê¸°í™”
        
        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            config: ì‹¤í—˜ ì„¤ì •
        """
        self.model_name = model_name
        self.config = config
        self.tokenizer = None
        self.model = None
        self.validation_results = {}
        
    def load_tokenizer_and_model(self) -> Tuple[bool, str]:
        """
        í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
        
        Returns:
            (ì„±ê³µ ì—¬ë¶€, ì—ëŸ¬ ë©”ì‹œì§€)
        """
        try:
            logger.info(f"í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘: {self.model_name}")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            logger.info(f"ëª¨ë¸ ì •ë³´ ë¡œë“œ ì¤‘: {self.model_name}")
            # ëª¨ë¸ configë§Œ ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(
                self.model_name, 
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
            )
            
            return True, ""
            
        except Exception as e:
            error_msg = f"ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
            logger.error(error_msg)
            return False, error_msg
    
    def validate_vocabulary_compatibility(self) -> Dict[str, Any]:
        """
        vocabulary í˜¸í™˜ì„± ê²€ì¦
        
        Returns:
            ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.tokenizer or not self.model:
            return {"success": False, "error": "í† í¬ë‚˜ì´ì € ë˜ëŠ” ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ"}
        
        try:
            # í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì˜ vocabulary í¬ê¸° ë¹„êµ
            tokenizer_vocab_size = self.tokenizer.vocab_size
            model_vocab_size = self.model.config.vocab_size
            
            logger.info(f"í† í¬ë‚˜ì´ì € vocab_size: {tokenizer_vocab_size}")
            logger.info(f"ëª¨ë¸ vocab_size: {model_vocab_size}")
            
            # íŠ¹ìˆ˜ í† í° ê²€ì¦
            special_tokens = self.config.get('tokenizer', {}).get('special_tokens', [])
            valid_special_tokens = []
            invalid_special_tokens = []
            
            for token in special_tokens:
                token_id = self.tokenizer.convert_tokens_to_ids(token)
                if token_id < model_vocab_size:
                    valid_special_tokens.append((token, token_id))
                else:
                    invalid_special_tokens.append((token, token_id))
            
            # í˜¸í™˜ì„± íŒë‹¨
            is_compatible = (
                tokenizer_vocab_size <= model_vocab_size and
                len(invalid_special_tokens) == 0
            )
            
            result = {
                "success": True,
                "is_compatible": is_compatible,
                "tokenizer_vocab_size": tokenizer_vocab_size,
                "model_vocab_size": model_vocab_size,
                "vocab_size_match": tokenizer_vocab_size == model_vocab_size,
                "valid_special_tokens": valid_special_tokens,
                "invalid_special_tokens": invalid_special_tokens,
                "special_token_count": len(special_tokens),
                "warning_messages": []
            }
            
            # ê²½ê³  ë©”ì‹œì§€ ìƒì„±
            if tokenizer_vocab_size > model_vocab_size:
                result["warning_messages"].append(
                    f"âš ï¸ í† í¬ë‚˜ì´ì € vocab_size({tokenizer_vocab_size})ê°€ ëª¨ë¸ vocab_size({model_vocab_size})ë³´ë‹¤ í½ë‹ˆë‹¤"
                )
            
            if invalid_special_tokens:
                result["warning_messages"].append(
                    f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ íŠ¹ìˆ˜ í† í° {len(invalid_special_tokens)}ê°œ: {[t[0] for t in invalid_special_tokens]}"
                )
            
            return result
            
        except Exception as e:
            return {"success": False, "error": f"vocabulary ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}"}
    
    def validate_sample_data(self, sample_size: int = 100) -> Dict[str, Any]:
        """
        ìƒ˜í”Œ ë°ì´í„°ë¡œ í† í° ë²”ìœ„ ê²€ì¦
        
        Args:
            sample_size: ê²€ì¦í•  ìƒ˜í”Œ ìˆ˜
            
        Returns:
            ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.tokenizer or not self.model:
            return {"success": False, "error": "í† í¬ë‚˜ì´ì € ë˜ëŠ” ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ"}
        
        try:
            # ë°ì´í„° ë¡œë“œ
            train_path = self.config.get('general', {}).get('train_path', 'data/train.csv')
            if not Path(train_path).exists():
                return {"success": False, "error": f"í•™ìŠµ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_path}"}
            
            df = pd.read_csv(train_path)
            if len(df) == 0:
                return {"success": False, "error": "í•™ìŠµ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"}
            
            # ìƒ˜í”Œ ì„ íƒ
            sample_df = df.head(min(sample_size, len(df)))
            
            model_vocab_size = self.model.config.vocab_size
            max_token_id = 0
            min_token_id = float('inf')
            problematic_samples = []
            
            # ê° ìƒ˜í”Œ ê²€ì¦
            for idx, row in sample_df.iterrows():
                try:
                    # ì…ë ¥ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
                    input_text = str(row.get('input', row.get('dialogue', '')))
                    target_text = str(row.get('target', row.get('summary', '')))
                    
                    # ëª¨ë¸ë³„ prefix ì ìš©
                    input_prefix = self.config.get('input_prefix', '')
                    if input_prefix:
                        input_text = f"{input_prefix}{input_text}"
                    
                    # í† í¬ë‚˜ì´ì§•
                    input_tokens = self.tokenizer.encode(input_text, add_special_tokens=True)
                    target_tokens = self.tokenizer.encode(target_text, add_special_tokens=True)
                    
                    # í† í° ID ë²”ìœ„ í™•ì¸
                    all_tokens = input_tokens + target_tokens
                    sample_max = max(all_tokens) if all_tokens else 0
                    sample_min = min(all_tokens) if all_tokens else 0
                    
                    max_token_id = max(max_token_id, sample_max)
                    min_token_id = min(min_token_id, sample_min)
                    
                    # ë²”ìœ„ ì´ˆê³¼ í† í° ê°ì§€
                    invalid_tokens = [t for t in all_tokens if t >= model_vocab_size]
                    if invalid_tokens:
                        problematic_samples.append({
                            "sample_idx": idx,
                            "fname": row.get('fname', f'sample_{idx}'),
                            "invalid_token_ids": invalid_tokens,
                            "max_invalid_id": max(invalid_tokens),
                            "input_length": len(input_tokens),
                            "target_length": len(target_tokens)
                        })
                
                except Exception as e:
                    logger.warning(f"ìƒ˜í”Œ {idx} ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            # ê²°ê³¼ ìƒì„±
            is_valid = len(problematic_samples) == 0 and max_token_id < model_vocab_size
            
            result = {
                "success": True,
                "is_valid": is_valid,
                "samples_checked": len(sample_df),
                "model_vocab_size": model_vocab_size,
                "max_token_id_found": max_token_id,
                "min_token_id_found": min_token_id if min_token_id != float('inf') else 0,
                "problematic_samples_count": len(problematic_samples),
                "problematic_samples": problematic_samples[:10],  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                "token_range_exceeded": max_token_id >= model_vocab_size,
                "recommendations": []
            }
            
            # ê¶Œì¥ì‚¬í•­ ìƒì„±
            if max_token_id >= model_vocab_size:
                result["recommendations"].append(
                    f"ğŸš¨ í† í° ID ë²”ìœ„ ì´ˆê³¼ ê°ì§€! ìµœëŒ€ í† í° ID({max_token_id}) >= ëª¨ë¸ vocab_size({model_vocab_size})"
                )
                result["recommendations"].append(
                    "ğŸ’¡ í•´ê²°ì±…: íŠ¹ìˆ˜ í† í°ì„ ëª¨ë¸ vocabularyì— ì¶”ê°€í•˜ê±°ë‚˜ í† í¬ë‚˜ì´ì € ì¬ì„¤ì • í•„ìš”"
                )
            
            if len(problematic_samples) > 0:
                result["recommendations"].append(
                    f"âš ï¸ {len(problematic_samples)}ê°œ ìƒ˜í”Œì—ì„œ ë¬¸ì œ ë°œê²¬. ë°ì´í„° ì „ì²˜ë¦¬ í™•ì¸ í•„ìš”"
                )
            
            return result
            
        except Exception as e:
            return {"success": False, "error": f"ìƒ˜í”Œ ë°ì´í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}"}
    
    def run_full_validation(self) -> Dict[str, Any]:
        """
        ì „ì²´ ê²€ì¦ ì‹¤í–‰
        
        Returns:
            ì¢…í•© ê²€ì¦ ê²°ê³¼
        """
        logger.info(f"ğŸ” í† í° ê²€ì¦ ì‹œì‘: {self.model_name}")
        
        # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        load_success, load_error = self.load_tokenizer_and_model()
        if not load_success:
            return {
                "success": False,
                "error": load_error,
                "stage": "model_loading"
            }
        
        # 2. Vocabulary í˜¸í™˜ì„± ê²€ì¦
        vocab_result = self.validate_vocabulary_compatibility()
        if not vocab_result["success"]:
            return {
                "success": False,
                "error": vocab_result["error"],
                "stage": "vocabulary_validation"
            }
        
        # 3. ìƒ˜í”Œ ë°ì´í„° ê²€ì¦
        sample_result = self.validate_sample_data()
        if not sample_result["success"]:
            return {
                "success": False,
                "error": sample_result["error"],
                "stage": "sample_validation"
            }
        
        # 4. ì¢…í•© ê²°ê³¼
        overall_valid = (
            vocab_result["is_compatible"] and 
            sample_result["is_valid"]
        )
        
        result = {
            "success": True,
            "overall_valid": overall_valid,
            "model_name": self.model_name,
            "vocabulary_validation": vocab_result,
            "sample_validation": sample_result,
            "summary": {
                "vocab_compatible": vocab_result["is_compatible"],
                "sample_data_valid": sample_result["is_valid"],
                "can_proceed": overall_valid
            },
            "recommendations": []
        }
        
        # ì „ì²´ ê¶Œì¥ì‚¬í•­ ìˆ˜ì§‘
        all_recommendations = []
        if vocab_result.get("warning_messages"):
            all_recommendations.extend(vocab_result["warning_messages"])
        if sample_result.get("recommendations"):
            all_recommendations.extend(sample_result["recommendations"])
        
        result["recommendations"] = all_recommendations
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if self.model:
            del self.model
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        logger.info(f"âœ… í† í° ê²€ì¦ ì™„ë£Œ: {'í†µê³¼' if overall_valid else 'ì‹¤íŒ¨'}")
        
        return result


def validate_model_tokenizer_compatibility(model_name: str, config: Dict[str, Any]) -> Dict[str, Any]:
    """
    ëª¨ë¸-í† í¬ë‚˜ì´ì € í˜¸í™˜ì„± ê²€ì¦ í¸ì˜ í•¨ìˆ˜
    
    Args:
        model_name: ëª¨ë¸ ì´ë¦„
        config: ì‹¤í—˜ ì„¤ì •
        
    Returns:
        ê²€ì¦ ê²°ê³¼
    """
    validator = TokenValidator(model_name, config)
    return validator.run_full_validation()


def fix_token_range_issues(model_name: str, config: Dict[str, Any]) -> Dict[str, Any]:
    """
    í† í° ë²”ìœ„ ë¬¸ì œ ìë™ ìˆ˜ì •
    
    Args:
        model_name: ëª¨ë¸ ì´ë¦„
        config: ì‹¤í—˜ ì„¤ì •
        
    Returns:
        ìˆ˜ì •ëœ ì„¤ì •ê³¼ ê²°ê³¼
    """
    validator = TokenValidator(model_name, config)
    validation_result = validator.run_full_validation()
    
    if validation_result["success"] and not validation_result["overall_valid"]:
        fixed_config = config.copy()
        
        # íŠ¹ìˆ˜ í† í° ì œê±° ë˜ëŠ” ìˆ˜ì •
        vocab_result = validation_result["vocabulary_validation"]
        if vocab_result.get("invalid_special_tokens"):
            logger.warning("ìœ íš¨í•˜ì§€ ì•Šì€ íŠ¹ìˆ˜ í† í° ì œê±°")
            invalid_tokens = [t[0] for t in vocab_result["invalid_special_tokens"]]
            current_special_tokens = fixed_config.get("tokenizer", {}).get("special_tokens", [])
            fixed_special_tokens = [t for t in current_special_tokens if t not in invalid_tokens]
            
            if "tokenizer" not in fixed_config:
                fixed_config["tokenizer"] = {}
            fixed_config["tokenizer"]["special_tokens"] = fixed_special_tokens
        
        # ê¸¸ì´ ì„¤ì • ì¡°ì • (í† í° ë²”ìœ„ ë¬¸ì œê°€ ìˆëŠ” ê²½ìš°)
        sample_result = validation_result["sample_validation"]
        if sample_result.get("token_range_exceeded"):
            logger.warning("í† í° ê¸¸ì´ ì„¤ì • ë³´ìˆ˜ì ìœ¼ë¡œ ì¡°ì •")
            current_encoder_len = fixed_config.get("tokenizer", {}).get("encoder_max_len", 1024)
            current_decoder_len = fixed_config.get("tokenizer", {}).get("decoder_max_len", 200)
            
            # ë³´ìˆ˜ì ìœ¼ë¡œ ê¸¸ì´ ì¤„ì´ê¸°
            safe_encoder_len = min(current_encoder_len, 512)
            safe_decoder_len = min(current_decoder_len, 128)
            
            fixed_config["tokenizer"]["encoder_max_len"] = safe_encoder_len
            fixed_config["tokenizer"]["decoder_max_len"] = safe_decoder_len
        
        return {
            "success": True,
            "config_modified": True,
            "original_config": config,
            "fixed_config": fixed_config,
            "validation_result": validation_result
        }
    
    return {
        "success": validation_result["success"],
        "config_modified": False,
        "fixed_config": config,
        "validation_result": validation_result
    }
