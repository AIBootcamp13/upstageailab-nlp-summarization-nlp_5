"""
ë©”ëª¨ë¦¬ ê²€ì¦ ë° ê´€ë¦¬ ëª¨ë“ˆ

ì‹¤í—˜ ì „ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ì„ ì¶”ì •í•˜ê³  ì‹¤í—˜ê°„ ë©”ëª¨ë¦¬ë¥¼ ì™„ì „íˆ ì •ë¦¬í•©ë‹ˆë‹¤.
"""

import gc
import logging
import torch
import psutil
from typing import Dict, Any, Optional, Tuple
import subprocess
import json
from pathlib import Path

logger = logging.getLogger(__name__)


class MemoryValidator:
    """ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ê²€ì¦ ë° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """MemoryValidator ì´ˆê¸°í™”"""
        self.gpu_available = torch.cuda.is_available()
        self.device_info = self._get_device_info()
        
    def _get_device_info(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘"""
        info = {
            "cpu_cores": psutil.cpu_count(),
            "ram_total_gb": psutil.virtual_memory().total / (1024**3),
            "ram_available_gb": psutil.virtual_memory().available / (1024**3),
            "gpu_available": self.gpu_available
        }
        
        if self.gpu_available:
            info["gpu_count"] = torch.cuda.device_count()
            info["gpu_memory_gb"] = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            info["gpu_name"] = torch.cuda.get_device_name(0)
            info["cuda_version"] = torch.version.cuda
        
        return info
    
    def cleanup_gpu_memory(self) -> Dict[str, Any]:
        """
        GPU ë©”ëª¨ë¦¬ ì™„ì „ ì •ë¦¬
        
        Returns:
            ì •ë¦¬ ê²°ê³¼
        """
        cleanup_result = {
            "success": False,
            "memory_before_mb": 0,
            "memory_after_mb": 0,
            "memory_freed_mb": 0
        }
        
        if not self.gpu_available:
            cleanup_result["success"] = True
            cleanup_result["message"] = "GPU ì—†ìŒ - ì •ë¦¬ ë¶ˆí•„ìš”"
            return cleanup_result
        
        try:
            # ì •ë¦¬ ì „ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            memory_before = torch.cuda.memory_allocated() / (1024**2)
            cleanup_result["memory_before_mb"] = memory_before
            
            # 1. PyTorch ìºì‹œ ì •ë¦¬
            torch.cuda.empty_cache()
            
            # 2. GPU ë™ê¸°í™”
            torch.cuda.synchronize()
            
            # 3. Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            # 4. PyTorch ë©”ëª¨ë¦¬ ê°•ì œ í•´ì œ
            if torch.cuda.is_available():
                # ëª¨ë“  GPU ë””ë°”ì´ìŠ¤ì˜ ë©”ëª¨ë¦¬ ì •ë¦¬
                for i in range(torch.cuda.device_count()):
                    with torch.cuda.device(i):
                        torch.cuda.empty_cache()
                        torch.cuda.ipc_collect()
            
            # ì •ë¦¬ í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            memory_after = torch.cuda.memory_allocated() / (1024**2)
            cleanup_result["memory_after_mb"] = memory_after
            cleanup_result["memory_freed_mb"] = memory_before - memory_after
            cleanup_result["success"] = True
            
            logger.info(f"ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: {cleanup_result['memory_freed_mb']:.1f}MB í•´ì œ")
            
        except Exception as e:
            cleanup_result["error"] = str(e)
            logger.error(f"GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        
        return cleanup_result
    
    def estimate_memory_requirements(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì‹¤í—˜ ì„¤ì •ì— ë”°ë¥¸ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ì¶”ì •
        
        Args:
            config: ì‹¤í—˜ ì„¤ì •
            
        Returns:
            ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ì¶”ì • ê²°ê³¼
        """
        model_name = config.get('general', {}).get('model_name', '')
        
        # ëª¨ë¸ë³„ ê¸°ë³¸ ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ (GB)
        model_base_memory = {
            'kobart': 2.0,      # KoBART (ì‘ì€ ëª¨ë¸)
            't5-base': 3.0,     # T5-base
            't5-large': 8.0,    # T5-large  
            'mt5': 4.0,         # mT5
            'eenzeenee': 3.0    # eenzeenee T5-base Korean
        }
        
        # ëª¨ë¸ íƒ€ì… ê°ì§€
        model_type = 'kobart'
        model_name_lower = model_name.lower()
        if 'eenzeenee' in model_name_lower:
            model_type = 'eenzeenee'
        elif 'mt5' in model_name_lower:
            model_type = 'mt5'
        elif 't5-large' in model_name_lower:
            model_type = 't5-large'
        elif 't5' in model_name_lower:
            model_type = 't5-base'
        elif 'bart' in model_name_lower or 'kobart' in model_name_lower:
            model_type = 'kobart'
        
        base_memory = model_base_memory.get(model_type, 3.0)
        
        # ì„¤ì •ë³„ ë©”ëª¨ë¦¬ ì¦ê°€ ê³„ì‚°
        training_config = config.get('training', {})
        batch_size = training_config.get('per_device_train_batch_size', 8)
        encoder_len = config.get('tokenizer', {}).get('encoder_max_len', 1024)
        decoder_len = config.get('tokenizer', {}).get('decoder_max_len', 200)
        
        # ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ ê³„ì‚°
        # ê³µì‹: base_memory * batch_multiplier * sequence_multiplier
        batch_multiplier = max(1.0, batch_size / 8)  # ë°°ì¹˜ 8 ê¸°ì¤€
        sequence_multiplier = max(1.0, (encoder_len + decoder_len) / 1200)  # 1200 í† í° ê¸°ì¤€
        
        estimated_memory_gb = base_memory * batch_multiplier * sequence_multiplier
        
        # QLoRA/Unsloth ì‚¬ìš©ì‹œ ë©”ëª¨ë¦¬ ì ˆì•½
        qlora_config = config.get('qlora', {})
        if qlora_config.get('use_qlora', False):
            estimated_memory_gb *= 0.4  # QLoRAëŠ” ì•½ 60% ë©”ëª¨ë¦¬ ì ˆì•½
        if qlora_config.get('use_unsloth', False):
            estimated_memory_gb *= 0.7  # UnslothëŠ” ì¶”ê°€ 30% ì ˆì•½
        
        # ê²°ê³¼ ìƒì„±
        result = {
            "model_type": model_type,
            "model_name": model_name,
            "base_memory_gb": base_memory,
            "batch_size": batch_size,
            "sequence_length": encoder_len + decoder_len,
            "estimated_memory_gb": estimated_memory_gb,
            "batch_multiplier": batch_multiplier,
            "sequence_multiplier": sequence_multiplier,
            "use_qlora": qlora_config.get('use_qlora', False),
            "use_unsloth": qlora_config.get('use_unsloth', False),
            "available_memory_gb": self.device_info.get("gpu_memory_gb", 0),
            "memory_sufficient": False,
            "recommendations": []
        }
        
        # ë©”ëª¨ë¦¬ ì¶©ë¶„ì„± íŒë‹¨
        available_memory = self.device_info.get("gpu_memory_gb", 0)
        if available_memory > 0:
            # 80% ì•ˆì „ ë§ˆì§„ ì ìš©
            safe_memory = available_memory * 0.8
            result["memory_sufficient"] = estimated_memory_gb <= safe_memory
            result["memory_utilization_percent"] = (estimated_memory_gb / available_memory) * 100
            
            # ê¶Œì¥ì‚¬í•­ ìƒì„±
            if not result["memory_sufficient"]:
                overage = estimated_memory_gb - safe_memory
                result["recommendations"].append(
                    f"ğŸš¨ ë©”ëª¨ë¦¬ ë¶€ì¡±: {overage:.1f}GB ì´ˆê³¼ (ìš”êµ¬: {estimated_memory_gb:.1f}GB, ì•ˆì „ í•œê³„: {safe_memory:.1f}GB)"
                )
                
                # ë°°ì¹˜ í¬ê¸° ì¡°ì • ê¶Œì¥
                safe_batch_size = max(1, int(batch_size * safe_memory / estimated_memory_gb))
                result["recommendations"].append(
                    f"ğŸ’¡ ë°°ì¹˜ í¬ê¸°ë¥¼ {batch_size} â†’ {safe_batch_size}ë¡œ ì¤„ì´ì„¸ìš”"
                )
                
                # ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì • ê¶Œì¥
                if encoder_len > 512:
                    result["recommendations"].append(
                        f"ğŸ’¡ encoder_max_lenì„ {encoder_len} â†’ 512ë¡œ ì¤„ì´ì„¸ìš”"
                    )
                
                # QLoRA ì‚¬ìš© ê¶Œì¥
                if not qlora_config.get('use_qlora', False):
                    result["recommendations"].append(
                        "ğŸ’¡ QLoRAë¥¼ í™œì„±í™”í•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ 60% ì ˆì•½í•˜ì„¸ìš”"
                    )
            else:
                result["recommendations"].append(
                    f"âœ… ë©”ëª¨ë¦¬ ì¶©ë¶„: {result['memory_utilization_percent']:.1f}% ì‚¬ìš©ì˜ˆì •"
                )
        
        return result
    
    def get_safe_batch_size(self, config: Dict[str, Any]) -> int:
        """
        ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸° ê³„ì‚°
        
        Args:
            config: ì‹¤í—˜ ì„¤ì •
            
        Returns:
            ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸°
        """
        memory_est = self.estimate_memory_requirements(config)
        
        if memory_est["memory_sufficient"]:
            return config.get('training', {}).get('per_device_train_batch_size', 8)
        
        # ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸° ê³„ì‚°
        current_batch = config.get('training', {}).get('per_device_train_batch_size', 8)
        available_memory = self.device_info.get("gpu_memory_gb", 8) * 0.8
        required_memory = memory_est["estimated_memory_gb"]
        
        if required_memory > 0:
            safe_batch = max(1, int(current_batch * available_memory / required_memory))
            return safe_batch
        
        return max(1, current_batch // 2)
    
    def auto_adjust_config_for_memory(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë©”ëª¨ë¦¬ì— ë§ê²Œ ì„¤ì • ìë™ ì¡°ì •
        
        Args:
            config: ì›ë³¸ ì„¤ì •
            
        Returns:
            ì¡°ì •ëœ ì„¤ì •
        """
        adjusted_config = config.copy()
        memory_est = self.estimate_memory_requirements(config)
        
        if memory_est["memory_sufficient"]:
            logger.info("âœ… ë©”ëª¨ë¦¬ ì¶©ë¶„ - ì„¤ì • ì¡°ì • ë¶ˆí•„ìš”")
            return adjusted_config
        
        logger.warning("âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡± ê°ì§€ - ì„¤ì • ìë™ ì¡°ì •")
        
        # 1. ë°°ì¹˜ í¬ê¸° ì¡°ì •
        safe_batch = self.get_safe_batch_size(config)
        if 'training' not in adjusted_config:
            adjusted_config['training'] = {}
        adjusted_config['training']['per_device_train_batch_size'] = safe_batch
        
        # í‰ê°€ ë°°ì¹˜ëŠ” ë” í¬ê²Œ ì„¤ì • ê°€ëŠ¥
        eval_batch = min(safe_batch * 2, 32)
        adjusted_config['training']['per_device_eval_batch_size'] = eval_batch
        
        logger.info(f"ğŸ“‰ ë°°ì¹˜ í¬ê¸° ì¡°ì •: {config.get('training', {}).get('per_device_train_batch_size', 8)} â†’ {safe_batch}")
        
        # 2. ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì • (í•„ìš”ì‹œ)
        tokenizer_config = adjusted_config.get('tokenizer', {})
        current_encoder_len = tokenizer_config.get('encoder_max_len', 1024)
        
        if current_encoder_len > 1024:
            adjusted_config['tokenizer']['encoder_max_len'] = 512
            logger.info(f"ğŸ“‰ Encoder ê¸¸ì´ ì¡°ì •: {current_encoder_len} â†’ 512")
        
        # 3. ë°ì´í„°ë¡œë” ì›Œì»¤ ìˆ˜ ì¡°ì •
        cpu_cores = self.device_info.get("cpu_cores", 4)
        safe_workers = min(cpu_cores // 2, 16)  # ë³´ìˆ˜ì  ì„¤ì •
        adjusted_config['training']['dataloader_num_workers'] = safe_workers
        
        # 4. QLoRA ê°•ì œ í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
        if 'qlora' not in adjusted_config:
            adjusted_config['qlora'] = {}
        
        adjusted_config['qlora']['use_qlora'] = True
        adjusted_config['qlora']['load_in_4bit'] = True
        
        logger.info("ğŸ”§ QLoRA ê°•ì œ í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½")
        
        return adjusted_config


def estimate_memory_requirements(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ì¶”ì • í¸ì˜ í•¨ìˆ˜
    
    Args:
        config: ì‹¤í—˜ ì„¤ì •
        
    Returns:
        ë©”ëª¨ë¦¬ ì¶”ì • ê²°ê³¼
    """
    validator = MemoryValidator()
    return validator.estimate_memory_requirements(config)


def cleanup_between_experiments() -> bool:
    """
    ì‹¤í—˜ê°„ ë©”ëª¨ë¦¬ ì™„ì „ ì •ë¦¬
    
    Returns:
        ì •ë¦¬ ì„±ê³µ ì—¬ë¶€
    """
    validator = MemoryValidator()
    result = validator.cleanup_gpu_memory()
    return result["success"]


def auto_fix_memory_config(config: Dict[str, Any]) -> Tuple[Dict[str, Any], bool]:
    """
    ë©”ëª¨ë¦¬ ë¬¸ì œ ìë™ ìˆ˜ì •
    
    Args:
        config: ì›ë³¸ ì„¤ì •
        
    Returns:
        (ìˆ˜ì •ëœ ì„¤ì •, ìˆ˜ì • ì—¬ë¶€)
    """
    validator = MemoryValidator()
    original_sufficient = validator.estimate_memory_requirements(config)["memory_sufficient"]
    
    if original_sufficient:
        return config, False
    
    fixed_config = validator.auto_adjust_config_for_memory(config)
    return fixed_config, True
