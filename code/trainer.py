"""
NLP ëŒ€í™” ìš”ì•½ ëª¨ë¸ í•™ìŠµ ëª¨ë“ˆ

baseline.ipynbì˜ í•µì‹¬ í•™ìŠµ ë¡œì§ì„ ëª¨ë“ˆí™”í•œ íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤.
WandB Sweepê³¼ì˜ í†µí•©ì„ ìœ„í•´ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ ì„¤ì •ì„ ì§€ì›í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union
from dataclasses import dataclass, field
import warnings
warnings.filterwarnings("ignore")

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm

from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    AutoModelForCausalLM,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    EarlyStoppingCallback,
    TrainerCallback,
    TrainerState,
    TrainerControl,
    PreTrainedModel,
    PreTrainedTokenizer
)

# QLoRA ë° unsloth ê´€ë ¨ import (ì„ íƒì )
try:
    from unsloth import FastLanguageModel
    from peft import LoraConfig, get_peft_model, TaskType
    UNSLOTH_AVAILABLE = True
except ImportError:
    # macOS í™˜ê²½ì´ë‚˜ unslothê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°
    FastLanguageModel = None
    LoraConfig = None
    get_peft_model = None
    TaskType = None
    UNSLOTH_AVAILABLE = False

from datasets import Dataset, DatasetDict
import evaluate
import wandb
# ë¡œì»¬ ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
try:
    from utils import load_config
    from utils.data_utils import DataProcessor
    from utils.metrics import RougeCalculator
    from utils.experiment_utils import ExperimentTracker, ModelRegistry
    from utils.environment_detector import EnvironmentDetector, get_auto_config, should_use_unsloth
    from utils.path_utils import PathManager, path_manager
    from utils.wandb_utils import setup_wandb_for_experiment, log_model_to_wandb
except ImportError:
    # code ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰ë˜ëŠ” ê²½ìš°
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from utils import load_config
    from utils.data_utils import DataProcessor
    from utils.metrics import RougeCalculator
    from utils.experiment_utils import ExperimentTracker, ModelRegistry
    from utils.environment_detector import EnvironmentDetector, get_auto_config, should_use_unsloth
    from utils.path_utils import PathManager, path_manager
    from utils.wandb_utils import setup_wandb_for_experiment, log_model_to_wandb
logger = logging.getLogger(__name__)


@dataclass
class TrainingResult:
    """í•™ìŠµ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    model_path: str
    best_metrics: Dict[str, float]
    final_metrics: Dict[str, float]
    wandb_run_id: Optional[str] = None
    experiment_id: Optional[str] = None
    config_used: Optional[Dict[str, Any]] = None
    submission_path: Optional[str] = None  # ì œì¶œ íŒŒì¼ ê²½ë¡œ ì¶”ê°€


class WandbCallback(TrainerCallback):
    """WandB ë¡œê¹…ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ì½œë°±"""
    
    def __init__(self, trainer_instance: 'DialogueSummarizationTrainer') -> None:
        self.trainer_instance = trainer_instance
        self.best_metrics = {}
        
    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, 
                   metrics: Dict[str, float], **kwargs):
        """í‰ê°€ ì‹œ WandBì— ë©”íŠ¸ë¦­ ë¡œê¹…"""
        if wandb.run is not None:
            # ROUGE ì ìˆ˜ ê²°í•© (F1 ê¸°ì¤€)
            rouge_combined = (
                metrics.get('eval_rouge1', 0) * 0.33 +
                metrics.get('eval_rouge2', 0) * 0.33 +
                metrics.get('eval_rougeL', 0) * 0.34
            )
            
            log_metrics = {
                'eval/rouge1_f1': metrics.get('eval_rouge1', 0),
                'eval/rouge2_f1': metrics.get('eval_rouge2', 0),
                'eval/rougeL_f1': metrics.get('eval_rougeL', 0),
                'eval/rouge_combined_f1': rouge_combined,
                'eval/loss': metrics.get('eval_loss', 0),
                'epoch': state.epoch,
                'step': state.global_step
            }
            
            # ë² ìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            if rouge_combined > self.best_metrics.get('rouge_combined_f1', 0):
                self.best_metrics = {
                    'rouge1_f1': metrics.get('eval_rouge1', 0),
                    'rouge2_f1': metrics.get('eval_rouge2', 0),
                    'rougeL_f1': metrics.get('eval_rougeL', 0),
                    'rouge_combined_f1': rouge_combined,
                    'loss': metrics.get('eval_loss', 0)
                }
                log_metrics['best/rouge_combined_f1'] = rouge_combined
            
            wandb.log(log_metrics)
            
            # ì‹¤í—˜ ì¶”ì ê¸°ì—ë„ ë¡œê¹…
            if self.trainer_instance.experiment_tracker:
                self.trainer_instance.experiment_tracker.log_metrics(
                    metrics, step=state.global_step
                )
    
    def on_train_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):
        """í•™ìŠµ ì¢…ë£Œ ì‹œ ìµœì¢… ê²°ê³¼ ë¡œê¹…"""
        if wandb.run is not None:
            wandb.run.summary.update(self.best_metrics)


class DialogueSummarizationTrainer:
    """
    ëŒ€í™” ìš”ì•½ ëª¨ë¸ í•™ìŠµ íŠ¸ë ˆì´ë„ˆ
    
    baseline.ipynbì˜ í•™ìŠµ ë¡œì§ì„ ëª¨ë“ˆí™”í•˜ê³  WandB Sweepê³¼ í†µí•©í•˜ì—¬
    ìƒì‚°ì„± ë†’ì€ ì‹¤í—˜ í™˜ê²½ì„ ì œê³µí•©ë‹ˆë‹¤.
    
    Features:
        - ë‹¤ì¤‘ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì§€ì› (BART, T5, KoBART ë“±)
        - ìë™ ë””ë°”ì´ìŠ¤ ê°ì§€ ë° ìµœì í™” (CUDA, MPS, CPU)
        - ì‹¤í—˜ ì¶”ì  ë° ëª¨ë¸ ë“±ë¡ ì‹œìŠ¤í…œ
        - ì»¤ìŠ¤í…€ ì½œë°± ë° ë©”íŠ¸ë¦­ ê³„ì‚°
        - í¬ê´„ì  ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…
        - WandB í†µí•© ì‹¤í—˜ ê´€ë¦¬
        
    Example:
        >>> config = load_config('configs/bart_base.yaml')
        >>> trainer = DialogueSummarizationTrainer(config)
        >>> datasets = trainer.prepare_data()
        >>> result = trainer.train(datasets)
    """
    
    def __init__(self, config: Dict[str, Any], 
                 sweep_mode: bool = False,
                 experiment_name: Optional[str] = None):
        """
        íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬ (ConfigManagerë¡œë¶€í„°)
            sweep_mode: WandB Sweep ëª¨ë“œ ì—¬ë¶€
            experiment_name: ì‹¤í—˜ëª… (Noneì´ë©´ ìë™ ìƒì„±)
        """
        self.config = config
        self.sweep_mode = sweep_mode
        self.experiment_name = experiment_name or config.get('meta', {}).get('experiment_name', 'dialogue_summarization')
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = self._setup_device()
        
        # ê²½ë¡œ ì„¤ì •
        self.setup_paths()
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.model = None
        self.tokenizer = None
        self.data_processor = None
        self.rouge_calculator = None
        self.trainer = None
        
        # ì‹¤í—˜ ê´€ë¦¬
        self.experiment_tracker = None
        self.model_registry = None
        
        # ë¡œê¹… ì„¤ì •
        self._setup_logging()
        
        logger.info(f"Trainer initialized with config: {self.experiment_name}")
        
    def setup_paths(self) -> None:
        """ê²½ë¡œ ì„¤ì •"""
        # ê²½ë¡œ ê´€ë¦¬ìë¥¼ ì‚¬ìš©í•˜ì—¬ ê²½ë¡œ ê´€ë¦¬
        experiment_name = self.experiment_name
        
        # Sweep ëª¨ë“œì¼ ë•ŒëŠ” run IDë¥¼ í¬í•¨
        if self.sweep_mode and wandb.run:
            experiment_name = f"sweep_{wandb.run.id}"
        else:
            from datetime import datetime
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            experiment_name = f"{self.experiment_name}_{timestamp}"
        
        # ê²½ë¡œ ê´€ë¦¬ìë¥¼ í†µí•œ ê²½ë¡œ ì„¤ì •
        self.output_dir = path_manager.get_output_path(experiment_name)
        self.model_save_dir = path_manager.get_model_path(experiment_name)
        self.results_dir = path_manager.ensure_dir(self.output_dir / "results")
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.log_dir = path_manager.get_log_path(experiment_name)
        
        # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€ (output_dirì´ ì„¤ì •ëœ í›„)
        self._add_file_handler()
    
    def initialize_components(self) -> None:
        """ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        logger.info("Initializing components...")
        
        # ì‹¤í—˜ ì¶”ì ê¸° ì´ˆê¸°í™”
        if self.config.get('experiment_tracking', {}).get('enabled', True):
            self.experiment_tracker = ExperimentTracker(
                experiments_dir=self.output_dir / "experiments"
            )
            self.model_registry = ModelRegistry(
                registry_dir=self.output_dir / "models"
            )
        
        # í† í¬ë‚˜ì´ì € ë¡œë”©
        self._load_tokenizer()
        
        # ëª¨ë¸ ë¡œë”©
        self._load_model()
        
        # ë°ì´í„° í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        self.data_processor = DataProcessor(
            tokenizer=self.tokenizer,
            config=self.config
        )
        
        # ROUGE ê³„ì‚°ê¸° ì´ˆê¸°í™”
        self.rouge_calculator = RougeCalculator(
            use_korean_tokenizer=self.config.get("evaluation", {}).get("rouge_tokenize_korean", True),
            use_stemmer=self.config.get("evaluation", {}).get("rouge_use_stemmer", True)
        )
        logger.info("All components initialized successfully")
    def prepare_data(self, train_path: Optional[str] = None, 
                    val_path: Optional[str] = None,
                    test_path: Optional[str] = None) -> DatasetDict:
        """
        ë°ì´í„° ì¤€ë¹„
        
        Args:
            train_path: í•™ìŠµ ë°ì´í„° ê²½ë¡œ
            val_path: ê²€ì¦ ë°ì´í„° ê²½ë¡œ  
            test_path: í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ
            
        Returns:
            ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ë”•ì…”ë„ˆë¦¬
        """
        data_paths = self.config.get('data', {})
        
        train_path = train_path or data_paths.get('train_path')
        val_path = val_path or data_paths.get('val_path')
        test_path = test_path or data_paths.get('test_path')
        
        logger.info("Loading and processing datasets...")
        
        datasets = {}
        
        if train_path:
            train_data = self.data_processor.load_data(train_path)
            datasets['train'] = self.data_processor.process_data(
                train_data, 
                is_training=True
            )
            logger.info(f"Train dataset size: {len(datasets['train'])}")
        
        if val_path:
            val_data = self.data_processor.load_data(val_path)
            datasets['validation'] = self.data_processor.process_data(
                val_data,
                is_training=False
            )
            logger.info(f"Validation dataset size: {len(datasets['validation'])}")
        
        if test_path:
            test_data = self.data_processor.load_data(test_path)
            datasets['test'] = self.data_processor.process_data(
                test_data,
                is_training=False
            )
            logger.info(f"Test dataset size: {len(datasets['test'])}")
        
        return DatasetDict(datasets)
    
    def train(self, dataset: DatasetDict, 
             resume_from_checkpoint: Optional[str] = None) -> TrainingResult:
        """
        í•™ìŠµ ì‹¤í–‰
        
        Args:
            dataset: í•™ìŠµ/ê²€ì¦ ë°ì´í„°ì…‹
            resume_from_checkpoint: ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (ì¬ê°œ ì‹œ)
            
        Returns:
            í•™ìŠµ ê²°ê³¼
        """
        # WandB ì´ˆê¸°í™” (sweep ëª¨ë“œê°€ ì•„ë‹ˆê³  ì´ë¯¸ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°)
        if not self.sweep_mode and wandb.run is None:
            wandb_config = setup_wandb_for_experiment(
                config=self.config,
                experiment_name=self.experiment_name,
                sweep_mode=self.sweep_mode
            )
            wandb.init(**wandb_config)
            logger.info(f"WandB run initialized: {wandb.run.name}")
        
        # ì‹¤í—˜ ì¶”ì  ì‹œì‘
        experiment_id = None
        if self.experiment_tracker:
            experiment_id = self.experiment_tracker.start_experiment(
                name=self.experiment_name,
                description=f"Training {self.config['model']['architecture']} model",
                config=self.config,
                model_type=self.config['model']['architecture'],
                dataset_info={
                    'train_size': len(dataset.get('train', [])),
                    'val_size': len(dataset.get('validation', []))
                },
                wandb_run_id=wandb.run.id if wandb.run else None
            )
        else:
            experiment_id = None
        
        # í•™ìŠµ ì¸ì ì„¤ì •
        training_args = self._get_training_arguments()
        
        # ë°ì´í„° ì½œë ˆì´í„°
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=self.tokenizer,
            model=self.model,
            padding=True,
            max_length=self.config['tokenizer']['encoder_max_len']
        )
        
        # í‰ê°€ ë©”íŠ¸ë¦­ í•¨ìˆ˜ - HuggingFace Trainerì˜ ì½œë°±ìœ¼ë¡œ ì‚¬ìš©ë¨
        def compute_metrics(eval_preds: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:
            """
            í•™ìŠµ ì¤‘ í‰ê°€ ë‹¨ê³„ì—ì„œ ROUGE ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•˜ëŠ” ì¤‘ì²© í•¨ìˆ˜
            
            Args:
                eval_preds: (predictions, labels) íŠœí”Œ
                
            Returns:
                ROUGE ì ìˆ˜ë“¤ì„ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
            """
            preds, labels = eval_preds
            
            # í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”© (íŠ¹ìˆ˜ í† í° ì œê±°)
            decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True)
            decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)
            
            # HuggingFaceì—ì„œ ì‚¬ìš©í•˜ëŠ” -100 íŒ¨ë”© í† í°ì„ ì •ìƒ í† í°ìœ¼ë¡œ ë³€í™˜
            # -100ì€ loss ê³„ì‚°ì—ì„œ ë¬´ì‹œë˜ëŠ” ë¼ë²¨ì´ì§€ë§Œ ë””ì½”ë”©ì—ì„œëŠ” ë¬¸ì œê°€ ë¨
            labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)
            decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)
            
            # ëŒ€í™” ìš”ì•½ì— íŠ¹í™”ëœ ROUGE ë©”íŠ¸ë¦­ ê³„ì‚° (Multi-reference ì§€ì›)
            result = self.rouge_calculator.compute_metrics(decoded_preds, decoded_labels)
            
            return result
        
        # ì½œë°± ì„¤ì •
        callbacks = [WandbCallback(self)]
        
        # ì¡°ê¸° ì¢…ë£Œ ì„¤ì •
        if self.config['training'].get('early_stopping_patience'):
            callbacks.append(
                EarlyStoppingCallback(
                    early_stopping_patience=self.config['training']['early_stopping_patience'],
                    early_stopping_threshold=0.001
                )
            )
        
        # íŠ¸ë ˆì´ë„ˆ ìƒì„±
        self.trainer = Seq2SeqTrainer(
            model=self.model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=dataset.get('train'),
            eval_dataset=dataset.get('validation'),
            tokenizer=self.tokenizer,
            compute_metrics=compute_metrics,
            callbacks=callbacks
        )
        
        # í•™ìŠµ ì‹œì‘
        logger.info("Starting training...")
        
        try:
            train_result = self.trainer.train(
                resume_from_checkpoint=resume_from_checkpoint
            )
            
            # ìµœì¢… í‰ê°€
            # ìµœì¢… í‰ê°€
            logger.info("Running final evaluation...")
            eval_results = self.trainer.evaluate()
            
            # í‰ê°€ ê²°ê³¼ë¥¼ eval_results.jsonìœ¼ë¡œ ì €ì¥
            eval_results_file = self.model_save_dir / 'eval_results.json'
            with open(eval_results_file, 'w', encoding='utf-8') as f:
                json.dump(eval_results, f, ensure_ascii=False, indent=2)
            logger.info(f"Evaluation results saved to {eval_results_file}")
            
            # ëª¨ë¸ ì €ì¥
            best_model_path = self.model_save_dir / "best_model"
            self.trainer.save_model(str(best_model_path))
            self.tokenizer.save_pretrained(str(best_model_path))
            
            # ê²°ê³¼ ì •ë¦¬
            wandb_callback = callbacks[0]
            training_result = TrainingResult(
                best_metrics=wandb_callback.best_metrics,
                final_metrics=eval_results,
                model_path=str(best_model_path),
                config_used=self.config,
                training_history=[], # í–¥í›„ êµ¬í˜„
                wandb_run_id=wandb.run.id if wandb.run else None,
                experiment_id=experiment_id
            )
            
            # ì‹¤í—˜ ì¢…ë£Œ
            if self.experiment_tracker:
                self.experiment_tracker.end_experiment(
                    experiment_id=experiment_id,
                    final_metrics=eval_results,
                    best_metrics=wandb_callback.best_metrics,
                    status="completed"
                )
            
            # ëª¨ë¸ ë“±ë¡
            if self.model_registry:
                model_id = self.model_registry.register_model(
                    name=f"{self.config['model']['architecture']}_{self.experiment_name}",
                    architecture=self.config['model']['architecture'],
                    checkpoint=self.config['model']['checkpoint'],
                    config=self.config,
                    performance=wandb_callback.best_metrics,
                    training_info={
                        'epochs': self.config['training']['num_train_epochs'],
                        'batch_size': self.config['training']['per_device_train_batch_size'],
                        'learning_rate': self.config['training']['learning_rate']
                    },
                    file_path=str(best_model_path),
                    experiment_id=experiment_id
                )
                logger.info(f"Model registered with ID: {model_id}")
                
                # WandB Model Registryì— ëª¨ë¸ ë“±ë¡
                if wandb.run is not None:
                    log_model_to_wandb(
                        model_path=str(best_model_path),
                        model_name=f"{self.config['model']['architecture']}_{self.experiment_name}",
                        metrics=wandb_callback.best_metrics,
                        config=self.config,
                        aliases=["latest", "best"] if wandb_callback.best_metrics.get('rouge_combined_f1', 0) > 0.3 else ["latest"]
                    )
                
                # ê²°ê³¼ ì €ì¥
            self._save_results(training_result)
            
            # test.csvì— ëŒ€í•œ ìë™ ì¶”ë¡  ìˆ˜í–‰
            if self.config.get('inference', {}).get('run_test_inference', True):
                try:
                    logger.info("Running inference on test.csv...")
                    from post_training_inference import generate_submission_after_training
                    
                    submission_path = generate_submission_after_training(
                        experiment_name=self.experiment_name,
                        model_path=str(best_model_path),
                        config_dict=self.config
                    )
                    
                    training_result.submission_path = submission_path
                    logger.info(f"Test inference completed. Submission file: {submission_path}")
                    
                except Exception as e:
                    logger.warning(f"Failed to run test inference: {e}")
                    # Test inference ì‹¤íŒ¨ëŠ” ì „ì²´ í•™ìŠµì„ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
            
            return training_result
            
        except Exception as e:
            logger.error(f"Training failed with error: {type(e).__name__}: {str(e)}")
            logger.error(f"Current config: {self.config.get('model', {}).get('checkpoint', 'Unknown')}")
            logger.error(f"Device: {self.device}")
            import traceback
            logger.debug(f"Full traceback: {traceback.format_exc()}")
            logger.error(f"Training failed: {str(e)}")
            if self.experiment_tracker and experiment_id:
                self.experiment_tracker.end_experiment(
                    experiment_id=experiment_id,
                    status="failed",
                    notes=str(e)
                )
            raise
    
    def evaluate(self, dataset: Dataset, 
                metric_key_prefix: str = "eval") -> Dict[str, float]:
        """
        ëª¨ë¸ í‰ê°€
        
        Args:
            dataset: í‰ê°€ ë°ì´í„°ì…‹
            metric_key_prefix: ë©”íŠ¸ë¦­ í‚¤ ì ‘ë‘ì‚¬
            
        Returns:
            í‰ê°€ ê²°ê³¼
        """
        if not self.trainer:
            raise ValueError("Trainer not initialized. Call train() first.")
        
        results = self.trainer.evaluate(
            eval_dataset=dataset,
            metric_key_prefix=metric_key_prefix
        )
        
        return results
    
    def generate_predictions(self, dataset: Dataset, 
                           max_samples: Optional[int] = None) -> List[Dict[str, str]]:
        """
        ì˜ˆì¸¡ ìƒì„±
        
        Args:
            dataset: ì…ë ¥ ë°ì´í„°ì…‹
            max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ì „ì²´)
            
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        self.model.eval()
        predictions = []
        
        # ìƒ˜í”Œë§
        if max_samples:
            indices = np.random.choice(len(dataset), min(max_samples, len(dataset)), replace=False)
            dataset = dataset.select(indices)
        
        # ìƒì„± ì„¤ì •
        gen_config = self.config['generation']
        
        with torch.no_grad():
            for example in tqdm(dataset, desc="Generating predictions"):
                # í† í°í™”
                inputs = self.tokenizer(
                    example['input'],
                    max_length=self.config['tokenizer']['encoder_max_len'],
                    truncation=True,
                    padding=True,
                    return_tensors="pt"
                ).to(self.device)
                
                # ìƒì„±
                outputs = self.model.generate(
                    **inputs,
                    max_length=gen_config['max_length'],
                    num_beams=gen_config['num_beams'],
                    length_penalty=gen_config.get('length_penalty', 1.0),
                    no_repeat_ngram_size=gen_config.get('no_repeat_ngram_size', 2),
                    early_stopping=gen_config.get('early_stopping', True),
                    do_sample=gen_config.get('do_sample', False),
                    temperature=gen_config.get('temperature', 1.0) if gen_config.get('do_sample') else None,
                    top_k=gen_config.get('top_k', 50) if gen_config.get('do_sample') else None,
                    top_p=gen_config.get('top_p', 0.95) if gen_config.get('do_sample') else None
                )
                
                # ë””ì½”ë”©
                prediction = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                predictions.append({
                    'input': example['input'],
                    'prediction': prediction,
                    'reference': example.get('target', '')
                })
        
        return predictions
    
    def _setup_device(self) -> torch.device:
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        from utils.device_utils import get_optimal_device, setup_device_config
        
        device_config = self.config['general'].get('device', 'auto')
        
        if device_config == 'auto':
            # ìµœì  ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ
            device, device_info = get_optimal_device()
            
            # ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            model_size = self.config.get('model', {}).get('size', 'base')
            use_qlora = self.config.get('qlora', {}).get('use_qlora', False)
            optimization_config = setup_device_config(device_info, model_size, use_qlora)
            
            # ìµœì í™” ì„¤ì •ì„ configì— ë³‘í•©
            if 'training' not in self.config:
                self.config['training'] = {}
            
            # ê¸°ì¡´ ì„¤ì •ê³¼ ë³‘í•© (ê¸°ì¡´ ì„¤ì • ìš°ì„ )
            opt_dict = optimization_config.to_dict()
            for key, value in opt_dict.items():
                if key not in self.config['training']:
                    self.config['training'][key] = value
            
            logger.info(f"ìë™ ê°ì§€ëœ ë””ë°”ì´ìŠ¤: {device_info}")
            logger.info(f"ìµœì í™” ì„¤ì • ì ìš©ë¨: batch_size={optimization_config.batch_size}, "
                       f"mixed_precision={optimization_config.mixed_precision}, "
                       f"num_workers={optimization_config.num_workers}")
        else:
            # ìˆ˜ë™ ì„¤ì •
            device = torch.device(device_config)
            logger.info(f"ìˆ˜ë™ ì„¤ì •ëœ ë””ë°”ì´ìŠ¤: {device}")
        
        logger.info(f"Using device: {device}")
        return device
    
    def _setup_logging(self) -> None:
        """ë¡œê¹… ì„¤ì •"""
        log_level = self.config.get('logging', {}).get('level', 'INFO')
        
        # ê¸°ë³¸ ë¡œê¹… ì„¤ì • (ì½˜ì†”ë§Œ)
        logging.basicConfig(
            level=getattr(logging, log_level),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(sys.stdout)
            ]
        )
        
        # output_dirì´ ì„¤ì •ëœ í›„ì— íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
        if hasattr(self, 'output_dir') and self.output_dir:
            self._add_file_handler()
    
    def _add_file_handler(self) -> None:
        """ë¡œê¹…ì— íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€ (output_dir ì„¤ì • í›„ í˜¸ì¶œ)"""
        if hasattr(self, 'output_dir') and self.output_dir:
            # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ê°€ì ¸ì˜¤ê¸°
            root_logger = logging.getLogger()
            
            # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
            file_handler = logging.FileHandler(self.output_dir / 'training.log')
            file_handler.setFormatter(
                logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            )
            root_logger.addHandler(file_handler)
            logger.info(f"ë¡œê¹… íŒŒì¼ ìƒì„±: {self.output_dir / 'training.log'}")
    
    def _load_tokenizer(self) -> None:
        """í† í¬ë‚˜ì´ì € ë¡œë”©"""
        # model ì„¹ì…˜ì´ ì—†ìœ¼ë©´ generalì—ì„œ model_name ì‚¬ìš©
        if 'model' in self.config:
            model_checkpoint = self.config['model']['checkpoint']
        else:
            model_checkpoint = self.config.get('general', {}).get('model_name')
            if not model_checkpoint:
                raise ValueError("Model checkpoint not found in config. Please specify 'model.checkpoint' or 'general.model_name'")
        
        logger.info(f"Loading tokenizer: {model_checkpoint}")
        
        # ëª¨ë¸ë³„ í† í¬ë‚˜ì´ì € ì„¤ì •
        use_fast = True
        if 'mt5' in model_checkpoint.lower() or 't5' in model_checkpoint.lower():
            use_fast = False  # T5/mT5ëŠ” SentencePieceë¡œ use_fast=False ì‚¬ìš©
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_checkpoint,
            use_fast=use_fast
        )
        
        # íŠ¹ìˆ˜ í† í° ì„¤ì • (í•„ìš”ì‹œ)
        if self.config['model']['architecture'] in ['kogpt2', 'gpt2']:
            # GPT ê³„ì—´ì€ pad_tokenì´ ì—†ì„ ìˆ˜ ìˆìŒ
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
    
    def _load_model(self) -> None:
        """ëª¨ë¸ ë¡œë”© (unsloth ë° QLoRA ì§€ì›)"""
        model_checkpoint = self.config['model']['checkpoint']
        architecture = self.config['model']['architecture']
        
        # QLoRA ì„¤ì • í™•ì¸
        qlora_config = self.config.get('qlora', {})
        use_unsloth = qlora_config.get('use_unsloth', False) and UNSLOTH_AVAILABLE
        use_qlora = qlora_config.get('use_qlora', False)
        
        logger.info(f"Loading model: {model_checkpoint} ({architecture})")
        logger.info(f"QLoRA enabled: {use_qlora}, unsloth enabled: {use_unsloth}")
        
        if use_unsloth and architecture in ['kobart', 'bart', 't5', 'mt5']:
            # unslothë¡œ ëª¨ë¸ ë¡œë”© (ìµœëŒ€ 75% ë©”ëª¨ë¦¬ ê°ì†Œ)
            self._load_model_with_unsloth(model_checkpoint, qlora_config)
            
        elif use_qlora:
            # ì¼ë°˜ QLoRA ëª¨ë¸ ë¡œë”©
            self._load_model_with_qlora(model_checkpoint, architecture, qlora_config)
            
        else:
            # ê¸°ì¡´ ëª¨ë¸ ë¡œë”© ë°©ì‹
            self._load_model_standard(model_checkpoint, architecture)
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (QLoRA ëª¨ë¸ì€ ì´ë¯¸ ì ì ˆí•œ ë””ë°”ì´ìŠ¤ì— ìˆìŒ)
        if not (use_unsloth or use_qlora):
            self.model = self.model.to(self.device)
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… (ë©”ëª¨ë¦¬ ìµœì í™”)
        if self.config['training'].get('gradient_checkpointing', False):
            if hasattr(self.model, 'gradient_checkpointing_enable'):
                self.model.gradient_checkpointing_enable()
            else:
                logger.warning("ëª¨ë¸ì´ gradient_checkpointingì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                
    def _load_model_with_unsloth(self, model_checkpoint: str, qlora_config: Dict[str, Any]) -> None:
        """
        unslothë¥¼ ì‚¬ìš©í•œ ê³ íš¨ìœ¨ ëª¨ë¸ ë¡œë”© (ë©”ëª¨ë¦¬ 75% ê°ì†Œ)
        
        Args:
            model_checkpoint: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            qlora_config: QLoRA ì„¤ì •
        """
        logger.info("ğŸš€ unslothë¡œ ê³ íš¨ìœ¨ ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        try:
            # unsloth FastLanguageModelë¡œ ëª¨ë¸ ë¡œë”©
            model, tokenizer = FastLanguageModel.from_pretrained(
                model_name=model_checkpoint,
                max_seq_length=self.config['tokenizer'].get('encoder_max_len', 512) + 
                              self.config['tokenizer'].get('decoder_max_len', 200),
                dtype=torch.float16 if self.config['training'].get('fp16') else torch.float32,
                load_in_4bit=qlora_config.get('load_in_4bit', True),
            )
            
            # LoRA ì„¤ì • ì¶”ê°€
            model = FastLanguageModel.get_peft_model(
                model,
                r=qlora_config.get('lora_rank', 16),
                target_modules=qlora_config.get('target_modules', [
                    "q_proj", "k_proj", "v_proj", "out_proj",
                    "fc1", "fc2"
                ]),
                lora_alpha=qlora_config.get('lora_alpha', 32),
                lora_dropout=qlora_config.get('lora_dropout', 0.1),
                bias="none",
                use_gradient_checkpointing="unsloth",  # unsloth ìµœì í™”
                random_state=42,
            )
            
            self.model = model
            logger.info("âœ… unsloth ëª¨ë¸ ë¡œë”© ì„±ê³µ! ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 75% ê°ì†Œ ì˜ˆìƒ")
            
        except Exception as e:
            logger.error(f"âŒ unsloth ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            logger.info("í´ë°± ëª¨ë“œ: ì¼ë°˜ QLoRAë¡œ ëŒ€ì²´")
            self._load_model_with_qlora(model_checkpoint, 'kobart', qlora_config)
    
    def _load_model_with_qlora(self, model_checkpoint: str, architecture: str, qlora_config: Dict[str, Any]) -> None:
        """
        ì¼ë°˜ QLoRAë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ ë¡œë”©
        
        Args:
            model_checkpoint: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            architecture: ëª¨ë¸ ì•„í‚¤í…ì²˜
            qlora_config: QLoRA ì„¤ì •
        """
        logger.info("ğŸ”‹ QLoRAë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        try:
            # 4-bit ì–‘ìí™” ì„¤ì •
            from transformers import BitsAndBytesConfig
            
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=qlora_config.get('load_in_4bit', True),
                bnb_4bit_compute_dtype=getattr(torch, qlora_config.get('bnb_4bit_compute_dtype', 'float16')),
                bnb_4bit_quant_type=qlora_config.get('bnb_4bit_quant_type', 'nf4'),
                bnb_4bit_use_double_quant=qlora_config.get('bnb_4bit_use_double_quant', True),
            )
            
            # ëª¨ë¸ ë¡œë”©
            if architecture in ['kobart', 'bart', 't5', 'mt5']:
                model = AutoModelForSeq2SeqLM.from_pretrained(
                    model_checkpoint,
                    quantization_config=bnb_config,
                    device_map="auto",
                    torch_dtype=torch.float16 if self.config['training'].get('fp16') else torch.float32
                )
            else:
                model = AutoModelForCausalLM.from_pretrained(
                    model_checkpoint,
                    quantization_config=bnb_config,
                    device_map="auto",
                    torch_dtype=torch.float16 if self.config['training'].get('fp16') else torch.float32
                )
            
            # LoRA ì„¤ì •
            if LoraConfig is not None:
                lora_config = LoraConfig(
                    r=qlora_config.get('lora_rank', 16),
                    lora_alpha=qlora_config.get('lora_alpha', 32),
                    target_modules=qlora_config.get('target_modules', [
                        "q_proj", "k_proj", "v_proj", "out_proj",
                        "fc1", "fc2"
                    ]),
                    lora_dropout=qlora_config.get('lora_dropout', 0.1),
                    bias="none",
                    task_type=TaskType.SEQ_2_SEQ_LM if architecture in ['kobart', 'bart', 't5', 'mt5'] else TaskType.CAUSAL_LM,
                )
                
                model = get_peft_model(model, lora_config)
                logger.info("âœ… QLoRA ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")
            
            self.model = model
            
        except ImportError:
            logger.error("âŒ bitsandbytes ë˜ëŠ” peft ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
            logger.info("í´ë°± ëª¨ë“œ: í‘œì¤€ ëª¨ë¸ ë¡œë”©")
            self._load_model_standard(model_checkpoint, architecture)
        except Exception as e:
            logger.error(f"âŒ QLoRA ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            logger.info("í´ë°± ëª¨ë“œ: í‘œì¤€ ëª¨ë¸ ë¡œë”©")
            self._load_model_standard(model_checkpoint, architecture)
    
    def _load_model_standard(self, model_checkpoint: str, architecture: str) -> None:
        """
        í‘œì¤€ ëª¨ë¸ ë¡œë”© (ê¸°ì¡´ ë°©ì‹)
        
        Args:
            model_checkpoint: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            architecture: ëª¨ë¸ ì•„í‚¤í…ì²˜
        """
        logger.info("ğŸ“š í‘œì¤€ ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # ëª¨ë¸ ì•„í‚¤í…ì²˜ì— ë”°ë¥¸ ë¡œë”©
        if architecture in ['kobart', 'bart', 't5', 'mt5']:
            # ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ ëª¨ë¸
            self.model = AutoModelForSeq2SeqLM.from_pretrained(
                model_checkpoint,
                torch_dtype=torch.float16 if self.config['training'].get('fp16') else torch.float32
            )
        elif architecture in ['kogpt2', 'gpt2', 'gpt-neo']:
            # ì¸ê³¼ ì–¸ì–´ ëª¨ë¸
            self.model = AutoModelForCausalLM.from_pretrained(
                model_checkpoint,
                torch_dtype=torch.float16 if self.config['training'].get('fp16') else torch.float32
            )
        else:
            raise ValueError(f"Unsupported architecture: {architecture}")
        
        logger.info("âœ… í‘œì¤€ ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def _get_model_specific_config(self, architecture: str, checkpoint: str) -> Dict[str, Any]:
        """ëª¨ë¸ë³„ íŠ¹ìˆ˜ ì„¤ì • ë°˜í™˜"""
        config = {}
        
        # T5 ê³„ì—´
        if architecture in ['t5', 'mt5', 'flan-t5']:
            config['prefix'] = "summarize: "  # T5ëŠ” task prefix í•„ìš”
            
        # GPT ê³„ì—´
        elif architecture in ['gpt2', 'kogpt2', 'gpt-neo']:
            config['max_length'] = self.config['tokenizer']['encoder_max_len'] + self.config['tokenizer']['decoder_max_len']
            config['pad_token_id'] = self.tokenizer.pad_token_id
            
        return config
    
    def _preprocess_for_model(self, examples: Dict[str, Any]) -> Dict[str, Any]:
        """ëª¨ë¸ë³„ ë°ì´í„° ì „ì²˜ë¦¬"""
        architecture = self.config['model']['architecture']
        
        if architecture in ['t5', 'mt5', 'flan-t5']:
            # T5ëŠ” prefix ì¶”ê°€
            examples['input'] = ["summarize: " + inp for inp in examples['input']]
            
        elif architecture in ['gpt2', 'kogpt2', 'gpt-neo']:
            # GPTëŠ” ì…ë ¥ê³¼ íƒ€ê²Ÿì„ ì—°ê²°
            examples['input'] = [
                f"{inp} TL;DR: {tgt}" 
                for inp, tgt in zip(examples['input'], examples['target'])
            ]
            
        return examples
    
    def _get_training_arguments(self) -> Seq2SeqTrainingArguments:
        """í•™ìŠµ ì¸ì ìƒì„±"""
        train_config = self.config['training']
        
        # ê¸°ë³¸ ì¸ì
        args_dict = {
            'output_dir': str(self.output_dir / 'checkpoints'),
            'overwrite_output_dir': True,
            'do_train': True,
            'do_eval': True,
            'eval_strategy': train_config.get('evaluation_strategy', 'steps'),
            'eval_steps': train_config.get('eval_steps', 500),
            'save_strategy': train_config.get('save_strategy', 'steps'),
            'save_steps': train_config.get('save_steps', 500),
            'save_total_limit': train_config.get('save_total_limit', 3),
            'per_device_train_batch_size': train_config['per_device_train_batch_size'],
            'per_device_eval_batch_size': train_config.get('per_device_eval_batch_size', 
                                                          train_config['per_device_train_batch_size']),
            'gradient_accumulation_steps': train_config.get('gradient_accumulation_steps', 1),
            'learning_rate': train_config['learning_rate'],
            'weight_decay': train_config.get('weight_decay', 0.01),
            'adam_beta1': train_config.get('adam_beta1', 0.9),
            'adam_beta2': train_config.get('adam_beta2', 0.999),
            'adam_epsilon': train_config.get('adam_epsilon', 1e-8),
            'max_grad_norm': train_config.get('max_grad_norm', 1.0),
            'num_train_epochs': train_config['num_train_epochs'],
            'lr_scheduler_type': train_config.get('lr_scheduler_type', 'linear'),
            'warmup_ratio': train_config.get('warmup_ratio', 0.1),
            'warmup_steps': train_config.get('warmup_steps', 0),
            'logging_dir': str(self.output_dir / 'logs'),
            'logging_steps': train_config.get('logging_steps', 50),
            'load_best_model_at_end': True,
            'metric_for_best_model': 'eval_rouge_combined_f1',
            'greater_is_better': True,
            'fp16': train_config.get('fp16', False),
            'fp16_opt_level': train_config.get('fp16_opt_level', 'O1'),
            'dataloader_num_workers': train_config.get('dataloader_num_workers', 4),
            'remove_unused_columns': False,
            'label_smoothing_factor': train_config.get('label_smoothing', 0.0),
            'optim': train_config.get('optim', 'adamw_torch'),
            'seed': self.config['general'].get('seed', 42),
            'report_to': ['wandb'] if wandb.run else ['none'],
            'run_name': self.experiment_name if wandb.run else None,
            'push_to_hub': False,
            'predict_with_generate': True,
            'generation_max_length': self.config['generation']['max_length'],
            'generation_num_beams': self.config['generation']['num_beams']
        }
        
        # ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ íŠ¹í™” ì¸ì
        seq2seq_args = Seq2SeqTrainingArguments(**args_dict)
        
        return seq2seq_args
    
    def _save_results(self, result: TrainingResult) -> None:
        """ê²°ê³¼ ì €ì¥"""
        # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ìƒì„±
        results_dict = {
            'experiment_name': self.experiment_name,
            'model_architecture': self.config['model']['architecture'],
            'model_checkpoint': self.config['model']['checkpoint'],
            'best_metrics': result.best_metrics,
            'final_metrics': result.final_metrics,
            'model_path': result.model_path,
            'wandb_run_id': result.wandb_run_id,
            'experiment_id': result.experiment_id,
            'config': result.config_used,
            'timestamp': str(Path(result.model_path).parent.parent.name),
            'submission_path': result.submission_path  # ì œì¶œ íŒŒì¼ ê²½ë¡œ ì¶”ê°€
        }
        # JSON ì €ì¥
        results_file = self.results_dir / 'training_results.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results_dict, f, ensure_ascii=False, indent=2)
        
        # ìš”ì•½ í…ìŠ¤íŠ¸ ì €ì¥
        summary_file = self.results_dir / 'summary.txt'
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"Training Summary for {self.experiment_name}\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"Model: {self.config['model']['architecture']} ({self.config['model']['checkpoint']})\n")
            f.write(f"Training Epochs: {self.config['training']['num_train_epochs']}\n")
            f.write(f"Batch Size: {self.config['training']['per_device_train_batch_size']}\n")
            f.write(f"Learning Rate: {self.config['training']['learning_rate']}\n\n")
            f.write("Best Metrics:\n")
            for metric, value in result.best_metrics.items():
                f.write(f"  {metric}: {value:.4f}\n")
            f.write("\nModel saved to: " + result.model_path + "\n")
            if result.wandb_run_id:
                f.write(f"WandB Run ID: {result.wandb_run_id}\n")
        
        logger.info(f"Results saved to {self.results_dir}")


def create_trainer(config: Union[str, Dict[str, Any]], 
                  sweep_mode: bool = False,
                  disable_eval: bool = False) -> DialogueSummarizationTrainer:
    """
    íŠ¸ë ˆì´ë„ˆ ìƒì„± í¸ì˜ í•¨ìˆ˜
    
    Args:
        config: ì„¤ì • íŒŒì¼ ê²½ë¡œ ë˜ëŠ” ì„¤ì • ë”•ì…”ë„ˆë¦¬
        sweep_mode: WandB Sweep ëª¨ë“œ ì—¬ë¶€
        disable_eval: í‰ê°€ ë¹„í™œì„±í™” ì—¬ë¶€
        
    Returns:
        ì´ˆê¸°í™”ëœ íŠ¸ë ˆì´ë„ˆ ì¸ìŠ¤í„´ìŠ¤
    """
    # ì„¤ì • ë¡œë”©
    if isinstance(config, str):
        config_dict = load_config(config)
    else:
        config_dict = config
    
    # í‰ê°€ ë¹„í™œì„±í™” ì˜µì…˜ ì²˜ë¦¬
    if disable_eval:
        if 'training' not in config_dict:
            config_dict['training'] = {}
        config_dict['training']['do_eval'] = False
        config_dict['training']['evaluation_strategy'] = 'no'
        config_dict['training']['load_best_model_at_end'] = False  # best model ë¡œë“œ ë¹„í™œì„±í™”
        config_dict['training']['metric_for_best_model'] = None  # best model ë©”íŠ¸ë¦­ ë¹„í™œì„±í™”
        print("âš ï¸  í‰ê°€ ë¹„í™œì„±í™”: evaluation_strategy=no, do_eval=False, load_best_model_at_end=False")
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = DialogueSummarizationTrainer(
        config=config_dict,
        sweep_mode=sweep_mode
    )
    
    # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    trainer.initialize_components()
    
    return trainer


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸/ë””ë²„ê¹…ìš© ë©”ì¸ í•¨ìˆ˜
    import argparse
    
    parser = argparse.ArgumentParser(description="Train dialogue summarization model")
    parser.add_argument("--config", type=str, required=True, help="Config file path")
    parser.add_argument("--train-data", type=str, help="Train data path")
    parser.add_argument("--val-data", type=str, help="Validation data path")
    parser.add_argument("--test-data", type=str, help="Test data path")
    parser.add_argument("--sweep", action="store_true", help="Run in sweep mode")
    parser.add_argument("--disable-eval", action="store_true", help="Disable evaluation (for 1-epoch mode)")
    parser.add_argument("--one-epoch", action="store_true", help="Run training for only 1 epoch (for testing)")
    
    args = parser.parse_args()
    
    # WandB ì´ˆê¸°í™” (ë¹„ Sweep ëª¨ë“œ)
    if not args.sweep:
        wandb.init(
            project="nlp-dialogue-summarization",
            name="manual_training",
            config={"manual_run": True}
        )
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í•™ìŠµ
    trainer = create_trainer(args.config, sweep_mode=args.sweep, disable_eval=args.disable_eval)
    
    # 1ì—í¬í¬ ëª¨ë“œ ì²˜ë¦¬
    if args.one_epoch:
        logger.info("1ì—í¬í¬ ëª¨ë“œ í™œì„±í™”: í•™ìŠµ ì—í¬í¬ë¥¼ 1ë¡œ ì„¤ì •")
        trainer.config['training']['num_epochs'] = 1
        trainer.config['training']['max_steps'] = None  # max_steps ë¹„í™œì„±í™”
        trainer.config['training']['evaluation_strategy'] = "no"  # í‰ê°€ ë¹„í™œì„±í™”
        trainer.config['training']['save_strategy'] = "epoch"  # ì—í¬í¬ë§ˆë‹¤ ì €ì¥
        trainer.config['training']['logging_steps'] = 10  # ë¡œê¹… ë¹ˆë„ ì¦ê°€
        trainer.config['training']['load_best_model_at_end'] = False  # best model ë¡œë“œ ë¹„í™œì„±í™”
        trainer.config['training']['metric_for_best_model'] = None  # best model ë©”íŠ¸ë¦­ ë¹„í™œì„±í™”
    
    # ë°ì´í„° ì¤€ë¹„
    datasets = trainer.prepare_data(
        train_path=args.train_data,
        val_path=args.val_data,
        test_path=args.test_data
    )
    
    # í•™ìŠµ ì‹¤í–‰
    result = trainer.train(datasets)
    
    print(f"Training completed! Best ROUGE combined F1: {result.best_metrics.get('rouge_combined_f1', 0):.4f}")
