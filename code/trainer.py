"""
NLP ëŒ€í™” ìš”ì•½ ëª¨ë¸ í•™ìŠµ ëª¨ë“ˆ

baseline.ipynbì˜ í•µì‹¬ í•™ìŠµ ë¡œì§ì„ ëª¨ë“ˆí™”í•œ íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤.
WandB Sweepê³¼ì˜ í†µí•©ì„ ìœ„í•´ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ ì„¤ì •ì„ ì§€ì›í•©ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path

# .env íŒŒì¼ ë¡œë“œ (WandB API í‚¤ ë“±)
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("âš ï¸ python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    pass

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(Path(__file__).parent))
import json
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union
from dataclasses import dataclass, field
import warnings
warnings.filterwarnings("ignore")

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
import pandas as pd
from tqdm import tqdm

from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    AutoModelForCausalLM,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    EarlyStoppingCallback,
    TrainerCallback,
    TrainerState,
    TrainerControl,
    PreTrainedModel,
    PreTrainedTokenizer
)

# QLoRA ë° unsloth ê´€ë ¨ import (ì„ íƒì )
try:
    from unsloth import FastLanguageModel
    from peft import LoraConfig, get_peft_model, TaskType
    UNSLOTH_AVAILABLE = True
except ImportError:
    # macOS í™˜ê²½ì´ë‚˜ unslothê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°
    FastLanguageModel = None
    LoraConfig = None
    get_peft_model = None
    TaskType = None
    UNSLOTH_AVAILABLE = False

from datasets import Dataset, DatasetDict

# evaluate ëª¨ë“ˆ ì„ íƒì  import
try:
    import evaluate
    EVALUATE_AVAILABLE = True
except ImportError:
    print("âš ï¸ evaluate ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ROUGE ë©”íŠ¸ë¦­ ê³„ì‚°ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print("ğŸ‘‰ 'pip install evaluate' ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
    evaluate = None
    EVALUATE_AVAILABLE = False

import wandb
# ë¡œì»¬ ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
from utils import load_config
from utils.data_utils import DataProcessor
from utils.metrics import RougeCalculator
from utils.experiment_utils import ExperimentTracker, ModelRegistry
from utils.environment_detector import EnvironmentDetector, get_auto_config, should_use_unsloth
from utils.path_utils import PathManager, path_manager


logger = logging.getLogger(__name__)


# BART ëª¨ë¸ì„ ìœ„í•œ ì»¤ìŠ¤í…€ DataCollator
class SmartDataCollatorForSeq2Seq(DataCollatorForSeq2Seq):
    """
    ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ìë™ìœ¼ë¡œ token_type_ids ì²˜ë¦¬ë¥¼ ì¡°ì •í•˜ëŠ” DataCollator
    """
    
    def __init__(self, tokenizer, model=None, **kwargs):
        super().__init__(tokenizer, model, **kwargs)
        
        # ëª¨ë¸ íƒ€ì… í™•ì¸
        self.model_type = None
        if model is not None:
            model_class_name = model.__class__.__name__
            if "Bart" in model_class_name or "bart" in model_class_name.lower():
                self.model_type = "bart"
            elif "T5" in model_class_name or "t5" in model_class_name.lower():
                self.model_type = "t5"
            elif "MT5" in model_class_name or "mt5" in model_class_name.lower():
                self.model_type = "mt5"
                
    def __call__(self, features, return_tensors=None):
        """
        ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì ì ˆíˆ ì²˜ë¦¬ëœ ë°°ì¹˜ ë°˜í™˜
        """
        batch = super().__call__(features, return_tensors)
        
        # BART ëª¨ë¸ì¸ ê²½ìš° token_type_ids ì œê±°
        if self.model_type == "bart":
            if "token_type_ids" in batch:
                del batch["token_type_ids"]
            if "decoder_token_type_ids" in batch:
                del batch["decoder_token_type_ids"]
                
        return batch


@dataclass
class TrainingResult:
    """í•™ìŠµ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    best_metrics: Dict[str, float]
    final_metrics: Dict[str, float]
    model_path: str
    config_used: Dict[str, Any]
    training_history: List[Dict[str, Any]] = field(default_factory=list)
    wandb_run_id: Optional[str] = None
    experiment_id: Optional[str] = None
    submission_file: Optional[str] = None  # CSV ì œì¶œ íŒŒì¼ ê²½ë¡œ


class WandbCallback(TrainerCallback):
    """WandB ë¡œê¹…ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ì½œë°±"""
    
    def __init__(self, trainer_instance: 'DialogueSummarizationTrainer') -> None:
        self.trainer_instance = trainer_instance
        self.best_metrics = {}
        
    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, 
                   metrics: Dict[str, float], **kwargs):
        """í‰ê°€ ì‹œ WandBì— ë©”íŠ¸ë¦­ ë¡œê¹…"""
        if wandb.run is not None:
            # ROUGE ì ìˆ˜ ê²°í•© (F1 ê¸°ì¤€)
            rouge_combined = (
                metrics.get('eval_rouge1', 0) * 0.33 +
                metrics.get('eval_rouge2', 0) * 0.33 +
                metrics.get('eval_rougeL', 0) * 0.34
            )
            
            log_metrics = {
                'eval/rouge1_f1': metrics.get('eval_rouge1', 0),
                'eval/rouge2_f1': metrics.get('eval_rouge2', 0),
                'eval/rougeL_f1': metrics.get('eval_rougeL', 0),
                'eval/rouge_combined_f1': rouge_combined,
                'eval/loss': metrics.get('eval_loss', 0),
                'epoch': state.epoch,
                'step': state.global_step
            }
            
            # ë² ìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            if rouge_combined > self.best_metrics.get('rouge_combined_f1', 0):
                self.best_metrics = {
                    'rouge1_f1': metrics.get('eval_rouge1', 0),
                    'rouge2_f1': metrics.get('eval_rouge2', 0),
                    'rougeL_f1': metrics.get('eval_rougeL', 0),
                    'rouge_combined_f1': rouge_combined,
                    'loss': metrics.get('eval_loss', 0)
                }
                log_metrics['best/rouge_combined_f1'] = rouge_combined
            
            wandb.log(log_metrics)
            # ì‹¤í—˜ ì¶”ì ê¸°ì—ë„ ë¡œê¹…
            if self.trainer_instance.experiment_tracker:
                self.trainer_instance.experiment_tracker.log_metrics(
                    metrics, step=state.global_step
                )
    
    def on_train_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):
        """í•™ìŠµ ì¢…ë£Œ ì‹œ ìµœì¢… ê²°ê³¼ ë¡œê¹…"""
        if wandb.run is not None:
            wandb.run.summary.update(self.best_metrics)


class DialogueSummarizationTrainer:
    """
    ëŒ€í™” ìš”ì•½ ëª¨ë¸ í•™ìŠµ íŠ¸ë ˆì´ë„ˆ
    
    baseline.ipynbì˜ í•™ìŠµ ë¡œì§ì„ ëª¨ë“ˆí™”í•˜ê³  WandB Sweepê³¼ í†µí•©í•˜ì—¬
    ìƒì‚°ì„± ë†’ì€ ì‹¤í—˜ í™˜ê²½ì„ ì œê³µí•©ë‹ˆë‹¤.
    
    Features:
        - ë‹¤ì¤‘ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì§€ì› (BART, T5, KoBART ë“±)
        - ìë™ ë””ë°”ì´ìŠ¤ ê°ì§€ ë° ìµœì í™” (CUDA, MPS, CPU)
        - ì‹¤í—˜ ì¶”ì  ë° ëª¨ë¸ ë“±ë¡ ì‹œìŠ¤í…œ
        - ì»¤ìŠ¤í…€ ì½œë°± ë° ë©”íŠ¸ë¦­ ê³„ì‚°
        - í¬ê´„ì  ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…
        - WandB í†µí•© ì‹¤í—˜ ê´€ë¦¬
        
    Example:
        >>> config = load_config('configs/bart_base.yaml')
        >>> trainer = DialogueSummarizationTrainer(config)
        >>> datasets = trainer.prepare_data()
        >>> result = trainer.train(datasets)
    """
    
    def __init__(self, config: Dict[str, Any], 
                 sweep_mode: bool = False,
                 experiment_name: Optional[str] = None):
        """
        íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬ (ConfigManagerë¡œë¶€í„°)
            sweep_mode: WandB Sweep ëª¨ë“œ ì—¬ë¶€
            experiment_name: ì‹¤í—˜ëª… (Noneì´ë©´ ìë™ ìƒì„±)
        """
        self.config = config
        self.sweep_mode = sweep_mode
        self.experiment_name = experiment_name or config.get('meta', {}).get('experiment_name', 'dialogue_summarization')
        
        # í™˜ê²½ ìë™ ê°ì§€ ë° ì„¤ì • ìµœì í™”
        self.env_detector = EnvironmentDetector()
        self.env_info = self.env_detector.detect_environment()
        self.auto_config = self.env_detector.get_recommended_config()
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = self._setup_device()
        
        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        self.model = None
        self.tokenizer = None
        self.data_processor = None
        self.rouge_calculator = None
        self.trainer = None
        
        # ì‹¤í—˜ ê´€ë¦¬
        self.experiment_tracker = None
        self.model_registry = None
        
        # ë¡œê¹… ì„¤ì •
        # ë¡œê¹… ì„¤ì •
        self._setup_logging()
        
        # í™˜ê²½ ì •ë³´ ì¶œë ¥
        self._print_environment_info()
        
        logger.info(f"Trainer initialized with config: {self.experiment_name}")
        
        def setup_wandb_with_korean_time(self, config: Dict[str, Any]) -> bool:
            """
            í•œêµ­ ì‹œê°„ ê¸°ë°˜ WandB ì´ˆê¸°í™”
            
            Args:
                config: ì‹¤í—˜ ì„¤ì • ë”•ì…”ë„ˆë¦¬
                
            Returns:
                WandB í™œì„±í™” ì—¬ë¶€
            """
            if os.environ.get("WANDB_MODE") == "disabled":
                print("âš ï¸ WandBê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                return False
                
            try:
                from utils.experiment_utils import get_wandb_run_name_with_korean_time
                korean_time = get_wandb_run_name_with_korean_time()
                
                # ê¸°ì¡´ wandb ì„¤ì •ì„ í•œêµ­ ì‹œê°„ìœ¼ë¡œ ê°œì„ 
                wandb_config = config.get('wandb', {})
                original_name = wandb_config.get('name', self.experiment_name)
                wandb_config['name'] = f"{original_name}_{korean_time}"
                
                # WandB ì´ˆê¸°í™” (ì´ë¯¸ ì´ˆê¸°í™”ëœ ê²½ìš° ìŠ¤í‚¨)
                if wandb.run is None:
                    wandb.init(
                        entity=wandb_config.get('entity', 'lyjune37-juneictlab'),
                        project=wandb_config.get('project', 'nlp-5'),
                        name=wandb_config['name'],
                        config=config,
                        tags=wandb_config.get('tags', []) + ['rtx3090_optimized', 'korean_time']
                    )
                    print(f"âœ… WandB ì´ˆê¸°í™” ì™„ë£Œ: {wandb_config['name']}")
                else:
                    print(f"â„¹ï¸ WandB ì´ë¯¸ ì´ˆê¸°í™”ë¨: {wandb.run.name}")
                
                return True
                
            except ImportError as e:
                print(f"âš ï¸ í•œêµ­ ì‹œê°„ ìœ í‹¸ë¦¬í‹° import ì‹¤íŒ¨: {e}")
                return False
            except Exception as e:
                print(f"âš ï¸ WandB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                return False
        
        def save_best_model_as_artifact(self, model_path: str, metrics: Dict[str, float]) -> None:
            """
            Best modelì„ WandB Artifactsë¡œ ì €ì¥
            
            Args:
                model_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
                metrics: ì„±ëŠ¥ ë©”íŠ¸ë¦­
            """
            if wandb.run is None:
                print("âš ï¸ WandBê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ Artifacts ì €ì¥ì„ ê±´ë„ˆë—ë‹ˆë‹¤.")
                return
                
            try:
                from utils.experiment_utils import get_korean_time_format
                korean_time = get_korean_time_format('MMDDHHMM')
                
                # ROUGE ì¢…í•© ì ìˆ˜ ê³„ì‚°
                rouge_combined = metrics.get('rouge_combined_f1', 0)
                if rouge_combined == 0:
                    # ëŒ€ì²´ ê³„ì‚° ë°©ë²•
                    rouge1 = metrics.get('eval_rouge1_f1', 0) or metrics.get('rouge1_f1', 0)
                    rouge2 = metrics.get('eval_rouge2_f1', 0) or metrics.get('rouge2_f1', 0)
                    rougeL = metrics.get('eval_rougeL_f1', 0) or metrics.get('rougeL_f1', 0)
                    rouge_combined = (rouge1 + rouge2 + rougeL) / 3
                
                # Artifact ìƒì„±
                artifact = wandb.Artifact(
                    name=f"best-model-{korean_time}",
                    type="model",
                    description=f"Best model (ROUGE-F1: {rouge_combined:.4f}) - Korean time: {korean_time}"
                )
                
                # ëª¨ë¸ ë””ë ‰í† ë¦¬ ì¶”ê°€
                model_path = Path(model_path)
                if model_path.exists():
                    artifact.add_dir(str(model_path))
                    
                    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    artifact.metadata = {
                        "best_metrics": metrics,
                        "korean_time": korean_time,
                        "model_path": str(model_path),
                        "rouge_combined_f1": rouge_combined
                    }
                    
                    # Artifact ë¡œê¹…
                    wandb.log_artifact(artifact, aliases=["latest", "best"])
                    print(f"âœ… WandB Artifacts ì €ì¥ ì™„ë£Œ: {artifact.name}")
                    print(f"   ëª¨ë¸ ê²½ë¡œ: {model_path}")
                    print(f"   ROUGE-F1: {rouge_combined:.4f}")
                else:
                    print(f"âš ï¸ ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
                    
            except Exception as e:
                print(f"âš ï¸ WandB Artifacts ì €ì¥ ì‹¤íŒ¨: {e}")
        
        def setup_paths(self) -> None:
            """ê²½ë¡œ ì„¤ì •"""
            # ê²½ë¡œ ê´€ë¦¬ìë¥¼ ì‚¬ìš©í•˜ì—¬ ê²½ë¡œ ê´€ë¦¬
            experiment_name = self.experiment_name
            
            # Sweep ëª¨ë“œì¼ ë•ŒëŠ” run IDë¥¼ í¬í•¨
            if self.sweep_mode and wandb.run:
                experiment_name = f"sweep_{wandb.run.id}"
            else:
                from datetime import datetime
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                experiment_name = f"{self.experiment_name}_{timestamp}"
            
            # ê²½ë¡œ ê´€ë¦¬ìë¥¼ í†µí•œ ê²½ë¡œ ì„¤ì •
            self.output_dir = path_manager.get_output_path(experiment_name)
            self.model_save_dir = path_manager.get_model_path(experiment_name)
            self.results_dir = path_manager.ensure_dir(self.output_dir / "results")
            
            # ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
            self.log_dir = path_manager.get_log_path(experiment_name)
        
    def initialize_components(self) -> None:
        """ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        logger.info("Initializing components...")
        
        # ì‹¤í—˜ ì¶”ì ê¸° ì´ˆê¸°í™”
        if self.config.get('experiment_tracking', {}).get('enabled', True):
            self.experiment_tracker = ExperimentTracker(
                experiments_dir=self.output_dir / "experiments"
            )
            self.model_registry = ModelRegistry(
                registry_dir=self.output_dir / "models"
            )
        
        # í† í¬ë‚˜ì´ì € ë¡œë”©
        self._load_tokenizer()
        
        # ëª¨ë¸ ë¡œë”©
        self._load_model()
        
        # ë°ì´í„° í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        self.data_processor = DataProcessor(
            tokenizer=self.tokenizer,
            config=self.config
        )
        
        # ROUGE ê³„ì‚°ê¸° ì´ˆê¸°í™”
        self.rouge_calculator = RougeCalculator(
            use_korean_tokenizer=self.config.get('evaluation', {}).get('rouge_tokenize_korean', True),
            use_stemmer=self.config.get('evaluation', {}).get('rouge_use_stemmer', True)
        )
        
        logger.info("All components initialized successfully")
    
    def prepare_data(self, train_path: Optional[str] = None, 
                    val_path: Optional[str] = None,
                    test_path: Optional[str] = None) -> DatasetDict:
        """
        ë°ì´í„° ì¤€ë¹„
        
        Args:
            train_path: í•™ìŠµ ë°ì´í„° ê²½ë¡œ
            val_path: ê²€ì¦ ë°ì´í„° ê²½ë¡œ  
            test_path: í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ
            
        Returns:
            ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ë”•ì…”ë„ˆë¦¬
        """
        # ê¸°ë³¸ ë°ì´í„° ê²½ë¡œ ì„¤ì •
        data_path = self.config.get('general', {}).get('data_path', 'data/')
        
        # ìƒëŒ€ ê²½ë¡œë¥¼ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì²˜ë¦¬
        if not Path(data_path).is_absolute():
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸° (trainer.pyëŠ” code/ ë””ë ‰í† ë¦¬ì— ìˆìŒ)
            project_root = Path(__file__).parent.parent
            base_path = project_root / data_path
        else:
            base_path = Path(data_path)
        
        # ê²½ë¡œê°€ ì—†ìœ¼ë©´ configì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ íŒŒì¼ëª… ì‚¬ìš©
        if train_path is None:
            train_path = self.config.get('general', {}).get('train_path')
            if train_path is None:
                train_path = str(base_path / 'train.csv')
        if val_path is None:
            val_path = self.config.get('general', {}).get('val_path')
            if val_path is None:
                val_path = str(base_path / 'dev.csv')
        if test_path is None:
            test_path = self.config.get('general', {}).get('test_path')
            if test_path is None and (base_path / 'test.csv').exists():
                test_path = str(base_path / 'test.csv')
        
        logger.info("Loading and processing datasets...")
        logger.info(f"Project root: {Path(__file__).parent.parent}")
        logger.info(f"Base data path: {base_path}")
        logger.info(f"Train path: {train_path}")
        logger.info(f"Val path: {val_path}")
        
        datasets = {}
        
        if train_path and Path(train_path).exists():
            train_data = self.data_processor.load_data(train_path)
            datasets['train'] = self.data_processor.process_data(
                train_data, 
                is_training=True
            )
            logger.info(f"Train dataset size: {len(datasets['train'])}")
        else:
            logger.warning(f"Train data not found at {train_path}")
        
        if val_path and Path(val_path).exists():
            val_data = self.data_processor.load_data(val_path)
            datasets['validation'] = self.data_processor.process_data(
                val_data,
                is_training=False
            )
            logger.info(f"Validation dataset size: {len(datasets['validation'])}")
        else:
            logger.warning(f"Validation data not found at {val_path}")
        
        if test_path and Path(test_path).exists():
            test_data = self.data_processor.load_data(test_path, is_test=True)
            datasets['test'] = self.data_processor.process_data(
                test_data,
                is_training=False,
                is_test=True
            )
            logger.info(f"Test dataset size: {len(datasets['test'])}")
        
        return DatasetDict(datasets)
    
    def train(self, dataset: DatasetDict, 
             resume_from_checkpoint: Optional[str] = None) -> TrainingResult:
        """
        ëª¨ë¸ í•™ìŠµ
        
        Args:
            dataset: í•™ìŠµ/ê²€ì¦ ë°ì´í„°ì…‹
            resume_from_checkpoint: ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (ì¬ê°œ ì‹œ)
            
        Returns:
            í•™ìŠµ ê²°ê³¼
        """
        # ì‹¤í—˜ ì‹œì‘
        if self.experiment_tracker:
            experiment_id = self.experiment_tracker.start_experiment(
                name=self.experiment_name,
                description=f"Training {self._get_model_architecture()} model",
                config=self.config,
                model_type=self._get_model_architecture(),
                dataset_info={
                    'train_size': len(dataset.get('train', [])),
                    'val_size': len(dataset.get('validation', []))
                },
                wandb_run_id=wandb.run.id if wandb.run else None
            )
        else:
            experiment_id = None
        
        # í•™ìŠµ ì¸ì ì„¤ì •
        training_args = self._get_training_arguments()
        
        # ë°ì´í„° ì½œë ˆì´í„° - ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ SmartDataCollatorForSeq2Seq ì‚¬ìš©
        data_collator = SmartDataCollatorForSeq2Seq(
            tokenizer=self.tokenizer,
            model=self.model,
            padding=True,
            max_length=self.config['tokenizer']['encoder_max_len']
        )
        
        # í‰ê°€ ë©”íŠ¸ë¦­ í•¨ìˆ˜ - HuggingFace Trainerì˜ ì½œë°±ìœ¼ë¡œ ì‚¬ìš©ë¨
        def compute_metrics(eval_preds: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:
            """
            í•™ìŠµ ì¤‘ í‰ê°€ ë‹¨ê³„ì—ì„œ ROUGE ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•˜ëŠ” ì¤‘ì²© í•¨ìˆ˜
            
            Args:
                eval_preds: (predictions, labels) íŠœí”Œ
                
            Returns:
                ROUGE ì ìˆ˜ë“¤ì„ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
            """
            preds, labels = eval_preds
            
            # í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”© (íŠ¹ìˆ˜ í† í° ì œê±°)
            # ë¨¼ì € predictions ë””ì½”ë”© (ë³´í†µ ë¬¸ì œì—†ìŒ)
            try:
                decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True)
            except Exception as e:
                logger.warning(f"âš ï¸  Predictions ë””ì½”ë”© ì˜¤ë¥˜: {e}")
                decoded_preds = ["" for _ in range(len(preds))]
            
            # labels ì•ˆì „ ë””ì½”ë”© (OverflowError ë°©ì§€)
            try:
                # HuggingFaceì—ì„œ ì‚¬ìš©í•˜ëŠ” -100 íŒ¨ë”© í† í°ì„ ì •ìƒ í† í°ìœ¼ë¡œ ë³€í™˜
                # -100ì€ loss ê³„ì‚°ì—ì„œ ë¬´ì‹œë˜ëŠ” ë¼ë²¨ì´ì§€ë§Œ ë””ì½”ë”©ì—ì„œëŠ” ë¬¸ì œê°€ ë¨
                labels_fixed = np.where(labels != -100, labels, self.tokenizer.pad_token_id)
                
                # í† í° ID ë²”ìœ„ ê²€ì¦ (ì¶”ê°€ ì•ˆì „ì¥ì¹˜)
                vocab_size = getattr(self.tokenizer, 'vocab_size', 50000)  # ê¸°ë³¸ê°’ ì„¤ì •
                labels_fixed = np.clip(labels_fixed, 0, vocab_size - 1)
                
                # ë°ì´í„° íƒ€ì… í™•ì¸ ë° ë³€í™˜
                if hasattr(labels_fixed, 'astype'):
                    labels_fixed = labels_fixed.astype(np.int32)
                
                decoded_labels = self.tokenizer.batch_decode(labels_fixed, skip_special_tokens=True)
                
            except Exception as e:
                logger.warning(f"âš ï¸  Labels ë””ì½”ë”© ì˜¤ë¥˜: {e}")
                # í´ë°±: ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
                decoded_labels = ["" for _ in range(len(labels))]
            
            # ëŒ€í™” ìš”ì•½ì— íŠ¹í™”ëœ ROUGE ë©”íŠ¸ë¦­ ê³„ì‚° (Multi-reference ì§€ì›)
            try:
                # ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸ë¡œ ì§ì ‘ ROUGE ê³„ì‚°
                rouge_scores = []
                for pred, ref in zip(decoded_preds, decoded_labels):
                    if pred.strip() and ref.strip():  # ë¹ˆ ë¬¸ìì—´ ë°©ì§€
                        score = self.rouge_calculator.calculate_single_reference(pred, ref)
                        rouge_scores.append(score)
                    else:
                        # ë¹ˆ ë¬¸ìì—´ì— ëŒ€í•œ 0ì  ì²˜ë¦¬
                        from utils.metrics import RougeScore, EvaluationResult
                        zero_rouge = RougeScore(precision=0.0, recall=0.0, f1=0.0)
                        zero_result = EvaluationResult(
                            rouge1=zero_rouge, rouge2=zero_rouge, rougeL=zero_rouge, rouge_combined_f1=0.0
                        )
                        rouge_scores.append(zero_result)
                
                if rouge_scores:
                    # í‰ê·  ì ìˆ˜ ê³„ì‚°
                    avg_rouge1_f1 = sum(score.rouge1.f1 for score in rouge_scores) / len(rouge_scores)
                    avg_rouge2_f1 = sum(score.rouge2.f1 for score in rouge_scores) / len(rouge_scores)
                    avg_rougeL_f1 = sum(score.rougeL.f1 for score in rouge_scores) / len(rouge_scores)
                    avg_combined_f1 = avg_rouge1_f1 + avg_rouge2_f1 + avg_rougeL_f1
                    
                    result = {
                        'rouge1_f1': avg_rouge1_f1,
                        'rouge2_f1': avg_rouge2_f1,
                        'rougeL_f1': avg_rougeL_f1,
                        'rouge_combined_f1': avg_combined_f1
                    }
                else:
                    # ëª¨ë“  ìƒ˜í”Œì´ ë¹„ì–´ìˆëŠ” ê²½ìš°
                    result = {
                        'rouge1_f1': 0.0,
                        'rouge2_f1': 0.0,
                        'rougeL_f1': 0.0,
                        'rouge_combined_f1': 0.0
                    }
                    
            except Exception as e:
                logger.warning(f"âš ï¸  ROUGE ê³„ì‚° ì˜¤ë¥˜: {e}")
                # í´ë°±: 0ì  ë°˜í™˜
                result = {
                    'rouge1_f1': 0.0,
                    'rouge2_f1': 0.0, 
                    'rougeL_f1': 0.0,
                    'rouge_combined_f1': 0.0
                }
            
            return result
        
        # ì½œë°± ì„¤ì •
        callbacks = [WandbCallback(self)]
        
        # ì¡°ê¸° ì¢…ë£Œ ì„¤ì •
        if self.config['training'].get('early_stopping_patience'):
            callbacks.append(
                EarlyStoppingCallback(
                    early_stopping_patience=self.config['training']['early_stopping_patience'],
                    early_stopping_threshold=0.001
                )
            )
        
        # íŠ¸ë ˆì´ë„ˆ ìƒì„±
        self.trainer = Seq2SeqTrainer(
            model=self.model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=dataset.get('train'),
            eval_dataset=dataset.get('validation'),
            tokenizer=self.tokenizer,
            compute_metrics=compute_metrics,
            callbacks=callbacks
        )
        
        # í•™ìŠµ ì‹œì‘
        logger.info("Starting training...")
        
        try:
            train_result = self.trainer.train(
                resume_from_checkpoint=resume_from_checkpoint
            )
            
            # ìµœì¢… í‰ê°€
            logger.info("Running final evaluation...")
            eval_results = self.trainer.evaluate()
            
            # ëª¨ë¸ ì €ì¥
            best_model_path = self.model_save_dir / "best_model"
            self.trainer.save_model(str(best_model_path))
            self.tokenizer.save_pretrained(str(best_model_path))
            
            # ê²°ê³¼ ì •ë¦¬
            wandb_callback = callbacks[0]
            training_result = TrainingResult(
                best_metrics=wandb_callback.best_metrics,
                final_metrics=eval_results,
                model_path=str(best_model_path),
                config_used=self.config,
                training_history=[], # í–¥í›„ êµ¬í˜„
                wandb_run_id=wandb.run.id if wandb.run else None,
                experiment_id=experiment_id
            )
            
            # ì‹¤í—˜ ì¢…ë£Œ
            if self.experiment_tracker:
                self.experiment_tracker.end_experiment(
                    experiment_id=experiment_id,
                    final_metrics=eval_results,
                    best_metrics=wandb_callback.best_metrics,
                    status="completed"
                )
            
            # ëª¨ë¸ ë“±ë¡
            if self.model_registry:
                model_id = self.model_registry.register_model(
                    name=f"{self._get_model_architecture()}_{self.experiment_name}",
                    architecture=self._get_model_architecture(),
                    checkpoint=self._get_model_checkpoint(),
                    config=self.config,
                    performance=wandb_callback.best_metrics,
                    training_info={
                        'epochs': self.config['training']['num_train_epochs'],
                        'batch_size': self.config['training']['per_device_train_batch_size'],
                        'learning_rate': self.config['training']['learning_rate']
                    },
                    file_path=str(best_model_path),
                    experiment_id=experiment_id
                )
                logger.info(f"Model registered with ID: {model_id}")
            
            # ê²°ê³¼ ì €ì¥
            self._save_results(training_result)

            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì´ ìˆìœ¼ë©´ ì˜ˆì¸¡ ë° CSV ìƒì„±
            if 'test' in dataset:
                logger.info("ğŸ”® í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ ìƒì„± ì¤‘...")
                try:
                    # ì˜ˆì¸¡ ìƒì„±
                    test_predictions = self.generate_test_predictions(dataset['test'])
                    
                    # CSV íŒŒì¼ ìƒì„±
                    submission_path = self._save_submission_csv(test_predictions)
                    logger.info(f"âœ… ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ: {submission_path}")
                    
                    # ê²°ê³¼ì— ì¶”ê°€
                    training_result.submission_file = str(submission_path)
                    
                    # ê²°ê³¼ ë‹¤ì‹œ ì €ì¥ (ì œì¶œ íŒŒì¼ ê²½ë¡œ í¬í•¨)
                    self._save_results(training_result)
                except Exception as e:
                    logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ìƒì„± ì‹¤íŒ¨: {e}")
                    import traceback
                    logger.debug(traceback.format_exc())
                    # ì˜ˆì¸¡ ì‹¤íŒ¨í•´ë„ í•™ìŠµì€ ì„±ê³µí•œ ê²ƒìœ¼ë¡œ ì²˜ë¦¬

            
                        # WandB Artifactsë¡œ best model ì €ì¥
                        try:
                            if training_result.best_metrics and training_result.model_path:
                                self.save_best_model_as_artifact(
                                    model_path=training_result.model_path,
                                    metrics=training_result.best_metrics
                                )
                        except Exception as e:
                            logger.warning(f"âš ï¸ WandB Artifacts ì €ì¥ ì‹¤íŒ¨: {e}")
                            # Artifacts ì €ì¥ ì‹¤íŒ¨ëŠ” ì „ì²´ í•™ìŠµì„ ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ
            
            return training_result
            
        except Exception as e:
            logger.error(f"Training failed with error: {type(e).__name__}: {str(e)}")
            logger.error(f"Current config: {self.config.get('model', {}).get('checkpoint', 'Unknown')}")
            logger.error(f"Device: {self.device}")
            import traceback
            logger.debug(f"Full traceback: {traceback.format_exc()}")
            logger.error(f"Training failed: {str(e)}")
            if self.experiment_tracker and experiment_id:
                self.experiment_tracker.end_experiment(
                    experiment_id=experiment_id,
                    status="failed",
                    notes=str(e)
                )
            raise
    
    def evaluate(self, dataset: Dataset, 
                metric_key_prefix: str = "eval") -> Dict[str, float]:
        """
        ëª¨ë¸ í‰ê°€
        
        Args:
            dataset: í‰ê°€ ë°ì´í„°ì…‹
            metric_key_prefix: ë©”íŠ¸ë¦­ í‚¤ ì ‘ë‘ì‚¬
            
        Returns:
            í‰ê°€ ê²°ê³¼
        """
        if not self.trainer:
            raise ValueError("Trainer not initialized. Call train() first.")
        
        results = self.trainer.evaluate(
            eval_dataset=dataset,
            metric_key_prefix=metric_key_prefix
        )
        
        return results
    
    def generate_predictions(self, dataset: Dataset, 
                           max_samples: Optional[int] = None) -> List[Dict[str, str]]:
        """
        ì˜ˆì¸¡ ìƒì„±
        
        Args:
            dataset: ì…ë ¥ ë°ì´í„°ì…‹
            max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ì „ì²´)
            
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        self.model.eval()
        predictions = []
        
        # ìƒ˜í”Œë§
        if max_samples:
            indices = np.random.choice(len(dataset), min(max_samples, len(dataset)), replace=False)
            dataset = dataset.select(indices)
        
        # ìƒì„± ì„¤ì •
        gen_config = self.config['generation']
        
        with torch.no_grad():
            for example in tqdm(dataset, desc="Generating predictions"):
                # í† í°í™”
                inputs = self.tokenizer(
                    example['input'],
                    max_length=self.config['tokenizer']['encoder_max_len'],
                    truncation=True,
                    padding=True,
                    return_tensors="pt"
                ).to(self.device)
                
                # ìƒì„±
                outputs = self.model.generate(
                    **inputs,
                    max_length=gen_config['max_length'],
                    num_beams=gen_config['num_beams'],
                    length_penalty=gen_config.get('length_penalty', 1.0),
                    no_repeat_ngram_size=gen_config.get('no_repeat_ngram_size', 2),
                    early_stopping=gen_config.get('early_stopping', True),
                    do_sample=gen_config.get('do_sample', False),
                    temperature=gen_config.get('temperature', 1.0) if gen_config.get('do_sample') else None,
                    top_k=gen_config.get('top_k', 50) if gen_config.get('do_sample') else None,
                    top_p=gen_config.get('top_p', 0.95) if gen_config.get('do_sample') else None
                )
                
                # ë””ì½”ë”©
                prediction = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                predictions.append({
                    'input': example['input'],
                    'prediction': prediction,
                    'reference': example.get('target', '')
                })
        
        return predictions
    
    def _setup_device(self) -> torch.device:
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        from utils.device_utils import get_optimal_device, setup_device_config
        
        device_config = self.config['general'].get('device', 'auto')
        
        if device_config == 'auto':
            # ìµœì  ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ
            device, device_info = get_optimal_device()
            
            # ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            model_size = self.config.get('model', {}).get('size', 'base')
            optimization_config = setup_device_config(device_info, model_size)
            
            # ìµœì í™” ì„¤ì •ì„ configì— ë³‘í•©
            if 'training' not in self.config:
                self.config['training'] = {}
            
            # ê¸°ì¡´ ì„¤ì •ê³¼ ë³‘í•© (ê¸°ì¡´ ì„¤ì • ìš°ì„ )
            opt_dict = optimization_config.to_dict()
            for key, value in opt_dict.items():
                if key not in self.config['training']:
                    self.config['training'][key] = value
            
            logger.info(f"ìë™ ê°ì§€ëœ ë””ë°”ì´ìŠ¤: {device_info}")
            logger.info(f"ìµœì í™” ì„¤ì • ì ìš©ë¨: batch_size={optimization_config.batch_size}, "
                       f"mixed_precision={optimization_config.mixed_precision}, "
                       f"num_workers={optimization_config.num_workers}")
        else:
            # ìˆ˜ë™ ì„¤ì •
            device = torch.device(device_config)
            logger.info(f"ìˆ˜ë™ ì„¤ì •ëœ ë””ë°”ì´ìŠ¤: {device}")
        
        logger.info(f"Using device: {device}")
        return device
    
    def _setup_logging(self) -> None:
        """ë¡œê¹… ì„¤ì •"""
        log_level = self.config.get('logging', {}).get('level', 'INFO')
        logging.basicConfig(
            level=getattr(logging, log_level),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.output_dir / 'training.log'),
                logging.StreamHandler(sys.stdout)
            ]
        )
    
    def _print_environment_info(self) -> None:
        """í™˜ê²½ ì •ë³´ ì¶œë ¥"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ” ìë™ í™˜ê²½ ê°ì§€ ê²°ê³¼")
        logger.info("="*60)
        logger.info(f"OS: {self.env_info['os']} ({self.env_info['os_release']})")
        logger.info(f"Python: {self.env_info['python_version']}")
        logger.info(f"CPU Cores: {self.env_info['cpu_count']}")
        
        if self.env_info['is_cuda_available']:
            logger.info(f"ğŸ® CUDA: Available (v{self.env_info['cuda_version']})")
            logger.info(f"GPU Count: {self.env_info['gpu_info']['count']}")
            for device in self.env_info['gpu_info']['devices']:
                logger.info(f"  - {device['name']}: {device['memory_gb']:.1f}GB")
        else:
            logger.info("ğŸ® CUDA: Not Available")
        
        logger.info(f"\nâš¡ Unsloth ì§€ì›")
        unsloth_status = "âœ… ì¶”ì²œ" if self.env_info['unsloth_recommended'] else "âŒ ë¹„ì¶”ì²œ"
        logger.info(f"ì¶”ì²œ ì—¬ë¶€: {unsloth_status}")
        
        install_status = "âœ… ì„¤ì¹˜ë¨" if self.env_info['unsloth_available'] else "âŒ ë¯¸ì„¤ì¹˜"
        logger.info(f"ì„¤ì¹˜ ìƒíƒœ: {install_status}")
        
        logger.info(f"\nğŸš€ ìë™ ìµœì í™” ì„¤ì •")
        logger.info(f"use_unsloth: {self.auto_config['use_unsloth']}")
        logger.info(f"recommended_batch_size: {self.auto_config['recommended_batch_size']}")
        logger.info(f"fp16: {self.auto_config['fp16']}, bf16: {self.auto_config['bf16']}")
        logger.info(f"dataloader_num_workers: {self.auto_config['dataloader_num_workers']}")
        logger.info("="*60 + "\n")
    
    def _load_tokenizer(self) -> None:
        """í† í¬ë‚˜ì´ì € ë¡œë”©"""
        # model ì„¹ì…˜ì´ ì—†ìœ¼ë©´ generalì—ì„œ model_name ì‚¬ìš©
        if 'model' in self.config:
            model_checkpoint = self.config['model']['checkpoint']
        else:
            model_checkpoint = self.config.get('general', {}).get('model_name')
            if not model_checkpoint:
                raise ValueError("Model checkpoint not found in config. Please specify 'model.checkpoint' or 'general.model_name'")
        
        logger.info(f"Loading tokenizer: {model_checkpoint}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_checkpoint,
            use_fast=True
        )
        
        # íŠ¹ìˆ˜ í† í° ì„¤ì • (í•„ìš”ì‹œ)
        if 'model' in self.config and self.config['model']['architecture'] in ['kogpt2', 'gpt2']:
            # GPT ê³„ì—´ì€ pad_tokenì´ ì—†ì„ ìˆ˜ ìˆìŒ
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
    def _get_model_architecture(self) -> str:
        """ëª¨ë¸ ì•„í‚¤í…ì²˜ ê°€ì ¸ì˜¤ê¸°"""
        if 'model' in self.config:
            return self.config['model']['architecture']
        else:
            # model_nameì—ì„œ ì¶”ë¡ 
            model_checkpoint = self.config.get('general', {}).get('model_name', '')
            if 'kobart' in model_checkpoint.lower():
                return 'bart'
            elif 't5' in model_checkpoint.lower():
                return 't5'
            elif 'mt5' in model_checkpoint.lower():
                return 'mt5'
            else:
                return 'seq2seq'
    
    def _get_model_checkpoint(self) -> str:
        """ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        if 'model' in self.config:
            return self.config['model']['checkpoint']
        else:
            model_checkpoint = self.config.get('general', {}).get('model_name')
            if not model_checkpoint:
                raise ValueError("Model checkpoint not found in config")
            return model_checkpoint
    def _load_model(self) -> None:
        """ëª¨ë¸ ë¡œë”© (unsloth ë° QLoRA ì§€ì›)"""
        # model ì„¹ì…˜ì´ ì—†ìœ¼ë©´ generalì—ì„œ model_name ì‚¬ìš©
        if 'model' in self.config:
            model_checkpoint = self.config['model']['checkpoint']
            architecture = self._get_model_architecture()
        else:
            model_checkpoint = self.config.get('general', {}).get('model_name')
            if not model_checkpoint:
                raise ValueError("Model checkpoint not found in config")
            # architectureë¥¼ ëª¨ë¸ ì´ë¦„ì—ì„œ ì¶”ë¡ 
            if 'kobart' in model_checkpoint.lower():
                architecture = 'BART'
            elif 't5' in model_checkpoint.lower():
                architecture = 'T5'
            elif 'mt5' in model_checkpoint.lower():
                architecture = 'mT5'
            else:
                architecture = 'seq2seq'  # ê¸°ë³¸ê°’
        
        # QLoRA ì„¤ì • í™•ì¸
        qlora_config = self.config.get('qlora', {})
        # QLoRA ì„¤ì • í™•ì¸ ë° ìë™ ìµœì í™”
        qlora_config = self.config.get('qlora', {})
        
        # í™˜ê²½ ê¸°ë°˜ ìë™ Unsloth í™œì„±í™”
        config_use_unsloth = qlora_config.get('use_unsloth', False)
        auto_use_unsloth = self.auto_config.get('use_unsloth', False)
        
        # ìµœì¢… Unsloth ì‚¬ìš© ê²°ì •: ì„¤ì •íŒŒì¼ OR ìë™ê°ì§€
        use_unsloth = (config_use_unsloth or auto_use_unsloth) and UNSLOTH_AVAILABLE
        use_qlora = qlora_config.get('use_qlora', True)  # ê¸°ë³¸ê°’ Trueë¡œ ë³€ê²½
        
        # ìë™ ìµœì í™” ì ìš©
        if auto_use_unsloth and not config_use_unsloth:
            logger.info(f"ğŸš€ í™˜ê²½ ìë™ ê°ì§€: Ubuntu + CUDA í™˜ê²½ì—ì„œ Unsloth ìë™ í™œì„±í™”")
            # ìë™ ë°°ì¹˜ í¬ê¸° ì ìš©
            recommended_batch = self.auto_config.get('recommended_batch_size', 4)
            logger.info(f"ğŸ“Š ê¸°ë³¸ ë°°ì¹˜ í¬ê¸° ê¶Œì¥: {recommended_batch}")
        
        logger.info(f"Loading model: {model_checkpoint} ({architecture})")
        logger.info(f"QLoRA enabled: {use_qlora}, unsloth enabled: {use_unsloth}")
        
        if use_unsloth and architecture in ['kobart', 'bart', 't5', 'mt5']:
            # unslothë¡œ ëª¨ë¸ ë¡œë”© (ìµœëŒ€ 75% ë©”ëª¨ë¦¬ ê°ì†Œ)
            self._load_model_with_unsloth(model_checkpoint, qlora_config)
            
        elif use_qlora:
            # ì¼ë°˜ QLoRA ëª¨ë¸ ë¡œë”©
            self._load_model_with_qlora(model_checkpoint, architecture, qlora_config)
            
        else:
            # ê¸°ì¡´ ëª¨ë¸ ë¡œë”© ë°©ì‹
            self._load_model_standard(model_checkpoint, architecture)
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (QLoRA ëª¨ë¸ì€ ì´ë¯¸ ì ì ˆí•œ ë””ë°”ì´ìŠ¤ì— ìˆìŒ)
        if not (use_unsloth or use_qlora):
            self.model = self.model.to(self.device)
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… (ë©”ëª¨ë¦¬ ìµœì í™”)
        if self.config['training'].get('gradient_checkpointing', False):
            if hasattr(self.model, 'gradient_checkpointing_enable'):
                self.model.gradient_checkpointing_enable()
            else:
                logger.warning("ëª¨ë¸ì´ gradient_checkpointingì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                
    def _load_model_with_unsloth(self, model_checkpoint: str, qlora_config: Dict[str, Any]) -> None:
        """
        unslothë¥¼ ì‚¬ìš©í•œ ê³ íš¨ìœ¨ ëª¨ë¸ ë¡œë”© (ë©”ëª¨ë¦¬ 75% ê°ì†Œ)
        
        Args:
            model_checkpoint: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            qlora_config: QLoRA ì„¤ì •
        """
        logger.info("ğŸš€ unslothë¡œ ê³ íš¨ìœ¨ ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        try:
            # unsloth FastLanguageModelë¡œ ëª¨ë¸ ë¡œë”©
            model, tokenizer = FastLanguageModel.from_pretrained(
                model_name=model_checkpoint,
                max_seq_length=self.config['tokenizer'].get('encoder_max_len', 512) + 
                              self.config['tokenizer'].get('decoder_max_len', 200),
                dtype=torch.float16 if self.config['training'].get('fp16') else torch.float32,
                load_in_4bit=qlora_config.get('load_in_4bit', True),
            )
            
            # LoRA ì„¤ì • ì¶”ê°€
            model = FastLanguageModel.get_peft_model(
                model,
                r=qlora_config.get('lora_rank', 16),
                target_modules=qlora_config.get('target_modules', [
                    "q_proj", "k_proj", "v_proj", "out_proj",
                    "fc1", "fc2"
                ]),
                lora_alpha=qlora_config.get('lora_alpha', 32),
                lora_dropout=qlora_config.get('lora_dropout', 0.1),
                bias="none",
                use_gradient_checkpointing="unsloth",  # unsloth ìµœì í™”
                random_state=42,
            )
            
            self.model = model
            logger.info("âœ… unsloth ëª¨ë¸ ë¡œë”© ì„±ê³µ! ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 75% ê°ì†Œ ì˜ˆìƒ")
            
        except Exception as e:
            logger.error(f"âŒ unsloth ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            logger.info("í´ë°± ëª¨ë“œ: ì¼ë°˜ QLoRAë¡œ ëŒ€ì²´")
            self._load_model_with_qlora(model_checkpoint, 'kobart', qlora_config)
    
    def _load_model_with_qlora(self, model_checkpoint: str, architecture: str, qlora_config: Dict[str, Any]) -> None:
        """
        ì¼ë°˜ QLoRAë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ ë¡œë”©
        
        Args:
            model_checkpoint: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            architecture: ëª¨ë¸ ì•„í‚¤í…ì²˜
            qlora_config: QLoRA ì„¤ì •
        """
        logger.info("ğŸ”‹ QLoRAë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        try:
            # 4-bit ì–‘ìí™” ì„¤ì •
            from transformers import BitsAndBytesConfig
            
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=qlora_config.get('load_in_4bit', True),
                bnb_4bit_compute_dtype=getattr(torch, qlora_config.get('bnb_4bit_compute_dtype', 'float16')),
                bnb_4bit_quant_type=qlora_config.get('bnb_4bit_quant_type', 'nf4'),
                bnb_4bit_use_double_quant=qlora_config.get('bnb_4bit_use_double_quant', True),
            )
            
            # ëª¨ë¸ ë¡œë”©
            if architecture in ['kobart', 'bart', 't5', 'mt5']:
                # csebuetnlp/mT5_multilingual_XLSum ëª¨ë¸ íŠ¹ìˆ˜ ì²˜ë¦¬
                if 'mT5_multilingual_XLSum' in model_checkpoint or 'csebuetnlp' in model_checkpoint:
                    model = AutoModelForSeq2SeqLM.from_pretrained(
                        model_checkpoint,
                        quantization_config=bnb_config,
                        device_map="auto",
                        torch_dtype=torch.float16 if self.config['training'].get('fp16') else torch.float32,
                        trust_remote_code=True  # DiaConfig í—ˆìš©
                    )
                else:
                    model = AutoModelForSeq2SeqLM.from_pretrained(
                        model_checkpoint,
                        quantization_config=bnb_config,
                        device_map="auto",
                        torch_dtype=torch.float16 if self.config['training'].get('fp16') else torch.float32
                    )
            else:
                model = AutoModelForCausalLM.from_pretrained(
                    model_checkpoint,
                    quantization_config=bnb_config,
                    device_map="auto",
                    torch_dtype=torch.float16 if self.config['training'].get('fp16') else torch.float32
                )
            
            # LoRA ì„¤ì •
            if LoraConfig is not None:
                lora_config = LoraConfig(
                    r=qlora_config.get('lora_rank', 16),
                    lora_alpha=qlora_config.get('lora_alpha', 32),
                    target_modules=qlora_config.get('target_modules', [
                        "q_proj", "k_proj", "v_proj", "out_proj",
                        "fc1", "fc2"
                    ]),
                    lora_dropout=qlora_config.get('lora_dropout', 0.1),
                    bias="none",
                    task_type=TaskType.SEQ_2_SEQ_LM if architecture in ['kobart', 'bart', 't5', 'mt5'] else TaskType.CAUSAL_LM,
                )
                
                model = get_peft_model(model, lora_config)
                logger.info("âœ… QLoRA ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")
            
            self.model = model
            
        except ImportError:
            logger.error("âŒ bitsandbytes ë˜ëŠ” peft ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
            logger.info("í´ë°± ëª¨ë“œ: í‘œì¤€ ëª¨ë¸ ë¡œë”©")
            self._load_model_standard(model_checkpoint, architecture)
        except Exception as e:
            logger.error(f"âŒ QLoRA ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            logger.info("í´ë°± ëª¨ë“œ: í‘œì¤€ ëª¨ë¸ ë¡œë”©")
            self._load_model_standard(model_checkpoint, architecture)
    
    def _load_model_standard(self, model_checkpoint: str, architecture: str) -> None:
        """
        í‘œì¤€ ëª¨ë¸ ë¡œë”© (ê¸°ì¡´ ë°©ì‹, DiaConfig ë¬¸ì œ í•´ê²° í¬í•¨)
        
        Args:
            model_checkpoint: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            architecture: ëª¨ë¸ ì•„í‚¤í…ì²˜
        """
        logger.info("ğŸ“š í‘œì¤€ ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # ëª¨ë¸ ì•„í‚¤í…ì²˜ì— ë”°ë¥¸ ë¡œë”©
        if architecture in ['kobart', 'bart', 't5', 'mt5']:
            # ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ ëª¨ë¸
            model_config = {
                'torch_dtype': torch.float16 if self.config['training'].get('fp16') else torch.float32
            }
            
            # csebuetnlp/mT5_multilingual_XLSum ëª¨ë¸ íŠ¹ìˆ˜ ì²˜ë¦¬ (DiaConfig ë¬¸ì œ í•´ê²°)
            if 'mT5_multilingual_XLSum' in model_checkpoint or 'csebuetnlp' in model_checkpoint:
                model_config.update({
                    'trust_remote_code': True,  # DiaConfig í—ˆìš©
                    'use_cache': False,  # gradient checkpointingê³¼ ì¶©ëŒ ë°©ì§€
                    'output_attentions': False,
                    'output_hidden_states': False
                })
                logger.info("ğŸ”§ mT5_multilingual_XLSum ëª¨ë¸ íŠ¹ìˆ˜ ì„¤ì • ì ìš© (DiaConfig ì§€ì›)")
            
            # ì¼ë°˜ mT5 ëª¨ë¸ íŠ¹ìˆ˜ ì„¤ì • (ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŠ¸ ì•ˆì •ì„±)
            elif 'mt5' in model_checkpoint.lower() or 'multilingual' in model_checkpoint.lower():
                model_config.update({
                    'use_cache': False,  # gradient checkpointingê³¼ ì¶©ëŒ ë°©ì§€
                    'output_attentions': False,
                    'output_hidden_states': False
                })
                logger.info("ğŸ”§ mT5 ëª¨ë¸ ì•ˆì •ì„± ì„¤ì • ì ìš©")
            
            self.model = AutoModelForSeq2SeqLM.from_pretrained(
                model_checkpoint,
                **model_config
            )
        elif architecture in ['kogpt2', 'gpt2', 'gpt-neo']:
            # ì¸ê³¼ ì–¸ì–´ ëª¨ë¸
            self.model = AutoModelForCausalLM.from_pretrained(
                model_checkpoint,
                torch_dtype=torch.float16 if self.config['training'].get('fp16') else torch.float32
            )
        else:
            raise ValueError(f"Unsupported architecture: {architecture}")
        
        logger.info("âœ… í‘œì¤€ ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def _get_model_specific_config(self, architecture: str, checkpoint: str) -> Dict[str, Any]:
        """ëª¨ë¸ë³„ íŠ¹ìˆ˜ ì„¤ì • ë°˜í™˜"""
        config = {}
        
        # T5 ê³„ì—´
        if architecture in ['t5', 'mt5', 'flan-t5']:
            config['prefix'] = "summarize: "  # T5ëŠ” task prefix í•„ìš”
            
        # GPT ê³„ì—´
        elif architecture in ['gpt2', 'kogpt2', 'gpt-neo']:
            config['max_length'] = self.config['tokenizer']['encoder_max_len'] + self.config['tokenizer']['decoder_max_len']
            config['pad_token_id'] = self.tokenizer.pad_token_id
            
        return config
    
    def _preprocess_for_model(self, examples: Dict[str, Any]) -> Dict[str, Any]:
        """ëª¨ë¸ë³„ ë°ì´í„° ì „ì²˜ë¦¬"""
        architecture = self.config['model']['architecture']
        
        if architecture in ['t5', 'mt5', 'flan-t5']:
            # T5ëŠ” prefix ì¶”ê°€
            examples['input'] = ["summarize: " + inp for inp in examples['input']]
            
        elif architecture in ['gpt2', 'kogpt2', 'gpt-neo']:
            # GPTëŠ” ì…ë ¥ê³¼ íƒ€ê²Ÿì„ ì—°ê²°
            examples['input'] = [
                f"{inp} TL;DR: {tgt}" 
                for inp, tgt in zip(examples['input'], examples['target'])
            ]
            
        return examples
    
    def _get_training_arguments(self) -> Seq2SeqTrainingArguments:
        """í•™ìŠµ ì¸ì ìƒì„±"""
        train_config = self.config['training']
        
        # ê¸°ë³¸ ì¸ì
        args_dict = {
            'output_dir': str(self.output_dir / 'checkpoints'),
            'overwrite_output_dir': True,
            'do_train': True,
            'do_eval': True,
            'eval_strategy': train_config.get('evaluation_strategy', 'steps'),
            'eval_steps': train_config.get('eval_steps', 500),
            'save_strategy': train_config.get('save_strategy', 'steps'),
            'save_steps': train_config.get('save_steps', 500),
            'save_total_limit': train_config.get('save_total_limit', 3),
            'per_device_train_batch_size': train_config['per_device_train_batch_size'],
            'per_device_eval_batch_size': train_config.get('per_device_eval_batch_size', 
                                                          train_config['per_device_train_batch_size']),
            'gradient_accumulation_steps': train_config.get('gradient_accumulation_steps', 1),
            'learning_rate': train_config['learning_rate'],
            'weight_decay': train_config.get('weight_decay', 0.01),
            'adam_beta1': train_config.get('adam_beta1', 0.9),
            'adam_beta2': train_config.get('adam_beta2', 0.999),
            'adam_epsilon': train_config.get('adam_epsilon', 1e-8),
            'max_grad_norm': train_config.get('max_grad_norm', 1.0),
            'num_train_epochs': 1 if os.environ.get('FORCE_ONE_EPOCH', '').lower() in ['1', 'true', 'yes'] else train_config['num_train_epochs'],
            'lr_scheduler_type': train_config.get('lr_scheduler_type', 'linear'),
            'warmup_ratio': train_config.get('warmup_ratio', 0.1),
            'warmup_steps': train_config.get('warmup_steps', 0),
            'logging_dir': str(self.output_dir / 'logs'),
            'logging_steps': train_config.get('logging_steps', 50),
            'load_best_model_at_end': True,
            'metric_for_best_model': 'eval_rouge_combined_f1',
            'greater_is_better': True,
            'fp16': train_config.get('fp16', False),
            'fp16_opt_level': train_config.get('fp16_opt_level', 'O1'),
            'dataloader_num_workers': train_config.get('dataloader_num_workers', 4),
            'remove_unused_columns': False,
            'label_smoothing_factor': train_config.get('label_smoothing', 0.0),
            'optim': train_config.get('optim', 'adamw_torch'),
            'seed': self.config['general'].get('seed', 42),
            'report_to': ['wandb'] if wandb.run else ['none'],
            'run_name': self.experiment_name if wandb.run else None,
            'push_to_hub': False,
            'predict_with_generate': True,
            'generation_max_length': train_config.get('generation_max_length', self.config['tokenizer']['decoder_max_len']),
            'generation_num_beams': train_config.get('generation_num_beams', 4)
        }
        
        # ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ íŠ¹í™” ì¸ì
        seq2seq_args = Seq2SeqTrainingArguments(**args_dict)
        
        return seq2seq_args
    
    def _save_results(self, result: TrainingResult) -> None:
        """ê²°ê³¼ ì €ì¥"""
        # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ìƒì„±
        results_dict = {
            'experiment_name': self.experiment_name,
            'model_architecture': self._get_model_architecture(),
            'model_checkpoint': self._get_model_checkpoint(),
            'best_metrics': result.best_metrics,
            'final_metrics': result.final_metrics,
            'model_path': result.model_path,
            'wandb_run_id': result.wandb_run_id,
            'experiment_id': result.experiment_id,
            'config': result.config_used,
            'timestamp': str(Path(result.model_path).parent.parent.name)
        }
        
        # JSON ì €ì¥
        results_file = self.results_dir / 'training_results.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results_dict, f, ensure_ascii=False, indent=2)
        
        # ìš”ì•½ í…ìŠ¤íŠ¸ ì €ì¥
        summary_file = self.results_dir / 'summary.txt'
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"Training Summary for {self.experiment_name}\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"Model: {self._get_model_architecture()} ({self._get_model_checkpoint()})\n")
            f.write(f"Training Epochs: {self.config['training']['num_train_epochs']}\n")
            f.write(f"Batch Size: {self.config['training']['per_device_train_batch_size']}\n")
            f.write(f"Learning Rate: {self.config['training']['learning_rate']}\n\n")
            f.write("Best Metrics:\n")
            for metric, value in result.best_metrics.items():
                f.write(f"  {metric}: {value:.4f}\n")
            f.write("\nModel saved to: " + result.model_path + "\n")
            if result.wandb_run_id:
                f.write(f"WandB Run ID: {result.wandb_run_id}\n")
            if result.submission_file:
                f.write(f"\nSubmission File: {result.submission_file}\n")
        
        logger.info(f"Results saved to {self.results_dir}")



    def generate_test_predictions(self, test_dataset: Dataset) -> pd.DataFrame:
        """
        í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ ìƒì„± (baseline.ipynbì™€ ì™„ì „íˆ ë™ì¼, ëª¨ë“  ëª¨ë¸ ì§€ì›)
        
        Args:
            test_dataset: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
            
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ DataFrame (fname, summary ì»¬ëŸ¼)
        """
        self.model.eval()
        summary = []
        fname_list = []
        
        # baseline.ipynbì˜ inference config ì‚¬ìš©
        inference_config = self.config.get('inference', {})
        if not inference_config:
            # ê¸°ë³¸ê°’ ì„¤ì • (baseline.ipynb configì™€ ë™ì¼)
            inference_config = {
                'batch_size': 32,
                'no_repeat_ngram_size': 2,
                'early_stopping': True,
                'generate_max_length': 100,
                'num_beams': 4,
                'remove_tokens': self._get_default_remove_tokens()
            }
        
        # DataLoader ìƒì„± (baselineì²˜ëŸ¼)
        batch_size = inference_config.get('batch_size', 32)
        dataloader = DataLoader(test_dataset, batch_size=batch_size)
        
        with torch.no_grad():
            for item in tqdm(dataloader, desc="Generating predictions"):
                # fname ë°ì´í„° ìˆ˜ì§‘ (baselineê³¼ ë™ì¼í•œ ë°©ì‹)
                if 'fname' in item:
                    fname_list.extend(item['fname'])
                elif 'ID' in item:
                    fname_list.extend(item['ID'])
                else:
                    # ê¸°ë³¸ê°’ìœ¼ë¡œ ë°°ì¹˜ ì¸ë±ìŠ¤ ì‚¬ìš©
                    batch_size_actual = len(item['input_ids'])
                    fname_list.extend([f"test_{len(fname_list) + i:04d}" for i in range(batch_size_actual)])
                
                # baseline.ipynbì™€ ë™ì¼í•œ ìƒì„±
                generated_ids = self.model.generate(
                    input_ids=item['input_ids'].to(self.device),
                    no_repeat_ngram_size=inference_config['no_repeat_ngram_size'],
                    early_stopping=inference_config['early_stopping'],
                    max_length=inference_config['generate_max_length'],
                    num_beams=inference_config['num_beams'],
                )
                
                # ê° IDë³„ë¡œ ë””ì½”ë”© (baselineê³¼ ë™ì¼, skip_special_tokens=False)
                for ids in generated_ids:
                    result = self.tokenizer.decode(ids, skip_special_tokens=False)
                    summary.append(result)
        
        # ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•˜ì—¬ ë…¸ì´ì¦ˆì— í•´ë‹¹ë˜ëŠ” ìŠ¤í˜ì…œ í† í°ì„ ì œê±°í•©ë‹ˆë‹¤.
        remove_tokens = inference_config.get('remove_tokens', self._get_default_remove_tokens())
        preprocessed_summary = summary.copy()
        for token in remove_tokens:
            if token is not None:  # None í† í° ë°©ì§€
                preprocessed_summary = [sentence.replace(str(token), " ") for sentence in preprocessed_summary]
        
        # DataFrame ìƒì„± (baselineê³¼ ë™ì¼)
        output = pd.DataFrame({
            "fname": fname_list[:len(preprocessed_summary)],
            "summary": preprocessed_summary,
        })
        
        return output
    
    def _get_default_remove_tokens(self) -> List[str]:
        """
        ëª¨ë¸ë³„ ê¸°ë³¸ ì œê±° í† í° ëª©ë¡ ë°˜í™˜
        
        Returns:
            ì œê±°í•  í† í° ëª©ë¡
        """
        tokens = ['<usr>']
        
        # í† í¬ë‚˜ì´ì € íŠ¹ìˆ˜ í† í°ë“¤ ì¶”ê°€ (None ì²´í¬)
        if hasattr(self.tokenizer, 'bos_token') and self.tokenizer.bos_token is not None:
            tokens.append(self.tokenizer.bos_token)
        if hasattr(self.tokenizer, 'eos_token') and self.tokenizer.eos_token is not None:
            tokens.append(self.tokenizer.eos_token)
        if hasattr(self.tokenizer, 'pad_token') and self.tokenizer.pad_token is not None:
            tokens.append(self.tokenizer.pad_token)
        
        # ëª¨ë¸ë³„ ì¶”ê°€ í† í°
        model_arch = self._get_model_architecture().lower()
        
        # T5/mT5 ê³„ì—´
        if 't5' in model_arch:
            if hasattr(self.tokenizer, 'additional_special_tokens'):
                tokens.extend([token for token in self.tokenizer.additional_special_tokens if token is not None])
        
        # BART ê³„ì—´
        elif 'bart' in model_arch:
            # BART íŠ¹ìˆ˜ í† í°ë“¤
            bart_tokens = ['<s>', '</s>', '<pad>', '<unk>']
            tokens.extend(bart_tokens)
        
        # ì¤‘ë³µ ì œê±° ë° None í•„í„°ë§
        return list(set([token for token in tokens if token is not None]))
    
    def _save_submission_csv(self, predictions_df: pd.DataFrame) -> Path:
        """
        ì˜ˆì¸¡ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥ (baseline.ipynb í˜•ì‹)
        
        Args:
            predictions_df: ì˜ˆì¸¡ ê²°ê³¼ DataFrame
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        from datetime import datetime
        
        # baseline.ipynbì²˜ëŸ¼ prediction í´ë”ì— ì €ì¥
        result_path = self.output_dir / "prediction"
        if not result_path.exists():
            result_path.mkdir(parents=True)
        
        # íŒŒì¼ëª… ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_name = self._get_model_architecture().replace('/', '_')
        
        # baseline.ipynbì—ì„œ output.csv ì‚¬ìš©í•˜ì§€ë§Œ ë‹¤ì¤‘ ëª¨ë¸ êµ¬ë¶„ì„ ìœ„í•´ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        filename = f"output_{model_name}_{timestamp}.csv"
        
        # ì €ì¥ ê²½ë¡œ
        submission_path = result_path / filename
        
        # CSV ì €ì¥ (baselineê³¼ ë™ì¼í•˜ê²Œ index=False)
        predictions_df.to_csv(submission_path, index=False)
        
        # ë³µì‚¬ë³¸ì„ ìµœìƒìœ„ ë””ë ‰í„°ë¦¬ì—ë„ ì €ì¥
        latest_submission_path = self.output_dir.parent / f"submission_latest_{model_name}.csv"
        predictions_df.to_csv(latest_submission_path, index=False)
        
        # í†µê³„ ì¶œë ¥
        logger.info(f"\n=== Submission Statistics ===") 
        logger.info(f"Total samples: {len(predictions_df)}")
        logger.info(f"Average summary length: {predictions_df['summary'].str.len().mean():.1f}")
        logger.info(f"Min summary length: {predictions_df['summary'].str.len().min()}")
        logger.info(f"Max summary length: {predictions_df['summary'].str.len().max()}")
        
        # ì²« 3ê°œ ì˜ˆì¸¡ ìƒ˜í”Œ ì¶œë ¥
        logger.info(f"\nì²« 3ê°œ ì˜ˆì¸¡ ìƒ˜í”Œ:")
        for i in range(min(3, len(predictions_df))):
            logger.info(f"[{predictions_df.iloc[i]['fname']}] {predictions_df.iloc[i]['summary'][:80]}...")
        
        return submission_path


def create_trainer(config: Union[str, Dict[str, Any]], 
                  sweep_mode: bool = False) -> DialogueSummarizationTrainer:
    """
    íŠ¸ë ˆì´ë„ˆ ìƒì„± í¸ì˜ í•¨ìˆ˜
    
    Args:
        config: ì„¤ì • íŒŒì¼ ê²½ë¡œ ë˜ëŠ” ì„¤ì • ë”•ì…”ë„ˆë¦¬
        sweep_mode: WandB Sweep ëª¨ë“œ ì—¬ë¶€
        
    Returns:
        ì´ˆê¸°í™”ëœ íŠ¸ë ˆì´ë„ˆ ì¸ìŠ¤í„´ìŠ¤
    """
    # ì„¤ì • ë¡œë”©
    if isinstance(config, str):
        config_dict = load_config(config)
    else:
        config_dict = config
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = DialogueSummarizationTrainer(
        config=config_dict,
        sweep_mode=sweep_mode
    )
    
    # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    trainer.initialize_components()
    
    return trainer


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸/ë””ë²„ê¹…ìš© ë©”ì¸ í•¨ìˆ˜
    import argparse
    
    parser = argparse.ArgumentParser(description="Train dialogue summarization model")
    parser.add_argument("--config", type=str, required=True, help="Config file path")
    parser.add_argument("--train-data", type=str, help="Train data path")
    parser.add_argument("--val-data", type=str, help="Validation data path")
    parser.add_argument("--test-data", type=str, help="Test data path")
    parser.add_argument("--sweep", action="store_true", help="Run in sweep mode")
    
    args = parser.parse_args()
    
    # 1ì—í¬í¬ ëª¨ë“œ í™•ì¸ ë° ë©”ì‹œì§€ ì¶œë ¥
    if os.environ.get('FORCE_ONE_EPOCH', '').lower() in ['1', 'true', 'yes']:
        print("ğŸš€ 1ì—í¬í¬ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤!")
        print("ğŸ“ í•™ìŠµ epoch ìˆ˜ê°€ 1ë¡œ ê°•ì œ ì„¤ì •ë©ë‹ˆë‹¤.")
    
    # WandB ì´ˆê¸°í™” (ë¹„ Sweep ëª¨ë“œ) - í•œêµ­ ì‹œê°„ ê¸°ë°˜ ê°œì„ 
    if not args.sweep:
        # í™˜ê²½ë³€ìˆ˜ë¡œ ë¹„í™œì„±í™” í™•ì¸
        if os.environ.get("WANDB_MODE") == "disabled":
            print("âš ï¸ WandBê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        else:
            # .envì—ì„œ ë¡œë“œëœ WandB ì„¤ì • í™•ì¸
            wandb_entity = os.getenv('WANDB_ENTITY', 'lyjune37-juneictlab')
            wandb_project = os.getenv('WANDB_PROJECT', 'nlp-5')
            
            # í•œêµ­ ì‹œê°„ ê¸°ë°˜ run name ìƒì„±
            try:
                from utils.experiment_utils import get_wandb_run_name_with_korean_time
                run_name = get_wandb_run_name_with_korean_time(
                    model_name="manual_training", 
                    prefix="auto"
                )
            except ImportError:
                run_name = "manual_training"
            
            wandb.init(
                entity=wandb_entity,
                project=wandb_project,
                name=run_name,
                config={"manual_run": True, "korean_time_enabled": True},
                tags=["rtx3090_optimized", "korean_time"]
            )
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í•™ìŠµ
    trainer = create_trainer(args.config, sweep_mode=args.sweep)
    
    # ë°ì´í„° ì¤€ë¹„
    datasets = trainer.prepare_data(
        train_path=args.train_data,
        val_path=args.val_data,
        test_path=args.test_data
    )
    
    # í•™ìŠµ ì‹¤í–‰
    result = trainer.train(datasets)
    
    print(f"Training completed! Best ROUGE combined F1: {result.best_metrics.get('rouge_combined_f1', 0):.4f}")
