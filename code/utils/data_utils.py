"""
ë°ì´í„° ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°

NLP ëŒ€í™” ìš”ì•½ í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬, í›„ì²˜ë¦¬, ë³€í™˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
ê¸°ì¡´ baseline.ipynbì˜ ë°ì´í„° ì²˜ë¦¬ ë¡œì§ì„ ëª¨ë“ˆí™”í•˜ê³  í™•ì¥í–ˆìŠµë‹ˆë‹¤.
"""

import re
import json
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional, Union, Tuple, Callable
from pathlib import Path
import logging
from dataclasses import dataclass
from transformers import PreTrainedTokenizer
import torch
from torch.utils.data import Dataset
from datasets import Dataset as HFDataset
from .path_utils import PathManager, path_manager


logger = logging.getLogger(__name__)


@dataclass
class DataSample:
    """ë°ì´í„° ìƒ˜í”Œ í´ë˜ìŠ¤"""
    dialogue: str
    summary: str
    fname: str
    dialogue_length: int = 0
    summary_length: int = 0
    
    def __post_init__(self) -> None:
        self.dialogue_length = len(self.dialogue)
        self.summary_length = len(self.summary)


class TextPreprocessor:
    """
    í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ê¸°
    
    í•œêµ­ì–´ ëŒ€í™” í…ìŠ¤íŠ¸ì˜ ì •ê·œí™”, ì •ì œ, íŠ¹ìˆ˜ í† í° ì²˜ë¦¬ ë“±ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        TextPreprocessor ì´ˆê¸°í™”
        
        Args:
            config: ì „ì²˜ë¦¬ ì„¤ì •
        """
        self.config = config or {}
        self.logger = logging.getLogger(__name__)
        
        # íŠ¹ìˆ˜ í† í° ì„¤ì •
        self.special_tokens = [
            '#Person1#', '#Person2#', '#Person3#', '#Person4#', 
            '#Person5#', '#Person6#', '#Person7#',
            '#PhoneNumber#', '#Address#', '#PassportNumber#'
        ]
        
        # ì •ê·œ í‘œí˜„ì‹ íŒ¨í„´
        self._compile_patterns()
    
    def _compile_patterns(self) -> None:
        """ì •ê·œ í‘œí˜„ì‹ íŒ¨í„´ ì»´íŒŒì¼"""
        # ê°œí–‰ ë¬¸ì ë³€í˜• íŒ¨í„´
        self.newline_pattern = re.compile(r'\\n')
        
        # HTML íƒœê·¸ íŒ¨í„´
        self.html_pattern = re.compile(r'<[^>]+>')
        
        # ì—°ì† ê³µë°± íŒ¨í„´
        self.whitespace_pattern = re.compile(r'\s+')
        
        # íŠ¹ìˆ˜ ë¬¸ì ì •ê·œí™” íŒ¨í„´
        self.quote_pattern = re.compile(r'["""]')
        self.dash_pattern = re.compile(r'[â€•â€”â€“-]')
        
        # í™”ì êµ¬ë¶„ íŒ¨í„´
        self.speaker_pattern = re.compile(r'#Person(\d+)#\s*:\s*')
    
    def preprocess_text(self, text: str, 
                       normalize_quotes: bool = True,
                       normalize_whitespace: bool = True,
                       remove_html: bool = True) -> str:
        """
        í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            normalize_quotes: ë”°ì˜´í‘œ ì •ê·œí™” ì—¬ë¶€
            normalize_whitespace: ê³µë°± ì •ê·œí™” ì—¬ë¶€
            remove_html: HTML íƒœê·¸ ì œê±° ì—¬ë¶€
            
        Returns:
            ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        if not text or not isinstance(text, str):
            return ""
        
        # ê°œí–‰ ë¬¸ì ë³€í˜• ì²˜ë¦¬
        text = self.newline_pattern.sub('\n', text)
        
        # HTML íƒœê·¸ ì œê±°
        if remove_html:
            text = self.html_pattern.sub('', text)
        
        # ë”°ì˜´í‘œ ì •ê·œí™”
        if normalize_quotes:
            text = self.quote_pattern.sub('"', text)
        
        # ëŒ€ì‹œ ì •ê·œí™”
        text = self.dash_pattern.sub('-', text)
        
        # ê³µë°± ì •ê·œí™”
        if normalize_whitespace:
            text = self.whitespace_pattern.sub(' ', text)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text
    
    def extract_speakers(self, dialogue: str) -> List[str]:
        """
        ëŒ€í™”ì—ì„œ í™”ì ëª©ë¡ ì¶”ì¶œ
        
        Args:
            dialogue: ëŒ€í™” í…ìŠ¤íŠ¸
            
        Returns:
            í™”ì ëª©ë¡
        """
        speakers = self.speaker_pattern.findall(dialogue)
        return [f"#Person{speaker}#" for speaker in sorted(set(speakers))]
    
    def count_turns(self, dialogue: str) -> int:
        """
        ëŒ€í™” í„´ ìˆ˜ ê³„ì‚°
        
        Args:
            dialogue: ëŒ€í™” í…ìŠ¤íŠ¸
            
        Returns:
            í„´ ìˆ˜
        """
        return len(self.speaker_pattern.findall(dialogue))
    
    def clean_dialogue(self, dialogue: str) -> str:
        """
        ëŒ€í™” í…ìŠ¤íŠ¸ ì •ì œ
        
        Args:
            dialogue: ì›ë³¸ ëŒ€í™” í…ìŠ¤íŠ¸
            
        Returns:
            ì •ì œëœ ëŒ€í™” í…ìŠ¤íŠ¸
        """
        # ê¸°ë³¸ ì „ì²˜ë¦¬
        dialogue = self.preprocess_text(dialogue)
        
        # í™”ì êµ¬ë¶„ í˜•ì‹ í‘œì¤€í™”
        dialogue = self.speaker_pattern.sub(r'#Person\1#: ', dialogue)
        
        return dialogue
    
    def clean_summary(self, summary: str) -> str:
        """
        ìš”ì•½ë¬¸ ì •ì œ
        
        Args:
            summary: ì›ë³¸ ìš”ì•½ë¬¸
            
        Returns:
            ì •ì œëœ ìš”ì•½ë¬¸
        """
        # ê¸°ë³¸ ì „ì²˜ë¦¬
        summary = self.preprocess_text(summary)
        
        # ìš”ì•½ë¬¸ì—ëŠ” í™”ì êµ¬ë¶„ ë¶ˆí•„ìš”í•˜ë¯€ë¡œ ì œê±°
        summary = self.speaker_pattern.sub('', summary)
        
        return summary


class DataProcessor:
    """
    ë°ì´í„° í”„ë¡œì„¸ì„œ
    
    CSV/JSON íŒŒì¼ ë¡œë”©, ë°ì´í„° í•„í„°ë§, í† í¬ë‚˜ì´ì§•, HuggingFace Dataset ë³€í™˜ ë“±ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, 
                 tokenizer: PreTrainedTokenizer, 
                 config: Optional[Dict[str, Any]] = None,
                 preprocessor: Optional[Callable] = None,
                 model: Optional[Any] = None):  # ëª¨ë¸ ì¶”ê°€
        """
        DataProcessor ì´ˆê¸°í™”
        
        Args:
            tokenizer: ì‚¬ì „ í•™ìŠµëœ í† í¬ë‚˜ì´ì €
            config: ë°ì´í„° ì²˜ë¦¬ ì„¤ì •
            preprocessor: ëª¨ë¸ë³„ ì „ì²˜ë¦¬ í•¨ìˆ˜ (optional)
            model: ëª¨ë¸ ê°ì²´ (embedding í¬ê¸° ì¡°ì •ì„ ìœ„í•´)
        """
        self.tokenizer = tokenizer
        self.config = config or {}
        self.logger = logging.getLogger(__name__)
        self.text_preprocessor = TextPreprocessor(config)
        self.model_preprocessor = preprocessor  # ëª¨ë¸ë³„ ì „ì²˜ë¦¬ í•¨ìˆ˜
        self.model = model  # ëª¨ë¸ ê°ì²´ ì €ì¥
        
        # í† í¬ë‚˜ì´ì € ì„¤ì •
        self.encoder_max_len = self.config.get('tokenizer', {}).get('encoder_max_len', 512)
        self.decoder_max_len = self.config.get('tokenizer', {}).get('decoder_max_len', 128)
        # ë°ì´í„° í•„í„° ì„¤ì •
        self.min_dialogue_length = self.config.get('data', {}).get('min_source_length', 10)
        self.max_dialogue_length = self.config.get('data', {}).get('max_source_length', 1024)
        self.min_summary_length = self.config.get('data', {}).get('min_target_length', 5)
        self.max_summary_length = self.config.get('data', {}).get('max_target_length', 256)
        
        # íŠ¹ìˆ˜ í† í° ì¶”ê°€
        self._add_special_tokens()
        
        def _add_special_tokens(self):
        """íŠ¹ìˆ˜ í† í°ì„ í† í¬ë‚˜ì´ì €ì— ì¶”ê°€"""
        special_tokens = self.text_preprocessor.special_tokens
        
        # ëª¨ë¸ ì´ë¦„ í™•ì¸
        model_name = self.config.get('general', {}).get('model_name', '')
        
        # KoBART ëª¨ë¸ì˜ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
        if "kobart" in model_name.lower() or "bart" in model_name.lower():
            logger.info("ğŸ” KoBART/BART ëª¨ë¸ ê°ì§€: íŠ¹ìˆ˜ í† í° ì²˜ë¦¬ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.")
            
            # ëª¨ë¸ì´ ìˆìœ¼ë©´ ì•ˆì „í•œ í† í° ì¶”ê°€ ì‚¬ìš©
            if self.model is not None:
                try:
                    from utils.model_utils import safe_add_special_tokens
                    self.tokenizer, self.model = safe_add_special_tokens(
                        self.tokenizer, self.model, special_tokens, model_name
                    )
                    return
                except ImportError:
                    logger.warning("âš ï¸ model_utilsë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            else:
                # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ìµœì†Œí•œì˜ í† í°ë§Œ ì¶”ê°€
                logger.warning("ëª¨ë¸ ê°ì²´ê°€ ì—†ì–´ ì•ˆì „ ëª¨ë“œë¡œ íŠ¹ìˆ˜ í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤.")
                # ê¸°ë³¸ PII í† í°ë§Œ ì¶”ê°€
                safe_tokens = ['#PhoneNumber#', '#Address#', '#PassportNumber#']
                new_tokens = [token for token in safe_tokens if token not in self.tokenizer.get_vocab()]
                if new_tokens:
                    self.tokenizer.add_tokens(new_tokens)
                    logger.info(f"ğŸ”’ {len(new_tokens)}ê°œì˜ ê¸°ë³¸ PII í† í°ë§Œ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
                return
        
        # eenzeenee ëª¨ë¸ì˜ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
        if "eenzeenee" in model_name.lower():
            try:
                from utils.eenzeenee_utils import check_and_fix_special_tokens
                self.tokenizer = check_and_fix_special_tokens(
                    self.tokenizer, special_tokens, model_name
                )
                return
            except ImportError:
                logger.warning("âš ï¸ eenzeenee_utilsë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # ê¸°ì¡´ì— ì—†ëŠ” í† í°ë§Œ ì¶”ê°€
        new_tokens = [token for token in special_tokens 
                     if token not in self.tokenizer.get_vocab()]
        
        if new_tokens:
            self.tokenizer.add_tokens(new_tokens)
            logger.info(f"Added {len(new_tokens)} special tokens to tokenizer")
            logger.info(f"Added {len(new_tokens)} special tokens to tokenizer")
    
    def load_data(self, file_path: Union[str, Path], is_test: bool = False) -> pd.DataFrame:
        """
        ë°ì´í„° íŒŒì¼ ë¡œë”© (CSV ë˜ëŠ” JSON ì§€ì›)
        # í† í¬ë‚˜ì´ì§•
        dataset = dataset.map(
            self._tokenize_function,
            batched=True,
            remove_columns=['input', 'target', 'fname']  # fnameë„ ì œê±°í•˜ì—¬ DataCollator ì—ëŸ¬ ë°©ì§€
        )
        Returns:
            ë¡œë”©ëœ ë°ì´í„°í”„ë ˆì„
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"Data file not found: {file_path}")
        
        try:
            if file_path.suffix == '.csv':
                df = pd.read_csv(file_path)
            elif file_path.suffix == '.json':
                df = pd.read_json(file_path)
            else:
                raise ValueError(f"Unsupported file format: {file_path.suffix}")
            
            logger.info(f"Loaded {len(df)} samples from {file_path}")
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            if is_test:
                required_columns = ['fname', 'dialogue']
            else:
                required_columns = ['fname', 'dialogue', 'summary']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                raise ValueError(f"Missing required columns: {missing_columns}")
            
            return df
            
        except Exception as e:
            logger.error(f"Failed to load data: {e}")
            raise
    
    def process_data(self, df: pd.DataFrame, is_training: bool = True, is_test: bool = False) -> HFDataset:
        """
        ë°ì´í„°í”„ë ˆì„ì„ HuggingFace Datasetìœ¼ë¡œ ë³€í™˜
        
        Args:
            df: ì›ë³¸ ë°ì´í„°í”„ë ˆì„
            is_training: í•™ìŠµ ë°ì´í„° ì—¬ë¶€
            is_test: í…ŒìŠ¤íŠ¸ ë°ì´í„° ì—¬ë¶€ (summary ì—†ìŒ)
            
        Returns:
            HuggingFace Dataset ê°ì²´
        """
        # í…ìŠ¤íŠ¸ ì •ì œ
        df = df.copy()
        df['dialogue'] = df['dialogue'].apply(self.text_preprocessor.clean_dialogue)
        
        # test ë°ì´í„°ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ summary ì²˜ë¦¬
        if not is_test:
            df['summary'] = df['summary'].apply(self.text_preprocessor.clean_summary)
        
        # ê¸¸ì´ í•„í„°ë§ (í•™ìŠµ ë°ì´í„°ë§Œ)
        if is_training:
            df = self._filter_by_length(df)
        
        # ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ìƒì„±
        data_dict = {
            'input': df['dialogue'].tolist(),
            'fname': df['fname'].tolist()
        }
        # test ë°ì´í„°ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ target ì¶”ê°€
        if not is_test:
            data_dict['target'] = df['summary'].tolist()
        else:
            # test ë°ì´í„°ì˜ ê²½ìš° ë¹ˆ target ìƒì„±
            data_dict['target'] = [''] * len(df)
        
        # ëª¨ë¸ë³„ ì „ì²˜ë¦¬ ì ìš©
        if self.model_preprocessor:
            data_dict = self.model_preprocessor(data_dict)
        
        # HuggingFace Dataset ìƒì„±
        dataset = HFDataset.from_dict(data_dict)
        
        # í† í¬ë‚˜ì´ì§•
        dataset = dataset.map(
            self._tokenize_function,
            batched=True,
            remove_columns=['input', 'target', 'fname']  # fnameë„ ì œê±°í•˜ì—¬ DataCollator ì—ëŸ¬ ë°©ì§€
        )
        
        return dataset
    
    def _filter_by_length(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ê¸¸ì´ ê¸°ë°˜ ë°ì´í„° í•„í„°ë§
        
        Args:
            df: ë°ì´í„°í”„ë ˆì„
            
        Returns:
            í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„
        """
        initial_count = len(df)
        
        # ëŒ€í™” ê¸¸ì´ í•„í„°ë§
        df['dialogue_length'] = df['dialogue'].str.len()
        df = df[
            (df['dialogue_length'] >= self.min_dialogue_length) &
            (df['dialogue_length'] <= self.max_dialogue_length)
        ]
        
        # ìš”ì•½ë¬¸ ê¸¸ì´ í•„í„°ë§
        df['summary_length'] = df['summary'].str.len()
        df = df[
            (df['summary_length'] >= self.min_summary_length) &
            (df['summary_length'] <= self.max_summary_length)
        ]
        
        final_count = len(df)
        if initial_count > final_count:
            logger.info(f"Filtered {initial_count - final_count} samples by length")
        
        return df
    
    def _preprocess_for_model(self, text: str, model_type: str = None) -> str:
        """
        ì†¡ê·œí—Œë‹˜ ìš”ì²­ì‚¬í•­: ëª¨ë¸ë³„ ì „ì²˜ë¦¬
        
        ê° ëª¨ë¸ ì•„í‚¤í…ì²˜ì— ë§ëŠ” ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.
        trainer.pyì˜ ë™ì¼ í•¨ìˆ˜ì™€ ì¼ê´€ëœ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            model_type: ëª¨ë¸ íƒ€ì… ('t5', 'gpt', 'bart', 'default'). Noneì¸ ê²½ìš° ìë™ ì¶”ë¡ 
            
        Returns:
            ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        if model_type is None:
            # í† í¬ë‚˜ì´ì €ë¥¼ í†µí•´ ëª¨ë¸ íƒ€ì… ì¶”ë¡ 
            model_name = getattr(self.tokenizer, 'name_or_path', '').lower()
            if any(keyword in model_name for keyword in ['t5', 'flan-t5', 'mt5']):
                model_type = 't5'
            elif any(keyword in model_name for keyword in ['gpt', 'kogpt']):
                model_type = 'gpt'
            elif 'bart' in model_name:
                model_type = 'bart'
            else:
                model_type = 'default'
        
        # ì…ë ¥ í…ìŠ¤íŠ¸ ê²€ì¦
        if not text or not isinstance(text, str):
            return str(text) if text else ""
        
        # ëª¨ë¸ë³„ ì „ì²˜ë¦¬
        text = text.strip()
        
        if model_type == 't5':
            # T5 ëª¨ë¸ë“¤ì— ëŒ€í•œ prefix ì²˜ë¦¬
            if not text.startswith('summarize:'):
                text = f'summarize: {text}'
        
        elif model_type == 'gpt':
            # GPT ëª¨ë¸ë“¤ì— ëŒ€í•œ TL;DR ì²˜ë¦¬
            if not text.endswith(' TL;DR:') and not text.endswith('TL;DR:'):
                text = f'{text} TL;DR:'
        
        # BART ë° ê¸°íƒ€ ëª¨ë¸ì€ ë³€ê²½ì‚¬í•­ ì—†ìŒ
        
        return text
    
    def _tokenize_function(self, examples: Dict[str, List]) -> Dict[str, List]:
        """
        ë°°ì¹˜ í† í¬ë‚˜ì´ì§• í•¨ìˆ˜
        
        Args:
            examples: ë°°ì¹˜ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            
        Returns:
            í† í¬ë‚˜ì´ì§•ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        # ëª¨ë¸ ì´ë¦„ í™•ì¸
        model_name = self.config.get('general', {}).get('model_name', '')
        
        # eenzeenee ëª¨ë¸ì˜ ê²½ìš° ê¸¸ì´ ì œí•œ
        if "eenzeenee" in model_name.lower():
            try:
                from utils.eenzeenee_utils import get_safe_max_length
                safe_lengths = get_safe_max_length(model_name, self.config)
                encoder_max_len = safe_lengths['encoder_max_len']
                decoder_max_len = safe_lengths['decoder_max_len']
            except ImportError:
                encoder_max_len = min(self.encoder_max_len, 256)
                decoder_max_len = min(self.decoder_max_len, 64)
        else:
            encoder_max_len = self.encoder_max_len
            decoder_max_len = self.decoder_max_len
        
        # ì…ë ¥ í† í¬ë‚˜ì´ì§•
        model_inputs = self.tokenizer(
            examples['input'],
            max_length=encoder_max_len,
            padding='max_length',
            truncation=True
        )
        
        # íƒ€ê²Ÿ í† í¬ë‚˜ì´ì§•
        with self.tokenizer.as_target_tokenizer():
            labels = self.tokenizer(
                examples['target'],
                max_length=decoder_max_len,
                padding='max_length',
                truncation=True
            )
        
        # íŒ¨ë”© í† í°ì„ -100ìœ¼ë¡œ ë³€ê²½ (loss ê³„ì‚°ì—ì„œ ë¬´ì‹œ)
        labels['input_ids'] = [
            [(label if label != self.tokenizer.pad_token_id else -100) for label in label_ids]
            for label_ids in labels['input_ids']
        ]
        
        model_inputs['labels'] = labels['input_ids']
        # model_inputs['fname'] = examples['fname']  # DataCollator ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ ì œê±°
        
        return model_inputs
    
    def create_data_samples(self, df: pd.DataFrame) -> List[DataSample]:
        """
        ë°ì´í„°í”„ë ˆì„ì„ DataSample ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        
        Args:
            df: ë°ì´í„°í”„ë ˆì„
            
        Returns:
            DataSample ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        samples = []
        
        for _, row in df.iterrows():
            sample = DataSample(
                dialogue=row['dialogue'],
                summary=row['summary'],
                fname=row['fname'],
                dialogue_length=row.get('dialogue_length', len(row['dialogue'])),
                summary_length=row.get('summary_length', len(row['summary']))
            )
            samples.append(sample)
        
        return samples


class DialogueSummarizationDataset(Dataset):
    """
    ëŒ€í™” ìš”ì•½ ë°ì´í„°ì…‹ í´ë˜ìŠ¤
    
    PyTorch Datasetì„ ìƒì†í•˜ì—¬ ë°°ì¹˜ ë‹¨ìœ„ ë°ì´í„° ë¡œë”©ì„ ì§€ì›í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, data_samples: List[DataSample], 
                 tokenizer: PreTrainedTokenizer,
                 max_source_length: int = 512,
                 max_target_length: int = 128,
                 prefix: str = ""):
        """
        DialogueSummarizationDataset ì´ˆê¸°í™”
        
        Args:
            data_samples: DataSample ê°ì²´ ë¦¬ìŠ¤íŠ¸
            tokenizer: í† í¬ë‚˜ì´ì €
            max_source_length: ìµœëŒ€ ì…ë ¥ ê¸¸ì´
            max_target_length: ìµœëŒ€ ì¶œë ¥ ê¸¸ì´
            prefix: ì…ë ¥ í”„ë¦¬í”½ìŠ¤ (T5 ë“±ì—ì„œ ì‚¬ìš©)
        """
        self.data_samples = data_samples
        self.tokenizer = tokenizer
        self.max_source_length = max_source_length
        self.max_target_length = max_target_length
        self.prefix = prefix
        
        # íŠ¹ìˆ˜ í† í° ì¶”ê°€
        self._add_special_tokens()
    
    def _add_special_tokens(self):
        """íŠ¹ìˆ˜ í† í°ì„ í† í¬ë‚˜ì´ì €ì— ì¶”ê°€"""
        special_tokens = [
            '#Person1#', '#Person2#', '#Person3#', '#Person4#',
            '#Person5#', '#Person6#', '#Person7#',
            '#PhoneNumber#', '#Address#', '#PassportNumber#'
        ]
        
        # ê¸°ì¡´ì— ì—†ëŠ” í† í°ë§Œ ì¶”ê°€
        new_tokens = [token for token in special_tokens 
                     if token not in self.tokenizer.get_vocab()]
        
        if new_tokens:
            self.tokenizer.add_tokens(new_tokens)
    
    def __len__(self) -> int:
        """ë°ì´í„°ì…‹ í¬ê¸° ë°˜í™˜"""
        return len(self.data_samples)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """
        ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° ìƒ˜í”Œ ë°˜í™˜
        
        Args:
            idx: ë°ì´í„° ì¸ë±ìŠ¤
            
        Returns:
            í† í¬ë‚˜ì´ì§•ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        sample = self.data_samples[idx]
        
        # ì…ë ¥ í…ìŠ¤íŠ¸ ì¤€ë¹„
        source_text = self.prefix + sample.dialogue
        target_text = sample.summary
        
        # í† í¬ë‚˜ì´ì§•
        source_encoding = self.tokenizer(
            source_text,
            max_length=self.max_source_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        target_encoding = self.tokenizer(
            target_text,
            max_length=self.max_target_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # ë ˆì´ë¸” ì¤€ë¹„ (íŒ¨ë”© í† í°ì€ -100ìœ¼ë¡œ)
        labels = target_encoding['input_ids'].clone()
        labels[labels == self.tokenizer.pad_token_id] = -100
        
        return {
            'input_ids': source_encoding['input_ids'].squeeze(),
            'attention_mask': source_encoding['attention_mask'].squeeze(),
            'labels': labels.squeeze()
            # fname ì œê±°: DataCollator í˜¸í™˜ì„±ì„ ìœ„í•´
        }


def collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    ë°°ì¹˜ ë°ì´í„° ì •ë¦¬ í•¨ìˆ˜
    
    Args:
        batch: ë°°ì¹˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ì •ë¦¬ëœ ë°°ì¹˜ ë”•ì…”ë„ˆë¦¬
    """
    # í…ì„œ ë°ì´í„°ë“¤ ìŠ¤íƒ
    input_ids = torch.stack([item['input_ids'] for item in batch])
    attention_mask = torch.stack([item['attention_mask'] for item in batch])
    labels = torch.stack([item['labels'] for item in batch])
    
    # ë¬¸ìì—´ ë°ì´í„°ë“¤ ë¦¬ìŠ¤íŠ¸ë¡œ ìœ ì§€
    fnames = [item['fname'] for item in batch]
    
    return {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'labels': labels,
        'fnames': fnames
    }
