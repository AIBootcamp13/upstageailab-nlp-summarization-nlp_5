"""
ë””ë°”ì´ìŠ¤ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°

Mac M1/M2ì˜ MPSì™€ Linux/Windowsì˜ CUDAë¥¼ ìë™ ê°ì§€í•˜ì—¬ 
ìµœì ì˜ ë””ë°”ì´ìŠ¤ë¥¼ ì„ íƒí•˜ê³  í”Œë«í¼ë³„ ìµœì í™” ì„¤ì •ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
import platform
import subprocess
import logging
from typing import Dict, Any, Tuple, Optional
from dataclasses import dataclass
import torch
import time

logger = logging.getLogger(__name__)


@dataclass
class DeviceInfo:
    """ë””ë°”ì´ìŠ¤ ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    device_type: str  # 'cuda', 'mps', 'cpu'
    device_name: str
    device_index: int = 0
    memory_gb: Optional[float] = None
    compute_capability: Optional[str] = None
    
    def __str__(self) -> str:
        return f"{self.device_type}:{self.device_index} ({self.device_name})"


@dataclass 
class OptimizationConfig:
    """ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì •"""
    fp16: bool = False
    fp16_opt_level: str = "O1"
    batch_size: int = 4
    gradient_accumulation_steps: int = 1
    num_workers: int = 0
    pin_memory: bool = False
    mixed_precision: str = "no"  # 'no', 'fp16', 'bf16'
    gradient_checkpointing: bool = False
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'fp16': self.fp16,
            'fp16_opt_level': self.fp16_opt_level,
            'per_device_train_batch_size': self.batch_size,
            'gradient_accumulation_steps': self.gradient_accumulation_steps,
            'dataloader_num_workers': self.num_workers,
            'dataloader_pin_memory': self.pin_memory,
            'mixed_precision': self.mixed_precision,
            'gradient_checkpointing': self.gradient_checkpointing
        }


def get_system_info() -> Dict[str, Any]:
    """
    ì‹œìŠ¤í…œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Returns:
        ì‹œìŠ¤í…œ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    info = {
        'platform': platform.system(),
        'platform_release': platform.release(),
        'platform_version': platform.version(),
        'architecture': platform.machine(),
        'processor': platform.processor(),
        'python_version': platform.python_version(),
        'torch_version': torch.__version__,
    }
    
    # PyTorch ë¹Œë“œ ì •ë³´
    info['torch_cuda_available'] = torch.cuda.is_available()
    info['torch_cuda_version'] = torch.version.cuda if torch.cuda.is_available() else None
    info['torch_cudnn_version'] = torch.backends.cudnn.version() if torch.cuda.is_available() else None
    
    # MPS ì§€ì› (Mac M1/M2)
    info['torch_mps_available'] = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
    
    # CPU ì •ë³´
    info['cpu_count'] = os.cpu_count()
    
    logger.info(f"ì‹œìŠ¤í…œ ì •ë³´: {info}")
    return info


def detect_cuda_devices() -> Dict[int, DeviceInfo]:
    """
    CUDA ë””ë°”ì´ìŠ¤ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
    
    Returns:
        ë””ë°”ì´ìŠ¤ ì¸ë±ìŠ¤ë¥¼ í‚¤ë¡œ í•˜ëŠ” DeviceInfo ë”•ì…”ë„ˆë¦¬
    """
    devices = {}
    
    if not torch.cuda.is_available():
        return devices
    
    device_count = torch.cuda.device_count()
    logger.info(f"CUDA ë””ë°”ì´ìŠ¤ {device_count}ê°œ ê°ì§€ë¨")
    
    for i in range(device_count):
        props = torch.cuda.get_device_properties(i)
        
        # ë©”ëª¨ë¦¬ í¬ê¸° (GB)
        memory_gb = props.total_memory / (1024**3)
        
        # ì»´í“¨íŠ¸ ëŠ¥ë ¥
        compute_capability = f"{props.major}.{props.minor}"
        
        device_info = DeviceInfo(
            device_type='cuda',
            device_name=props.name,
            device_index=i,
            memory_gb=memory_gb,
            compute_capability=compute_capability
        )
        
        devices[i] = device_info
        logger.info(f"  GPU {i}: {device_info}")
    
    return devices


def detect_mps_device() -> Optional[DeviceInfo]:
    """
    MPS (Metal Performance Shaders) ë””ë°”ì´ìŠ¤ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
    
    Returns:
        MPS ë””ë°”ì´ìŠ¤ ì •ë³´ ë˜ëŠ” None
    """
    if not hasattr(torch.backends, 'mps') or not torch.backends.mps.is_available():
        return None
    
    # macOS ë²„ì „ í™•ì¸
    try:
        macos_version = platform.mac_ver()[0]
        logger.info(f"macOS ë²„ì „: {macos_version}")
    except:
        macos_version = "Unknown"
    
    # Apple Silicon í™•ì¸
    processor = platform.processor()
    if 'arm' in processor.lower():
        chip_name = "Apple Silicon (M1/M2)"
    else:
        chip_name = "Unknown"
    
    device_info = DeviceInfo(
        device_type='mps',
        device_name=f"{chip_name} - macOS {macos_version}",
        device_index=0
    )
    
    logger.info(f"MPS ë””ë°”ì´ìŠ¤ ê°ì§€ë¨: {device_info}")
    return device_info


def get_optimal_device() -> Tuple[torch.device, DeviceInfo]:
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ìµœì ì˜ ë””ë°”ì´ìŠ¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
    ìš°ì„ ìˆœìœ„: CUDA > MPS > CPU
    
    Returns:
        (torch.device, DeviceInfo) íŠœí”Œ
    """
    # CUDA í™•ì¸
    cuda_devices = detect_cuda_devices()
    if cuda_devices:
        # ë©”ëª¨ë¦¬ê°€ ê°€ì¥ í° GPU ì„ íƒ
        best_gpu = max(cuda_devices.values(), key=lambda d: d.memory_gb or 0)
        device = torch.device(f'cuda:{best_gpu.device_index}')
        logger.info(f"ìµœì  ë””ë°”ì´ìŠ¤ë¡œ CUDA ì„ íƒ: {best_gpu}")
        return device, best_gpu
    
    # MPS í™•ì¸
    mps_device = detect_mps_device()
    if mps_device:
        device = torch.device('mps')
        logger.info(f"ìµœì  ë””ë°”ì´ìŠ¤ë¡œ MPS ì„ íƒ: {mps_device}")
        return device, mps_device
    
    # CPU ì‚¬ìš©
    cpu_info = DeviceInfo(
        device_type='cpu',
        device_name=platform.processor() or 'CPU',
        device_index=0
    )
    device = torch.device('cpu')
    logger.info(f"GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ CPU ì‚¬ìš©: {cpu_info}")
    return device, cpu_info
def get_rtx3090_extreme_config(model_architecture: str = 'base', use_unsloth: bool = True) -> Dict[str, Any]:
    """
    RTX 3090 + CUDA 12.2 + Unsloth ê·¹í•œ ìµœì í™” ì„¤ì •
    
    Args:
        model_architecture: ëª¨ë¸ ì•„í‚¤í…ì²˜ ('mt5', 'bart', 't5', 'kobart', 'base')
        use_unsloth: Unsloth ì‚¬ìš© ì—¬ë¶€
        
    Returns:
        RTX 3090 ê·¹í•œ ìµœì í™” ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    
    # RTX 3090 ê¸°ë³¸ ì‚¬ì–‘: 24GB VRAM, 48ì½”ì–´ CPU, 251GB RAM
    base_config = {
        'device_memory_gb': 24.0,
        'cpu_cores': 48,
        'system_ram_gb': 251.0,
        'cuda_version': '12.2',
        'use_unsloth': use_unsloth
    }
    
    # ì•„í‚¤í…ì²˜ë³„ ê·¹í•œ ìµœì í™” ë§¤íŠ¸ë¦­ìŠ¤
    optimization_matrix = {
        'mt5': {
            'per_device_train_batch_size': 12 if use_unsloth else 6,
            'per_device_eval_batch_size': 24 if use_unsloth else 12,
            'gradient_accumulation_steps': 4,
            'encoder_max_len': 1536 if use_unsloth else 1024,
            'decoder_max_len': 256,
            'dataloader_num_workers': 36,
            'lora_rank': 256 if use_unsloth else 128,
            'lora_alpha': 512 if use_unsloth else 256,
            'generation_num_beams': 16 if use_unsloth else 10
        },
        'bart': {
            'per_device_train_batch_size': 20 if use_unsloth else 8,
            'per_device_eval_batch_size': 32 if use_unsloth else 16,
            'gradient_accumulation_steps': 3,
            'encoder_max_len': 1280 if use_unsloth else 1024,
            'decoder_max_len': 256,
            'dataloader_num_workers': 36,
            'lora_rank': 192 if use_unsloth else 96,
            'lora_alpha': 384 if use_unsloth else 192,
            'generation_num_beams': 12 if use_unsloth else 8
        },
        'kobart': {
            'per_device_train_batch_size': 20 if use_unsloth else 8,
            'per_device_eval_batch_size': 32 if use_unsloth else 16,
            'gradient_accumulation_steps': 3,
            'encoder_max_len': 1280 if use_unsloth else 1024,
            'decoder_max_len': 256,
            'dataloader_num_workers': 36,
            'lora_rank': 192 if use_unsloth else 96,
            'lora_alpha': 384 if use_unsloth else 192,
            'generation_num_beams': 12 if use_unsloth else 8
        },
        't5': {
            'per_device_train_batch_size': 16 if use_unsloth else 8,
            'per_device_eval_batch_size': 28 if use_unsloth else 14,
            'gradient_accumulation_steps': 3,
            'encoder_max_len': 1024,
            'decoder_max_len': 200,
            'dataloader_num_workers': 36,
            'lora_rank': 128 if use_unsloth else 64,
            'lora_alpha': 256 if use_unsloth else 128,
            'generation_num_beams': 10 if use_unsloth else 6
        },
        'base': {
            'per_device_train_batch_size': 16 if use_unsloth else 8,
            'per_device_eval_batch_size': 24 if use_unsloth else 12,
            'gradient_accumulation_steps': 4,
            'encoder_max_len': 1024,
            'decoder_max_len': 200,
            'dataloader_num_workers': 32,
            'lora_rank': 128 if use_unsloth else 64,
            'lora_alpha': 256 if use_unsloth else 128,
            'generation_num_beams': 8
        }
    }
    
    # CUDA 12.2 + RTX 3090 íŠ¹í™” ì„¤ì •
    cuda_optimizations = {
        'fp16': False,  # RTX 3090ì—ì„œëŠ” bf16 ì„ í˜¸
        'bf16': True,   # RTX 3090 + CUDA 12.2 ìµœì 
        'tf32': True,   # Ampere ì•„í‚¤í…ì²˜ ìµœì í™”
        'torch_compile': True,  # PyTorch 2.7.1 ì»´íŒŒì¼ ìµœì í™”
        'gradient_checkpointing': False if use_unsloth else True,  # UnslothëŠ” ìì²´ ìµœì í™”
        'dataloader_pin_memory': True,  # 251GB RAM í™œìš©
        'dataloader_persistent_workers': True,  # ì›Œì»¤ ì¬ì‚¬ìš©
        'group_by_length': True,  # íš¨ìœ¨ì„± ì¦ëŒ€
        'remove_unused_columns': False,  # ì „ì²´ ì •ë³´ í™œìš©
        'optim': 'adamw_8bit' if use_unsloth else 'adamw_hf',  # Unsloth í˜¸í™˜ ì˜µí‹°ë§ˆì´ì €
        'adam_beta1': 0.9,
        'adam_beta2': 0.95 if use_unsloth else 0.999,  # Unsloth ì¶”ì²œê°’
        'max_grad_norm': 0.3 if use_unsloth else 1.0   # Unslothì™€ í•¨ê»˜ ë‚®ì€ ê°’
    }
    
    # ì•„í‚¤í…ì²˜ë³„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    arch_config = optimization_matrix.get(model_architecture.lower(), optimization_matrix['base'])
    
    # ìµœì¢… ì„¤ì • ì¡°í•©
    final_config = {
        **base_config,
        **arch_config,
        **cuda_optimizations
    }
    
    # ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ ê³„ì‚°
    memory_efficiency = 40 if use_unsloth else 20  # Unsloth ë©”ëª¨ë¦¬ ì ˆì•½
    speed_improvement = 2.5 if use_unsloth else 1.5  # Unsloth ì†ë„ í–¥ìƒ
    
    final_config.update({
        'expected_memory_efficiency_percent': memory_efficiency,
        'expected_speed_improvement': speed_improvement,
        'effective_batch_size': arch_config['per_device_train_batch_size'] * arch_config['gradient_accumulation_steps'],
        'total_vram_utilization_target': 90.0,  # 24GB ì¤‘ 90% ëª©í‘œ
        'cpu_utilization_target': 75.0,  # 48ì½”ì–´ ì¤‘ 75% ëª©í‘œ
        'optimization_level': 'extreme',
        'architecture': model_architecture,
        'profile_name': f'RTX3090_Extreme_{model_architecture.upper()}_{"Unsloth" if use_unsloth else "Standard"}'
    })
    
    logger.info(f"RTX 3090 ê·¹í•œ ìµœì í™” í”„ë¡œíŒŒì¼ ìƒì„±: {final_config['profile_name']}")
    logger.info(f"ë°°ì¹˜ í¬ê¸°: {arch_config['per_device_train_batch_size']}, "
                f"ìœ íš¨ ë°°ì¹˜: {final_config['effective_batch_size']}, "
                f"ì›Œì»¤ ìˆ˜: {arch_config['dataloader_num_workers']}")
    
    return final_config


def apply_rtx3090_extreme_optimizations(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    ê¸°ì¡´ ì„¤ì •ì— RTX 3090 ê·¹í•œ ìµœì í™”ë¥¼ ì ìš©
    
    Args:
        config: ê¸°ì¡´ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        
    Returns:
        RTX 3090 ê·¹í•œ ìµœì í™”ê°€ ì ìš©ëœ ì„¤ì •
    """
    # GPU ì •ë³´ í™•ì¸
    if not torch.cuda.is_available():
        logger.warning("CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ RTX 3090 ìµœì í™”ë¥¼ ì ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return config
    
    device_props = torch.cuda.get_device_properties(0)
    if 'RTX 3090' not in device_props.name:
        logger.warning(f"RTX 3090ì´ ì•„ë‹ˆë¯€ë¡œ ê·¹í•œ ìµœì í™”ë¥¼ ì ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {device_props.name}")
        return config
    
    # ì•„í‚¤í…ì²˜ ì¶”ë¡ 
    model_arch = 'base'
    if 'mt5' in config.get('model_name', '').lower():
        model_arch = 'mt5'
    elif 'bart' in config.get('model_name', '').lower():
        model_arch = 'kobart' if 'kobart' in config.get('model_name', '').lower() else 'bart'
    elif 't5' in config.get('model_name', '').lower():
        model_arch = 't5'
    
    # Unsloth ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    use_unsloth = config.get('qlora', {}).get('use_unsloth', True)
    
    # ê·¹í•œ ìµœì í™” ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    extreme_config = get_rtx3090_extreme_config(model_arch, use_unsloth)
    
    # ê¸°ì¡´ ì„¤ì •ì— ê·¹í•œ ìµœì í™” ì ìš©
    optimized_config = config.copy()
    
    # training ì„¹ì…˜ ì—…ë°ì´íŠ¸
    if 'training' not in optimized_config:
        optimized_config['training'] = {}
    
    training_updates = {
        'per_device_train_batch_size': extreme_config['per_device_train_batch_size'],
        'per_device_eval_batch_size': extreme_config['per_device_eval_batch_size'],
        'gradient_accumulation_steps': extreme_config['gradient_accumulation_steps'],
        'dataloader_num_workers': extreme_config['dataloader_num_workers'],
        'dataloader_pin_memory': extreme_config['dataloader_pin_memory'],
        'bf16': extreme_config['bf16'],
        'fp16': extreme_config['fp16'],
        'gradient_checkpointing': extreme_config['gradient_checkpointing'],
        'group_by_length': extreme_config['group_by_length'],
        'optim': extreme_config['optim'],
        'adam_beta1': extreme_config['adam_beta1'],
        'adam_beta2': extreme_config['adam_beta2'],
        'max_grad_norm': extreme_config['max_grad_norm']
    }
    
    optimized_config['training'].update(training_updates)
    
    # tokenizer ì„¹ì…˜ ì—…ë°ì´íŠ¸
    if 'tokenizer' not in optimized_config:
        optimized_config['tokenizer'] = {}
    
    tokenizer_updates = {
        'encoder_max_len': extreme_config['encoder_max_len'],
        'decoder_max_len': extreme_config['decoder_max_len']
    }
    
    optimized_config['tokenizer'].update(tokenizer_updates)
    
    # qlora ì„¹ì…˜ ì—…ë°ì´íŠ¸
    if 'qlora' not in optimized_config:
        optimized_config['qlora'] = {}
    
    qlora_updates = {
        'lora_rank': extreme_config['lora_rank'],
        'lora_alpha': extreme_config['lora_alpha'],
        'use_unsloth': use_unsloth
    }
    
    optimized_config['qlora'].update(qlora_updates)
    
    # generation ì„¤ì • ì—…ë°ì´íŠ¸
    if 'generation' in optimized_config or 'training' in optimized_config:
        generation_updates = {
            'generation_num_beams': extreme_config['generation_num_beams'],
            'generation_max_length': extreme_config['decoder_max_len']
        }
        
        if 'training' in optimized_config:
            optimized_config['training'].update(generation_updates)
        if 'generation' in optimized_config:
            optimized_config['generation'].update(generation_updates)
    
    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
    optimized_config['_rtx3090_extreme_optimization'] = {
        'applied': True,
        'profile_name': extreme_config['profile_name'],
        'expected_memory_efficiency': extreme_config['expected_memory_efficiency_percent'],
        'expected_speed_improvement': extreme_config['expected_speed_improvement'],
        'effective_batch_size': extreme_config['effective_batch_size'],
        'optimization_timestamp': time.time()
    }
    
    logger.info(f"RTX 3090 ê·¹í•œ ìµœì í™” ì ìš© ì™„ë£Œ: {extreme_config['profile_name']}")
    
    return optimized_config


def setup_device_config(device_info: DeviceInfo, model_size: str = 'base', use_qlora: bool = False) -> OptimizationConfig:
    """
    ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì •ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        device_info: ë””ë°”ì´ìŠ¤ ì •ë³´
        model_size: ëª¨ë¸ í¬ê¸° ('small', 'base', 'large')
        use_qlora: QLoRA ì‚¬ìš© ì—¬ë¶€
        
    Returns:
        ìµœì í™” ì„¤ì •
    """
    config = OptimizationConfig()
    
    # ëª¨ë¸ í¬ê¸°ë³„ ê¸°ë³¸ ë°°ì¹˜ í¬ê¸°
    # QLoRA ì‚¬ìš© ì‹œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì´ ë†’ì•„ ë°°ì¹˜ í¬ê¸°ë¥¼ í¬ê²Œ ì„¤ì •
    if use_qlora:
        # ì¡°ì¥ë‹˜ ê¶Œì¥ì‚¬í•­: QLoRAë¡œ batch_size 24~32
        base_batch_sizes = {
            'small': {'cuda': 32, 'mps': 16, 'cpu': 8},
            'base': {'cuda': 24, 'mps': 12, 'cpu': 6},
            'large': {'cuda': 16, 'mps': 8, 'cpu': 4}
        }
    else:
        # Full fine-tuning ë°°ì¹˜ í¬ê¸° (ë³´ìˆ˜ì )
        base_batch_sizes = {
            'small': {'cuda': 16, 'mps': 8, 'cpu': 4},
            'base': {'cuda': 8, 'mps': 4, 'cpu': 2},
            'large': {'cuda': 4, 'mps': 2, 'cpu': 1}
        }
    
    base_batch_size = base_batch_sizes.get(model_size, base_batch_sizes['base'])
    
    if device_info.device_type == 'cuda':
        # RTX 3090 ìë™ ê°ì§€ ë° ê·¹í•œ ìµœì í™” ì ìš©
        if 'RTX 3090' in device_info.device_name:
            logger.info(f"RTX 3090 ê°ì§€ë¨: {device_info.device_name} - ê·¹í•œ ìµœì í™” ëª¨ë“œ í™œì„±í™”")
            
            # RTX 3090 ê·¹í•œ ìµœì í™” ì„¤ì • ì‚¬ìš©
            extreme_config = get_rtx3090_extreme_config(model_size, use_qlora)
            
            # OptimizationConfigì— ê·¹í•œ ìµœì í™” ì ìš©
            config.fp16 = extreme_config['fp16']
            config.mixed_precision = 'bf16' if extreme_config['bf16'] else ('fp16' if extreme_config['fp16'] else 'no')
            config.batch_size = extreme_config['per_device_train_batch_size']
            config.gradient_accumulation_steps = extreme_config['gradient_accumulation_steps']
            config.num_workers = extreme_config['dataloader_num_workers']
            config.pin_memory = extreme_config['dataloader_pin_memory']
            config.gradient_checkpointing = extreme_config['gradient_checkpointing']
            
            logger.info(f"RTX 3090 ê·¹í•œ ìµœì í™” ì ìš©: ë°°ì¹˜={config.batch_size}, ì›Œì»¤={config.num_workers}, ìœ íš¨ë°°ì¹˜={extreme_config['effective_batch_size']}")
            
            return config
        # CUDA ìµœì í™” ì„¤ì •
        config.fp16 = True
        config.mixed_precision = 'fp16'
        config.batch_size = base_batch_size['cuda']
        config.num_workers = 4
        config.pin_memory = True
        
        # ë©”ëª¨ë¦¬ í¬ê¸°ì— ë”°ë¥¸ ì¡°ì •
        if device_info.memory_gb:
            if device_info.memory_gb < 8:
                # ë©”ëª¨ë¦¬ê°€ ì ì„ ë•Œ
                config.batch_size = max(1, config.batch_size // 2)
                config.gradient_checkpointing = True
            elif device_info.memory_gb >= 16 and device_info.memory_gb < 24:
                # ì¤‘ê°„ ë©”ëª¨ë¦¬ (16-24GB)
                if use_qlora:
                    config.batch_size = min(32, config.batch_size)
                else:
                    config.batch_size = config.batch_size
            elif device_info.memory_gb >= 24:
                # ì¶©ë¶„í•œ ë©”ëª¨ë¦¬ (24GB ì´ìƒ)
                if use_qlora:
                    config.batch_size = min(48, int(config.batch_size * 1.5))
                else:
                    config.batch_size = min(32, config.batch_size * 2)
        
        # ì»´í“¨íŠ¸ ëŠ¥ë ¥ì— ë”°ë¥¸ ì¡°ì •
        if device_info.compute_capability:
            major, minor = map(int, device_info.compute_capability.split('.'))
            if major >= 7:  # Volta ì´ìƒ
                config.fp16_opt_level = "O2"
            if major >= 8:  # Ampere ì´ìƒ
                config.mixed_precision = 'bf16'  # BF16 ì§€ì›
                
    elif device_info.device_type == 'mps':
        # MPS ìµœì í™” ì„¤ì •
        config.fp16 = False  # MPSëŠ” í˜„ì¬ fp16 ì§€ì›ì´ ì œí•œì 
        config.mixed_precision = 'no'
        config.batch_size = base_batch_size['mps']
        config.num_workers = 0  # MPSì—ì„œëŠ” ë©€í‹°í”„ë¡œì„¸ì‹± ì´ìŠˆ ìˆìŒ
        config.pin_memory = False
        
        # MPS íŠ¹í™” í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
        os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
        
    else:  # CPU
        # CPU ìµœì í™” ì„¤ì •
        config.fp16 = False
        config.mixed_precision = 'no'
        config.batch_size = base_batch_size['cpu']
        config.num_workers = min(2, os.cpu_count() or 1)
        config.pin_memory = False
        config.gradient_checkpointing = True  # ë©”ëª¨ë¦¬ ì ˆì•½
    
    # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ìœ¼ë¡œ ìœ íš¨ ë°°ì¹˜ í¬ê¸° ìœ ì§€
    # QLoRA ì‚¬ìš© ì‹œëŠ” í° ë°°ì¹˜ í¬ê¸°ë¥¼ í—ˆìš©í•˜ë¯€ë¡œ gradient accumulation ìµœì†Œí™”
    if use_qlora:
        target_batch_size = config.batch_size  # QLoRAëŠ” í° ë°°ì¹˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    else:
        target_batch_size = base_batch_sizes['base']['cuda']
        
    if config.batch_size < target_batch_size and not use_qlora:
        config.gradient_accumulation_steps = target_batch_size // config.batch_size
    
    logger.info(f"{device_info.device_type} ìµœì í™” ì„¤ì • ({'QLoRA' if use_qlora else 'Full FT'}): {config}")
    return config


def print_device_summary():
    """ë””ë°”ì´ìŠ¤ ìš”ì•½ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print("\n" + "="*60)
    print("ë””ë°”ì´ìŠ¤ ê°ì§€ ë° ì„¤ì • ìš”ì•½ (RTX 3090 ê·¹í•œ ìµœì í™” í¬í•¨)")
    print("="*60)
    
    # ì‹œìŠ¤í…œ ì •ë³´
    sys_info = get_system_info()
    print(f"\ní”Œë«í¼: {sys_info['platform']} {sys_info['platform_release']}")
    print(f"ì•„í‚¤í…ì²˜: {sys_info['architecture']}")
    print(f"PyTorch ë²„ì „: {sys_info['torch_version']}")
    
    # CUDA ì •ë³´
    if sys_info['torch_cuda_available']:
        print(f"\nCUDA ì‚¬ìš© ê°€ëŠ¥: Yes")
        print(f"CUDA ë²„ì „: {sys_info['torch_cuda_version']}")
        cuda_devices = detect_cuda_devices()
        for device in cuda_devices.values():
            print(f"  - {device}")
    else:
        print(f"\nCUDA ì‚¬ìš© ê°€ëŠ¥: No")
    
    # MPS ì •ë³´
    if sys_info['torch_mps_available']:
        print(f"\nMPS ì‚¬ìš© ê°€ëŠ¥: Yes")
        mps_device = detect_mps_device()
        if mps_device:
            print(f"  - {mps_device}")
    else:
        print(f"\nMPS ì‚¬ìš© ê°€ëŠ¥: No")
    
    # ìµœì  ë””ë°”ì´ìŠ¤
    device, device_info = get_optimal_device()
    print(f"\nì„ íƒëœ ë””ë°”ì´ìŠ¤: {device}")
    
    # ìµœì í™” ì„¤ì •
    print(f"\nğŸš€ ìµœì í™” ì„¤ì •:")
    
    # RTX 3090 ê·¹í•œ ìµœì í™” í™•ì¸
    if device_info.device_type == 'cuda' and 'RTX 3090' in device_info.device_name:
        print(f"\nğŸ”¥ RTX 3090 ê·¹í•œ ìµœì í™” ëª¨ë“œ í™œì„±í™”!")
        print(f"   ë””ë°”ì´ìŠ¤: {device_info.device_name}")
        print(f"   VRAM: {device_info.memory_gb:.1f}GB")
        
        # ì•„í‚¤í…ì²˜ë³„ ê·¹í•œ ìµœì í™” ì„¤ì • í‘œì‹œ
        for arch in ['mt5', 'bart', 't5', 'kobart']:
            extreme_config = get_rtx3090_extreme_config(arch, True)  # Unsloth ì‚¬ìš© ê°€ì •
            print(f"\n   {arch.upper()} ê·¹í•œ ìµœì í™”:")
            print(f"     - ë°°ì¹˜ í¬ê¸°: {extreme_config['per_device_train_batch_size']} (ìœ íš¨: {extreme_config['effective_batch_size']})")
            print(f"     - ì‹œí€€ìŠ¤ ê¸¸ì´: {extreme_config['encoder_max_len']}")
            print(f"     - LoRA Rank: {extreme_config['lora_rank']}")
            print(f"     - ì›Œì»¤ ìˆ˜: {extreme_config['dataloader_num_workers']}")
            print(f"     - ì˜ˆìƒ ì†ë„ í–¥ìƒ: {extreme_config['expected_speed_improvement']:.1f}x")
            print(f"     - ë©”ëª¨ë¦¬ íš¨ìœ¨: {extreme_config['expected_memory_efficiency_percent']}% ì ˆì•½")
    
    print(f"\nğŸ“Š ê¸°ë³¸ ëª¨ë¸ í¬ê¸°ë³„ ìµœì í™” ì„¤ì •:")
    for model_size in ['small', 'base', 'large']:
        config = setup_device_config(device_info, model_size)
        print(f"\n{model_size.upper()} ëª¨ë¸ ìµœì í™” ì„¤ì •:")
        print(f"  - ë°°ì¹˜ í¬ê¸°: {config.batch_size}")
        print(f"  - Mixed Precision: {config.mixed_precision}")
        print(f"  - Gradient Accumulation: {config.gradient_accumulation_steps}")
        print(f"  - DataLoader Workers: {config.num_workers}")
    
    print("\n" + "="*60)


def test_rtx3090_extreme_config():
    """
    RTX 3090 ê·¹í•œ ìµœì í™” ì„¤ì • í…ŒìŠ¤íŠ¸
    """
    print("\n" + "="*50)
    print("ğŸ§ª RTX 3090 ê·¹í•œ ìµœì í™” í…ŒìŠ¤íŠ¸")
    print("="*50)
    
    # ì•„í‚¤í…ì²˜ë³„ í…ŒìŠ¤íŠ¸
    architectures = ['mt5', 'bart', 't5', 'kobart', 'base']
    
    for arch in architectures:
        print(f"\nğŸ” {arch.upper()} ì•„í‚¤í…ì²˜ í…ŒìŠ¤íŠ¸:")
        
        # Unsloth ì‚¬ìš©/ë¯¸ì‚¬ìš© ë¹„êµ
        for use_unsloth in [True, False]:
            config = get_rtx3090_extreme_config(arch, use_unsloth)
            mode = "Unsloth" if use_unsloth else "Standard"
            
            print(f"  {mode} ëª¨ë“œ:")
            print(f"    ë°°ì¹˜: {config['per_device_train_batch_size']} | ìœ íš¨: {config['effective_batch_size']}")
            print(f"    ì‹œí€€ìŠ¤: {config['encoder_max_len']} | LoRA: {config['lora_rank']}")
            print(f"    ì›Œì»¤: {config['dataloader_num_workers']} | ì†ë„: {config['expected_speed_improvement']:.1f}x")
    
    # GPU ê°ì§€ í…ŒìŠ¤íŠ¸
    if torch.cuda.is_available():
        device_props = torch.cuda.get_device_properties(0)
        print(f"\nğŸ“Š í˜„ì¬ GPU: {device_props.name}")
        print(f"VRAM: {device_props.total_memory / (1024**3):.1f}GB")
        
        if 'RTX 3090' in device_props.name:
            print("âœ… RTX 3090 ê°ì§€ë¨ - ê·¹í•œ ìµœì í™” ì‚¬ìš© ê°€ëŠ¥!")
        else:
            print("âš ï¸  RTX 3090ì´ ì•„ë‹ˆë¯€ë¡œ ê·¹í•œ ìµœì í™” ë¹„í™œì„±í™”")
    else:
        print("
âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    print("\n" + "="*50)


if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ê¸°ë³¸ ë””ë°”ì´ìŠ¤ ìš”ì•½ ì¶œë ¥
    print_device_summary()
    
    # RTX 3090 ê·¹í•œ ìµœì í™” í…ŒìŠ¤íŠ¸
    test_rtx3090_extreme_config()
