#!/usr/bin/env python3
"""
ìë™ ì‹¤í—˜ ì‹¤í–‰ ì‹œìŠ¤í…œ (ìƒëŒ€ ê²½ë¡œ ê¸°ì¤€, MPS/CUDA ìµœì í™”)

ì—¬ëŸ¬ YAML ì„¤ì •ì„ ìˆœì°¨ì ìœ¼ë¡œ ìë™ ì‹¤í–‰í•˜ëŠ” ì‹œìŠ¤í…œ
- ìƒëŒ€ ê²½ë¡œ ê¸°ì¤€ íŒŒì¼ ì²˜ë¦¬
- MPS/CUDA ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ ë° ìµœì í™”
- ì‹¤í—˜ ê²°ê³¼ ìë™ ì¶”ì  ë° ë¶„ì„
"""

import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import subprocess
import logging

# code ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

from utils.path_utils import PathManager
from utils.device_utils import get_optimal_device, setup_device_config
from utils.experiment_utils import ExperimentTracker, ModelRegistry
from utils.config_manager import ConfigManager

class AutoExperimentRunner:
    """ìë™ ì‹¤í—˜ ì‹¤í–‰ ê´€ë¦¬ì (ìƒëŒ€ ê²½ë¡œ ê¸°ì¤€)"""
    
    def __init__(self, 
                 base_config_path: str = "config/base_config.yaml",
                 output_dir: str = "outputs/auto_experiments"):
        """
        Args:
            base_config_path: ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ)
            output_dir: ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ìƒëŒ€ ê²½ë¡œ)
        """
        # ìƒëŒ€ ê²½ë¡œ í™•ì¸
        if Path(base_config_path).is_absolute() or Path(output_dir).is_absolute():
            raise ValueError("ëª¨ë“  ê²½ë¡œëŠ” ìƒëŒ€ ê²½ë¡œì—¬ì•¼ í•©ë‹ˆë‹¤")
        
        self.base_config_path = PathManager.resolve_path(base_config_path)
        self.output_dir = PathManager.ensure_dir(output_dir)
        
        # ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        self.device = get_optimal_device()
        
        # ì‹¤í—˜ ì¶”ì  ì´ˆê¸°í™”
        self.tracker = ExperimentTracker(f"{output_dir}/experiments")
        self.registry = ModelRegistry(f"{output_dir}/models")
        
        # ë¡œê¹… ì„¤ì •
        self.logger = self._setup_logger()
        
        print(f"ğŸš€ ìë™ ì‹¤í—˜ ì‹¤í–‰ê¸° ì´ˆê¸°í™”")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    
    def _setup_logger(self) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        
        # ë¡œê·¸ íŒŒì¼ í•¸ë“¤ëŸ¬
        log_file = PathManager.ensure_dir("logs") / "auto_experiments.log"
        file_handler = logging.FileHandler(log_file)
        
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
        
        return logger
    
    def discover_experiment_configs(self, config_dir: str = "config/experiments") -> List[Path]:
        """
        ì‹¤í—˜ ì„¤ì • íŒŒì¼ë“¤ì„ ìë™ ë°œê²¬
        
        Args:
            config_dir: ì‹¤í—˜ ì„¤ì • ë””ë ‰í† ë¦¬ (ìƒëŒ€ ê²½ë¡œ)
            
        Returns:
            ë°œê²¬ëœ YAML ì„¤ì • íŒŒì¼ ëª©ë¡
        """
        if Path(config_dir).is_absolute():
            raise ValueError(f"ì„¤ì • ë””ë ‰í† ë¦¬ëŠ” ìƒëŒ€ ê²½ë¡œì—¬ì•¼ í•©ë‹ˆë‹¤: {config_dir}")
        
        config_path = PathManager.resolve_path(config_dir)
        
        if not config_path.exists():
            self.logger.warning(f"ì„¤ì • ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {config_dir}")
            return []
        
        # YAML íŒŒì¼ ê²€ìƒ‰
        yaml_files = []
        for pattern in ['*.yaml', '*.yml']:
            yaml_files.extend(config_path.glob(pattern))
        
        # íŒŒì¼ëª…ìœ¼ë¡œ ì •ë ¬ (ì‹¤í–‰ ìˆœì„œ ë³´ì¥)
        yaml_files.sort(key=lambda x: x.name)
        
        self.logger.info(f"ë°œê²¬ëœ ì‹¤í—˜ ì„¤ì •: {len(yaml_files)}ê°œ")
        for file in yaml_files:
            self.logger.info(f"  - {file.relative_to(PathManager.get_project_root())}")
        
        return yaml_files
    
    def load_experiment_config(self, config_path: Path) -> Dict[str, Any]:
        """
        ì‹¤í—˜ ì„¤ì • ë¡œë”© ë° ë””ë°”ì´ìŠ¤ ìµœì í™” ì ìš©
        
        Args:
            config_path: ì„¤ì • íŒŒì¼ ì ˆëŒ€ ê²½ë¡œ
            
        Returns:
            ë””ë°”ì´ìŠ¤ ìµœì í™”ê°€ ì ìš©ëœ ì„¤ì •
        """
        # ìƒëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        relative_path = config_path.relative_to(PathManager.get_project_root())
        
        # ê¸°ë³¸ ì„¤ì • ë¡œë”©
        base_manager = ConfigManager(self.base_config_path.relative_to(PathManager.get_project_root()))
        base_config = base_manager.get_config()
        
        # ì‹¤í—˜ë³„ ì„¤ì • ë¡œë”©
        exp_manager = ConfigManager(str(relative_path))
        exp_config = exp_manager.get_config()
        
        # ì„¤ì • ë³‘í•© (ì‹¤í—˜ ì„¤ì •ì´ ìš°ì„ )
        merged_config = base_manager.merge_configs(base_config, exp_config)
        
        # ë””ë°”ì´ìŠ¤ ìµœì í™” ì ìš©
        optimized_config = setup_device_config(merged_config)
        
        self.logger.info(f"ì„¤ì • ë¡œë”© ì™„ë£Œ: {relative_path}")
        self.logger.info(f"ë””ë°”ì´ìŠ¤ ìµœì í™” ì ìš©: {self.device}")
        
        return optimized_config
    
    def run_single_experiment(self, 
                            config_path: Path, 
                            experiment_name: Optional[str] = None) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰
        
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
            experiment_name: ì‹¤í—˜ ì´ë¦„ (Noneì´ë©´ íŒŒì¼ëª… ì‚¬ìš©)
            
        Returns:
            ì‹¤í—˜ ê²°ê³¼ ìš”ì•½
        """
        if experiment_name is None:
            experiment_name = config_path.stem
        
        self.logger.info(f"ì‹¤í—˜ ì‹œì‘: {experiment_name}")
        
        try:
            # ì„¤ì • ë¡œë”©
            config = self.load_experiment_config(config_path)
            
            # ì‹¤í—˜ ì¶”ì  ì‹œì‘
            exp_id = self.tracker.start_experiment(
                name=experiment_name,
                description=f"ìë™ ì‹¤í—˜: {config_path.name}",
                config=config,
                device=self.device,
                config_file=str(config_path.relative_to(PathManager.get_project_root()))
            )
            
            # ì‹¤ì œ í•™ìŠµ ì‹¤í–‰
            result = self._execute_training(config, exp_id)
            
            # ì‹¤í—˜ ì¢…ë£Œ
            final_metrics = {
                'best_rouge_combined_f1': result.get('best_rouge_combined_f1', 0),
                'training_time_minutes': result.get('training_time_minutes', 0),
                'total_epochs': result.get('total_epochs', 0)
            }
            
            experiment_summary = self.tracker.end_experiment(
                exp_id, 
                final_metrics, 
                "completed"
            )
            
            # ëª¨ë¸ ë“±ë¡ (ì„±ëŠ¥ì´ ì¢‹ì€ ê²½ìš°)
            if result.get('best_rouge_combined_f1', 0) > 0.3:  # ì„ê³„ê°’
                model_id = self.registry.register_model(
                    name=f"{experiment_name}_model",
                    architecture=config.get('model', {}).get('name', 'unknown'),
                    config=config,
                    performance=final_metrics,
                    model_path=result.get('model_path', ''),
                    experiment_id=exp_id
                )
                result['model_id'] = model_id
            
            self.logger.info(f"ì‹¤í—˜ ì™„ë£Œ: {experiment_name} (ROUGE: {final_metrics['best_rouge_combined_f1']:.4f})")
            
            return result
            
        except Exception as e:
            self.logger.error(f"ì‹¤í—˜ ì‹¤íŒ¨: {experiment_name} - {e}")
            
            # ì‹¤íŒ¨í•œ ì‹¤í—˜ë„ ì¶”ì 
            try:
                self.tracker.end_experiment(exp_id, {}, "failed")
            except:
                pass
            
            return {
                'experiment_name': experiment_name,
                'status': 'failed',
                'error': str(e)
            }
    
    def _execute_training(self, config: Dict[str, Any], exp_id: str) -> Dict[str, Any]:
        """
        ì‹¤ì œ í•™ìŠµ ì‹¤í–‰ (trainer.py í˜¸ì¶œ)
        
        Args:
            config: ì‹¤í—˜ ì„¤ì •
            exp_id: ì‹¤í—˜ ID
            
        Returns:
            í•™ìŠµ ê²°ê³¼
        """
        # ì„ì‹œ ì„¤ì • íŒŒì¼ ìƒì„±
        temp_config_path = self.output_dir / f"temp_config_{exp_id}.yaml"
        
        import yaml
        with open(temp_config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        
        try:
            # trainer.py ì‹¤í–‰
            cmd = [
                sys.executable,
                str(PathManager.resolve_path("code/trainer.py")),
                "--config", str(temp_config_path.relative_to(PathManager.get_project_root())),
                "--experiment-name", f"auto_exp_{exp_id[:8]}",
                "--device", self.device
            ]
            
            start_time = time.time()
            
            # subprocessë¡œ ì‹¤í–‰
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                cwd=PathManager.get_project_root(),
                timeout=7200  # 2ì‹œê°„ íƒ€ì„ì•„ì›ƒ
            )
            
            end_time = time.time()
            training_time = (end_time - start_time) / 60  # ë¶„ ë‹¨ìœ„
            
            if result.returncode == 0:
                # ì„±ê³µ ì‹œ ê²°ê³¼ íŒŒì‹±
                return self._parse_training_result(result.stdout, training_time)
            else:
                raise RuntimeError(f"Training failed: {result.stderr}")
        
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if temp_config_path.exists():
                temp_config_path.unlink()
    
    def _parse_training_result(self, stdout: str, training_time: float) -> Dict[str, Any]:
        """í•™ìŠµ ê²°ê³¼ íŒŒì‹±"""
        # ê°„ë‹¨í•œ ê²°ê³¼ íŒŒì‹± (ì‹¤ì œë¡œëŠ” ë” ì •êµí•˜ê²Œ êµ¬í˜„)
        result = {
            'training_time_minutes': training_time,
            'status': 'completed'
        }
        
        # ROUGE ì ìˆ˜ ì¶”ì¶œ (ë¡œê·¸ì—ì„œ)
        lines = stdout.split('\n')
        for line in lines:
            if 'rouge_combined_f1' in line.lower():
                try:
                    # ì˜ˆ: "ROUGE Combined F1: 0.4567"
                    score = float(line.split(':')[-1].strip())
                    result['best_rouge_combined_f1'] = score
                except:
                    pass
        
        return result
    
    def run_all_experiments(self, 
                          config_dir: str = "config/experiments",
                          max_parallel: int = 1) -> Dict[str, Any]:
        """
        ëª¨ë“  ì‹¤í—˜ ìˆœì°¨ ì‹¤í–‰
        
        Args:
            config_dir: ì‹¤í—˜ ì„¤ì • ë””ë ‰í† ë¦¬ (ìƒëŒ€ ê²½ë¡œ)
            max_parallel: ìµœëŒ€ ë³‘ë ¬ ì‹¤í–‰ ìˆ˜ (í˜„ì¬ëŠ” 1ë§Œ ì§€ì›)
            
        Returns:
            ì „ì²´ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½
        """
        print(f"ğŸ” ì‹¤í—˜ ì„¤ì • ê²€ìƒ‰ ì¤‘: {config_dir}")
        config_files = self.discover_experiment_configs(config_dir)
        
        if not config_files:
            print(f"âŒ ì‹¤í—˜ ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {config_dir}")
            return {'status': 'no_configs', 'results': []}
        
        print(f"ğŸ“‹ ì´ {len(config_files)}ê°œ ì‹¤í—˜ì„ ìˆœì°¨ ì‹¤í–‰í•©ë‹ˆë‹¤")
        
        overall_start_time = time.time()
        all_results = []
        
        for i, config_file in enumerate(config_files, 1):
            print(f"\nğŸš€ ì‹¤í—˜ {i}/{len(config_files)}: {config_file.name}")
            
            experiment_name = f"{i:02d}_{config_file.stem}"
            result = self.run_single_experiment(config_file, experiment_name)
            all_results.append(result)
            
            # ì‹¤í—˜ ê°„ íœ´ì‹ (ë¦¬ì†ŒìŠ¤ ì •ë¦¬)
            if i < len(config_files):
                print("â±ï¸ ë‹¤ìŒ ì‹¤í—˜ ì¤€ë¹„ ì¤‘... (30ì´ˆ ëŒ€ê¸°)")
                time.sleep(30)
        
        overall_end_time = time.time()
        total_time = (overall_end_time - overall_start_time) / 3600  # ì‹œê°„ ë‹¨ìœ„
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        summary = self._generate_experiment_summary(all_results, total_time)
        
        # ê²°ê³¼ ì €ì¥
        summary_file = self.output_dir / "experiment_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ‰ ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ!")
        print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì‹œê°„")
        print(f"ğŸ“„ ê²°ê³¼ ìš”ì•½: {summary_file.relative_to(PathManager.get_project_root())}")
        
        return summary
    
    def _generate_experiment_summary(self, 
                                   results: List[Dict[str, Any]], 
                                   total_time: float) -> Dict[str, Any]:
        """ì „ì²´ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        successful_results = [r for r in results if r.get('status') != 'failed']
        failed_results = [r for r in results if r.get('status') == 'failed']
        
        summary = {
            'execution_info': {
                'total_experiments': len(results),
                'successful_experiments': len(successful_results),
                'failed_experiments': len(failed_results),
                'total_time_hours': total_time,
                'device_used': self.device,
                'execution_date': datetime.now().isoformat()
            },
            'performance_summary': {},
            'best_experiment': None,
            'all_results': results
        }
        
        # ì„±ëŠ¥ ìš”ì•½
        if successful_results:
            rouge_scores = [r.get('best_rouge_combined_f1', 0) for r in successful_results]
            
            summary['performance_summary'] = {
                'best_rouge_score': max(rouge_scores),
                'average_rouge_score': sum(rouge_scores) / len(rouge_scores),
                'worst_rouge_score': min(rouge_scores)
            }
            
            # ìµœê³  ì„±ëŠ¥ ì‹¤í—˜ ì°¾ê¸°
            best_idx = rouge_scores.index(max(rouge_scores))
            summary['best_experiment'] = successful_results[best_idx]
        
        return summary
    
    def create_sample_configs(self, output_dir: str = "config/experiments"):
        """ìƒ˜í”Œ ì‹¤í—˜ ì„¤ì • íŒŒì¼ë“¤ ìƒì„±"""
        if Path(output_dir).is_absolute():
            raise ValueError(f"ì¶œë ¥ ë””ë ‰í† ë¦¬ëŠ” ìƒëŒ€ ê²½ë¡œì—¬ì•¼ í•©ë‹ˆë‹¤: {output_dir}")
        
        config_dir = PathManager.ensure_dir(output_dir)
        
        sample_configs = {
            "01_baseline.yaml": {
                "experiment_name": "baseline",
                "model": {"name": "gogamza/kobart-base-v2"},
                "training": {
                    "learning_rate": 0.0001,
                    "per_device_train_batch_size": 8,
                    "num_train_epochs": 5
                }
            },
            "02_high_lr.yaml": {
                "experiment_name": "high_learning_rate",
                "model": {"name": "gogamza/kobart-base-v2"},
                "training": {
                    "learning_rate": 0.0005,
                    "per_device_train_batch_size": 8,
                    "num_train_epochs": 5
                }
            },
            "03_large_batch.yaml": {
                "experiment_name": "large_batch_size",
                "model": {"name": "gogamza/kobart-base-v2"},
                "training": {
                    "learning_rate": 0.0001,
                    "per_device_train_batch_size": 16,
                    "num_train_epochs": 5
                }
            },
            "04_longer_training.yaml": {
                "experiment_name": "longer_training",
                "model": {"name": "gogamza/kobart-base-v2"},
                "training": {
                    "learning_rate": 0.0001,
                    "per_device_train_batch_size": 8,
                    "num_train_epochs": 10
                }
            }
        }
        
        import yaml
        for filename, config in sample_configs.items():
            file_path = config_dir / filename
            with open(file_path, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
            
            print(f"âœ… ìƒ˜í”Œ ì„¤ì • ìƒì„±: {file_path.relative_to(PathManager.get_project_root())}")
        
        print(f"\nğŸ“ ì´ {len(sample_configs)}ê°œ ìƒ˜í”Œ ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ")
        print(f"ğŸš€ ì‹¤í–‰ ë°©ë²•: python code/auto_experiment_runner.py --run-all")


def main():
    """CLI ì¸í„°í˜ì´ìŠ¤"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ìë™ ì‹¤í—˜ ì‹¤í–‰ ì‹œìŠ¤í…œ")
    parser.add_argument('--base-config', default="config/base_config.yaml",
                       help='ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ)')
    parser.add_argument('--config-dir', default="config/experiments",
                       help='ì‹¤í—˜ ì„¤ì • ë””ë ‰í† ë¦¬ (ìƒëŒ€ ê²½ë¡œ)')
    parser.add_argument('--output-dir', default="outputs/auto_experiments",
                       help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ìƒëŒ€ ê²½ë¡œ)')
    parser.add_argument('--run-all', action='store_true',
                       help='ëª¨ë“  ì‹¤í—˜ ìˆœì°¨ ì‹¤í–‰')
    parser.add_argument('--create-samples', action='store_true',
                       help='ìƒ˜í”Œ ì„¤ì • íŒŒì¼ë“¤ ìƒì„±')
    parser.add_argument('--experiment', type=str,
                       help='íŠ¹ì • ì‹¤í—˜ í•˜ë‚˜ë§Œ ì‹¤í–‰ (íŒŒì¼ëª…)')
    
    args = parser.parse_args()
    
    try:
        runner = AutoExperimentRunner(
            base_config_path=args.base_config,
            output_dir=args.output_dir
        )
        
        if args.create_samples:
            runner.create_sample_configs(args.config_dir)
        
        elif args.run_all:
            runner.run_all_experiments(args.config_dir)
        
        elif args.experiment:
            config_path = PathManager.resolve_path(f"{args.config_dir}/{args.experiment}")
            if not config_path.exists():
                print(f"âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
                return 1
            
            result = runner.run_single_experiment(config_path)
            print(f"ì‹¤í—˜ ê²°ê³¼: {result}")
        
        else:
            print("âŒ ì‹¤í–‰í•  ì‘ì—…ì„ ì§€ì •í•˜ì„¸ìš” (--run-all, --create-samples, --experiment)")
            return 1
        
        return 0
        
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
