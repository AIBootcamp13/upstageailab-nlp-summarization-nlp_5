#!/usr/bin/env python3
"""
ìë™ ì‹¤í—˜ ì‹¤í–‰ ì‹œìŠ¤í…œ (ìƒëŒ€ ê²½ë¡œ ê¸°ì¤€, MPS/CUDA ìµœì í™”)

ì—¬ëŸ¬ YAML ì„¤ì •ì„ ìˆœì°¨ì ìœ¼ë¡œ ìë™ ì‹¤í–‰í•˜ëŠ” ì‹œìŠ¤í…œ
- ìƒëŒ€ ê²½ë¡œ ê¸°ì¤€ íŒŒì¼ ì²˜ë¦¬
- MPS/CUDA ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ ë° ìµœì í™”
- ì‹¤í—˜ ê²°ê³¼ ìë™ ì¶”ì  ë° ë¶„ì„
"""

import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import subprocess
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# code ë””ë ‰í† ë¦¬ì˜ utils ì„í¬íŠ¸
from utils.path_utils import PathManager, path_manager
from utils.device_utils import get_optimal_device, setup_device_config
from utils.experiment_utils import ExperimentTracker, ModelRegistry
from utils import load_config

class AutoExperimentRunner:
    """ìë™ ì‹¤í—˜ ì‹¤í–‰ ê´€ë¦¬ì (ìƒëŒ€ ê²½ë¡œ ê¸°ì¤€)"""
    
    def __init__(self, 
                 base_config_path: str = "config/base_config.yaml",
                 output_dir: str = "outputs/auto_experiments"):
        """
        Args:
            base_config_path: ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ)
            output_dir: ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ìƒëŒ€ ê²½ë¡œ)
        """
        # ìƒëŒ€ ê²½ë¡œ í™•ì¸
        if Path(base_config_path).is_absolute() or Path(output_dir).is_absolute():
            raise ValueError("ëª¨ë“  ê²½ë¡œëŠ” ìƒëŒ€ ê²½ë¡œì—¬ì•¼ í•©ë‹ˆë‹¤")
        
        self.base_config_path = path_manager.resolve_path(base_config_path)
        self.output_dir = path_manager.ensure_dir(output_dir)
        
        # ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        device_tuple = get_optimal_device()
        if isinstance(device_tuple, tuple):
            self.device = device_tuple[0]  # torch.device ê°ì²´
            self.device_info = device_tuple[1] if len(device_tuple) > 1 else None
        else:
            self.device = device_tuple
            self.device_info = None
        
        # ì‹¤í—˜ ì¶”ì  ì´ˆê¸°í™”
        self.tracker = ExperimentTracker(f"{output_dir}/experiments")
        print(f"\nğŸ†— ExperimentTracker ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   log_experiment ë©”ì„œë“œ ì¡´ì¬: {hasattr(self.tracker, 'log_experiment')}")
        self.registry = ModelRegistry(f"{output_dir}/models")
        
        # ë¡œê¹… ì„¤ì •
        self.logger = self._setup_logger()
        
        print(f"ğŸš€ ìë™ ì‹¤í—˜ ì‹¤í–‰ê¸° ì´ˆê¸°í™”")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        if self.device_info and hasattr(self.device_info, 'device_name'):
            print(f"   GPU ì •ë³´: {self.device_info.device_name} ({self.device_info.memory_gb:.1f}GB)")
        print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    
    def _setup_logger(self) -> logging.Logger:
        log_file = path_manager.ensure_dir("logs") / "auto_experiments.log"
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        
        # ë¡œê·¸ íŒŒì¼ í•¸ë“¤ëŸ¬
        log_file = path_manager.ensure_dir("logs") / "auto_experiments.log"
        file_handler = logging.FileHandler(log_file)
        
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
        
        return logger
    
    def run_experiments(self, 
                       experiment_configs: List[str],
                       dry_run: bool = False,
                       continue_on_error: bool = True,
                       one_epoch: bool = False) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ ì‹¤í—˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
        
        Args:
            experiment_configs: ì‹¤í—˜ ì„¤ì • íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (ìƒëŒ€ ê²½ë¡œ)
            dry_run: ì‹¤ì œ ì‹¤í–‰ ì—†ì´ ì„¤ì •ë§Œ í™•ì¸
            continue_on_error: ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ìŒ ì‹¤í—˜ ê³„ì† ì§„í–‰
            one_epoch: 1ì—í¬í¬ë§Œ ì‹¤í–‰ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
            
        Returns:
            ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        results = {}
        
        for i, config_path in enumerate(experiment_configs):
            try:
                print(f"\n{'='*60}")
                print(f"ì‹¤í—˜ {i+1}/{len(experiment_configs)}: {config_path}")
                print(f"{'='*60}")
                
                # ìƒëŒ€ ê²½ë¡œ í™•ì¸
                if Path(config_path).is_absolute():
                    raise ValueError(f"ì„¤ì • ê²½ë¡œëŠ” ìƒëŒ€ ê²½ë¡œì—¬ì•¼ í•©ë‹ˆë‹¤: {config_path}")
                
                # ì„¤ì • ë¡œë“œ
                full_config = self._load_and_merge_config(config_path)
                
                # ë””ë°”ì´ìŠ¤ ì„¤ì • ì ìš©
                self._apply_device_config(full_config)
                
                if dry_run:
                    print("\n[DRY RUN] ì„¤ì • ë‚´ìš©:")
                    print(json.dumps(full_config, indent=2, ensure_ascii=False))
                    results[config_path] = {"status": "dry_run", "config": full_config}
                    continue
                
                # ì‹¤í—˜ ì‹¤í–‰
                result = self._run_single_experiment(full_config, config_path, one_epoch)
                results[config_path] = result
                
                # ì‹¤í—˜ ì¶”ì  - try-except ë¸”ë¡ ì¶”ê°€
                try:
                    self.tracker.log_experiment(
                        experiment_name=Path(config_path).stem,
                        config=full_config,
                        results=result
                    )
                except Exception as e:
                    self.logger.warning(f"ì‹¤í—˜ ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨: {e}")
                    # ë¡œê·¸ ì‹¤íŒ¨ê°€ ì „ì²´ ì‹¤í–‰ì„ ì¤‘ë‹¨í•˜ì§€ ì•Šë„ë¡ í•¨
                
                # ì‹¤í—˜ ê°„ ëŒ€ê¸° (GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ë“±)
                if i < len(experiment_configs) - 1:
                    print("\në‹¤ìŒ ì‹¤í—˜ ì¤€ë¹„ ì¤‘...")
                    time.sleep(5)
                    
            except Exception as e:
                self.logger.error(f"ì‹¤í—˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {config_path}", exc_info=True)
                results[config_path] = {"status": "error", "error": str(e)}
                
                if not continue_on_error:
                    raise
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        self._print_summary(results)
        
        return results
    
    def _load_and_merge_config(self, config_path: str) -> Dict[str, Any]:
        """ê¸°ë³¸ ì„¤ì •ê³¼ ì‹¤í—˜ ì„¤ì •ì„ ë³‘í•©"""
        # ê¸°ë³¸ ì„¤ì • ë¡œë“œ
        base_config = load_config(self.base_config_path)
        
        # ì‹¤í—˜ ì„¤ì • ë¡œë“œ
        exp_config_path = path_manager.resolve_path(config_path)
        exp_config = load_config(exp_config_path)
        
        # ì„¤ì • ë³‘í•© (ì‹¤í—˜ ì„¤ì •ì´ ìš°ì„ )
        merged = self._deep_merge(base_config, exp_config)
        
        return merged
    
    def _deep_merge(self, base: Dict, override: Dict) -> Dict:
        """ë”•ì…”ë„ˆë¦¬ ê¹Šì€ ë³‘í•©"""
        result = base.copy()
        
        for key, value in override.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = self._deep_merge(result[key], value)
            else:
                result[key] = value
        
        return result
    
    def _apply_device_config(self, config: Dict[str, Any]) -> None:
        """ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì • ì ìš©"""
        if not self.device_info:
            return
            
        # ëª¨ë¸ í¬ê¸° ì¶”ì •
        model_name = config.get('general', {}).get('model_name', '')
        if 'large' in model_name.lower() or 'xl' in model_name.lower():
            model_size = 'large'
        elif 'small' in model_name.lower() or 'tiny' in model_name.lower():
            model_size = 'small'
        else:
            model_size = 'base'
        
        # ìµœì í™” ì„¤ì • ìƒì„±
        opt_config = setup_device_config(self.device_info, model_size)
        
        # training ì„¹ì…˜ì— ì ìš©
        if 'training' not in config:
            config['training'] = {}
        
        # ê¸°ì¡´ ì„¤ì •ì„ ìœ ì§€í•˜ë©´ì„œ ë””ë°”ì´ìŠ¤ ìµœì í™” ì„¤ì • ì¶”ê°€
        training_config = config['training']
        opt_dict = opt_config.to_dict()
        
        for key, value in opt_dict.items():
            if key not in training_config:
                training_config[key] = value
        config['device'] = str(self.device)
        config['device_info'] = {
            'type': self.device_info.device_type,
            'name': self.device_info.device_name,
            'memory_gb': self.device_info.memory_gb
        } if hasattr(self.device_info, 'device_type') else None
    
    def _run_single_experiment(self, config: Dict[str, Any], config_path: str, one_epoch: bool = False) -> Dict[str, Any]:
        """ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰"""
        print(f"\nğŸ”§ _run_single_experiment ì‹œì‘: {config_path}")
        start_time = time.time()
        
        try:
            # í•­ìƒ í™˜ê²½ ë³€ìˆ˜ ë³µì‚¬
            import os
            env = os.environ.copy()
            
            # 1ì—í¬í¬ ëª¨ë“œë¥¼ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            if one_epoch:
                env['FORCE_ONE_EPOCH'] = '1'
                print(f"\nğŸš€ 1ì—í¬í¬ ëª¨ë“œë¡œ ì‹¤í–‰: {Path(config_path).stem}")
            
            # trainer.py ì‹¤í–‰
            cmd = [
                sys.executable,
                str(path_manager.resolve_path("code/trainer.py")),
                "--config", config_path
            ]
            
            print(f"\nì‹¤í–‰ ëª…ë ¹: {' '.join(cmd)}")
            print(f"í˜„ì¬ ë””ë ‰í† ë¦¬: {os.getcwd()}")
            print(f"Python ê²½ë¡œ: {sys.executable}")
            
            # í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                env=env
            )
            
            # ëª¨ë“  ì¶œë ¥ì„ ìˆ˜ì§‘í•˜ë©´ì„œ ì‹¤ì‹œê°„ í‘œì‹œ
            output_lines = []
            for line in process.stdout:
                print(line, end='')
                output_lines.append(line)
            
            # í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ëŒ€ê¸°
            process.wait()
            
            # ê²°ê³¼ ìˆ˜ì§‘
            if process.returncode == 0:
                result = self._collect_results(config, Path(config_path).stem)
                result['status'] = 'success'
                result['duration'] = time.time() - start_time
            else:
                # ì—ëŸ¬ ì‹œ ì¶œë ¥ ë‚´ìš©ë„ í¬í•¨
                error_output = '\n'.join(output_lines[-50:])  # ë§ˆì§€ë§‰ 50ì¤„ë§Œ
                result = {
                    'status': 'error',
                    'error': f'Training failed with exit code {process.returncode}\n\nLast output:\n{error_output}',
                    'duration': time.time() - start_time
                }
        
        except Exception as e:
            result = {
                'status': 'error',
                'error': str(e),
                'duration': time.time() - start_time
            }
        
        return result
    
    def _collect_results(self, config: Dict[str, Any], experiment_name: str) -> Dict[str, Any]:
        """ì‹¤í—˜ ê²°ê³¼ ìˆ˜ì§‘"""
        results = {
            'experiment_name': experiment_name,
            'model_name': config.get('general', {}).get('model_name', 'unknown')
        }
        
        # ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
        output_dir = Path(config.get('training', {}).get('output_dir', 'outputs'))
        
        # ë©”íŠ¸ë¦­ íŒŒì¼ ì½ê¸°
        metrics_file = output_dir / 'eval_results.json'
        if metrics_file.exists():
            with open(metrics_file, 'r') as f:
                metrics = json.load(f)
                results['metrics'] = metrics
        
        # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì •ë³´
        checkpoint_dirs = list(output_dir.glob('checkpoint-*'))
        if checkpoint_dirs:
            results['best_checkpoint'] = str(max(checkpoint_dirs, key=lambda p: p.stat().st_mtime))
        
        return results
    
    def _print_summary(self, results: Dict[str, Dict[str, Any]]) -> None:
        """ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        success_count = sum(1 for r in results.values() if r.get('status') == 'success')
        total_count = len(results)
        
        print(f"\nì´ ì‹¤í—˜: {total_count}")
        print(f"ì„±ê³µ: {success_count}")
        print(f"ì‹¤íŒ¨: {total_count - success_count}")
        
        # ì„±ê³µí•œ ì‹¤í—˜ì˜ ë©”íŠ¸ë¦­ ë¹„êµ
        print("\në©”íŠ¸ë¦­ ë¹„êµ:")
        print(f"{'ì‹¤í—˜ëª…':<30} {'ROUGE-1':<10} {'ROUGE-2':<10} {'ROUGE-L':<10}")
        print("-" * 60)
        
        for config_path, result in results.items():
            exp_name = Path(config_path).stem[:30]
            if result.get('status') == 'success' and 'metrics' in result:
                metrics = result['metrics']
                rouge1 = metrics.get('eval_rouge1', 0)
                rouge2 = metrics.get('eval_rouge2', 0)
                rougeL = metrics.get('eval_rougeL', 0)
                print(f"{exp_name:<30} {rouge1:<10.4f} {rouge2:<10.4f} {rougeL:<10.4f}")
            else:
                status = result.get('status', 'unknown')
                print(f"{exp_name:<30} {status}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        best_model = None
        best_score = 0
        
        for config_path, result in results.items():
            if result.get('status') == 'success' and 'metrics' in result:
                metrics = result['metrics']
                score = (metrics.get('eval_rouge1', 0) + 
                        metrics.get('eval_rouge2', 0) + 
                        metrics.get('eval_rougeL', 0)) / 3
                if score > best_score:
                    best_score = score
                    best_model = Path(config_path).stem
        
        if best_model:
            print(f"\nìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model} (í‰ê·  ROUGE: {best_score:.4f})")
    
    def run_single_config(self, config_path: str, dry_run: bool = False) -> Dict[str, Any]:
        """ë‹¨ì¼ ì„¤ì • íŒŒì¼ë¡œ ì‹¤í—˜ ì‹¤í–‰"""
        return self.run_experiments([config_path], dry_run=dry_run)


def main():
    """CLI ì§„ì…ì """
    import argparse
    
    parser = argparse.ArgumentParser(description="ìë™ ì‹¤í—˜ ì‹¤í–‰ ì‹œìŠ¤í…œ")
    parser.add_argument(
        '--config', '-c',
        type=str,
        nargs='+',
        help='ì‹¤í—˜ ì„¤ì • íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ, ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)'
    )
    parser.add_argument(
        '--base-config',
        type=str,
        default='config/base_config.yaml',
        help='ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: config/base_config.yaml)'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='outputs/auto_experiments',
        help='ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: outputs/auto_experiments)'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='ì‹¤ì œ ì‹¤í–‰ ì—†ì´ ì„¤ì •ë§Œ í™•ì¸'
    )
    parser.add_argument(
        '--stop-on-error',
        action='store_true',
        help='ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¤‘ë‹¨ (ê¸°ë³¸ê°’: ê³„ì† ì§„í–‰)'
    )
    parser.add_argument(
        '--one-epoch',
        action='store_true',
        help='1ì—í¬í¬ë§Œ ì‹¤í–‰ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)'
    )
    
    args = parser.parse_args()
    
    if not args.config:
        # ê¸°ë³¸ ì‹¤í—˜ ì„¸íŠ¸
        default_configs = [
            "config/experiments/01_baseline.yaml",
            "config/experiments/02_simple_augmentation.yaml",
            "config/experiments/03_high_learning_rate.yaml"
        ]
        print(f"ì„¤ì • íŒŒì¼ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤:")
        for config in default_configs:
            print(f"  - {config}")
        args.config = default_configs
    
    # ì‹¤í–‰ê¸° ì´ˆê¸°í™”
    runner = AutoExperimentRunner(
        base_config_path=args.base_config,
        output_dir=args.output_dir
    )
    
    # ì‹¤í—˜ ì‹¤í–‰
    results = runner.run_experiments(
        experiment_configs=args.config,
        dry_run=args.dry_run,
        continue_on_error=not args.stop_on_error,
        one_epoch=args.one_epoch
    )
    
    # ê²°ê³¼ ì €ì¥
    if not args.dry_run:
        result_file = Path(args.output_dir) / f"experiment_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(result_file, 'w') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nì‹¤í—˜ ê²°ê³¼ ì €ì¥: {result_file}")
    
    # ì„±ê³µ ì—¬ë¶€ ë°˜í™˜
    success_count = sum(1 for r in results.values() if r.get('status') == 'success')
    return 0 if success_count == len(results) else 1


if __name__ == "__main__":
    sys.exit(main())
