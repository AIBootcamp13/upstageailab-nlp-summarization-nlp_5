#!/usr/bin/env python3
"""
í•™ìŠµ í›„ ìë™ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ - baseline.py ì™„ë²½ í˜¸í™˜ ë²„ì „

baseline.pyì˜ inference() í•¨ìˆ˜ë¥¼ ì •í™•íˆ ì¬í˜„í•˜ë©´ì„œ
ë‹¤ì¤‘ ëª¨ë¸ì„ ì§€ì›í•˜ë„ë¡ í™•ì¥
"""

import os
import sys
import logging
from pathlib import Path
import pandas as pd
from datetime import datetime
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
from typing import Dict, Any, List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from code.utils.path_utils import path_manager
from code.utils.baseline_compatible import (
    BaselinePreprocess, 
    DatasetForInference,
    prepare_test_dataset,
    remove_special_tokens
)
from code.utils.model_handler import ModelSpecificHandler

logger = logging.getLogger(__name__)


class PostTrainingInference:
    """í•™ìŠµ í›„ ìë™ ì¶”ë¡  í´ë˜ìŠ¤ - baseline.py ì™„ë²½ í˜¸í™˜"""
    
    def __init__(self, experiment_name: str, model_path: str, config: dict):
        """
        Args:
            experiment_name: ì‹¤í—˜ëª…
            model_path: í•™ìŠµëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            config: ì‹¤í—˜ ì„¤ì •
        """
        self.experiment_name = experiment_name
        self.model_path = model_path
        self.config = config
        
        # ëª¨ë¸ë³„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        model_name = config.get('general', {}).get('model_name', '')
        self.model_config = ModelSpecificHandler.get_model_config(model_name, config)
        
        logger.info(f"Initializing PostTrainingInference for {experiment_name}")
        logger.info(f"Model: {model_name}, Architecture: {self.model_config.get('architecture')}")
        
        # eenzeenee ëª¨ë¸ìš© NLTK ì´ˆê¸°í™”
        if 'eenzeenee' in model_name.lower():
            self._setup_nltk_for_eenzeenee()
    
    def _setup_nltk_for_eenzeenee(self):
        """
        eenzeenee ëª¨ë¸ìš© NLTK punkt tokenizer ì„¤ì •
        """
        try:
            import nltk
            
            logger.info("ğŸ”§ eenzeenee ëª¨ë¸ìš© NLTK punkt tokenizer ì´ˆê¸°í™” ì¤‘...")
            
            # punkt tokenizerê°€ ì´ë¯¸ ë‹¤ìš´ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            try:
                nltk.data.find('tokenizers/punkt')
                logger.info("âœ… NLTK punkt tokenizer ì´ë¯¸ ì„¤ì¹˜ë¨")
            except LookupError:
                logger.info("ğŸ’¾ NLTK punkt tokenizer ë‹¤ìš´ë¡œë“œ ì¤‘...")
                nltk.download('punkt', quiet=True)
                logger.info("âœ… NLTK punkt tokenizer ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                
        except ImportError:
            logger.error("âŒ NLTKê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install nltk' ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
            raise
        except Exception as e:
            logger.warning(f"âš ï¸ NLTK ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def run_test_inference(self, test_file: str) -> str:
        """
        baseline.pyì˜ inference() í•¨ìˆ˜ë¥¼ ì •í™•íˆ ì¬í˜„
        
        Args:
            test_file: í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ìƒì„±ëœ ì œì¶œ íŒŒì¼ ê²½ë¡œ
        """
        # 1. ë””ë°”ì´ìŠ¤ ì„¤ì • (baseline.pyì™€ ë™ì¼)
        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        print('-'*10, f'device : {device}', '-'*10)
        print(torch.__version__)
        
        # 2. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        print('-'*10, 'Load tokenizer & model', '-'*10)
        model, tokenizer = self._load_model_and_tokenizer(device)
        
        # 3. íŠ¹ìˆ˜ í† í° ì¶”ê°€ (baseline.pyì™€ ë™ì¼)
        special_tokens_dict = {
            'additional_special_tokens': self.config['tokenizer']['special_tokens']
        }
        tokenizer.add_special_tokens(special_tokens_dict)
        model.resize_token_embeddings(len(tokenizer))
        
        # 4. ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™” (baseline.pyì˜ Preprocessì™€ ë™ì¼)
        preprocessor = BaselinePreprocess(
            self.config['tokenizer']['bos_token'],
            self.config['tokenizer']['eos_token']
        )
        
        # 5. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ (baseline.pyì˜ prepare_test_datasetì™€ ë™ì¼)
        test_data, test_encoder_inputs_dataset = self._prepare_test_data(
            preprocessor, tokenizer, test_file
        )
        
        # 6. DataLoader ìƒì„± (baseline.pyì™€ ë™ì¼)
        dataloader = DataLoader(
            test_encoder_inputs_dataset,
            batch_size=self.config['inference']['batch_size']
        )
        
        # 7. ì¶”ë¡  ì‹¤í–‰ (baseline.pyì™€ ì™„ì „ ë™ì¼)
        summary = []
        text_ids = []
        
        with torch.no_grad():
            for item in tqdm(dataloader):
                text_ids.extend(item['ID'])
                generated_ids = model.generate(
                    input_ids=item['input_ids'].to(device),
                    no_repeat_ngram_size=self.config['inference']['no_repeat_ngram_size'],
                    early_stopping=self.config['inference']['early_stopping'],
                    max_length=self.config['inference']['generate_max_length'],
                    num_beams=self.config['inference']['num_beams'],
                )
                for ids in generated_ids:
                    result = tokenizer.decode(ids)
                    summary.append(result)
        
        # 8. íŠ¹ìˆ˜ í† í° ì œê±° (baseline.pyì™€ ë™ì¼)
        remove_tokens = self.config['inference']['remove_tokens']
        preprocessed_summary = remove_special_tokens(summary, remove_tokens)
        
        # 9. ê²°ê³¼ ì €ì¥ (baseline.pyì™€ ë™ì¼í•œ í˜•ì‹)
        output = pd.DataFrame({
            "fname": test_data['fname'],
            "summary": preprocessed_summary,
        })
        
        # 10. íŒŒì¼ ì €ì¥
        submission_path = self._save_results(output)
        
        return str(submission_path)
    
    def _load_model_and_tokenizer(self, device):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ - ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›"""
        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
        
        # ëª¨ë¸ëª…ì—ì„œ ì›ë³¸ í† í¬ë‚˜ì´ì € ê²½ë¡œ ì¶”ì¶œ
        model_name = self.config['general']['model_name']
        
        print('-'*10, f'Model Name : {model_name}', '-'*10)
        
        # baseline.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # ëª¨ë¸ ë¡œë“œ (ì²´í¬í¬ì¸íŠ¸ì—ì„œ)
        model, _ = ModelSpecificHandler.load_model_for_inference(
            self.model_path,
            self.model_config,
            device
        )
        
        print('-'*10, 'Load tokenizer & model complete', '-'*10)
        
        return model, tokenizer
    
    def _prepare_test_data(self, preprocessor, tokenizer, test_file):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ - prefix ì²˜ë¦¬ í¬í•¨"""
        # config ì—…ë°ì´íŠ¸ (data_path ì„¤ì •)
        config_copy = self.config.copy()
        test_path = Path(test_file)
        config_copy['general']['data_path'] = str(test_path.parent)
        
        # baseline.pyì˜ prepare_test_dataset í˜¸ì¶œ
        test_data, test_encoder_inputs_dataset = prepare_test_dataset(
            config_copy, preprocessor, tokenizer
        )
        
        # ëª¨ë¸ë³„ prefix ì²˜ë¦¬ - ì„¤ì • íŒŒì¼ì˜ input_prefix ì§ì ‘ ì‚¬ìš©
        input_prefix = self.config.get('input_prefix', '')
        if input_prefix and input_prefix.strip():  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
            # ì´ë¯¸ í† í¬ë‚˜ì´ì§€ëœ ë°ì´í„°ì´ë¯€ë¡œ ì›ë³¸ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ì²˜ë¦¬
            test_df = pd.read_csv(test_file)
            dialogues = test_df['dialogue'].tolist()
            
            # prefix ì¶”ê°€
            dialogues_with_prefix = [f"{input_prefix}{d}" for d in dialogues]
            
            # ë‹¤ì‹œ í† í¬ë‚˜ì´ì§€
            test_tokenized_encoder_inputs = tokenizer(
                dialogues_with_prefix,
                return_tensors="pt",
                padding=True,
                add_special_tokens=True,
                truncation=True,
                max_length=self.config['tokenizer']['encoder_max_len'],
                return_token_type_ids=False
            )
            
            # Dataset ì¬ìƒì„±
            test_encoder_inputs_dataset = DatasetForInference(
                test_tokenized_encoder_inputs,
                test_data['fname'].tolist(),
                len(dialogues)
            )
            
            print(f"Applied prefix: '{input_prefix}' to {len(dialogues)} samples")
        
        return test_data, test_encoder_inputs_dataset
    
    def _save_results(self, output_df: pd.DataFrame) -> Path:
        """ê²°ê³¼ ì €ì¥ - baseline.pyì™€ ë™ì¼í•œ êµ¬ì¡°"""
        # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ì €ì¥ ê²½ë¡œ ì„¤ì •
        result_path = path_manager.ensure_dir("prediction")
        
        # ì‹¤í—˜ë³„ í´ë” ìƒì„±
        exp_folder = result_path / f"{self.experiment_name}_{timestamp}"
        exp_folder.mkdir(parents=True, exist_ok=True)
        
        # output.csv ì €ì¥
        output_path = exp_folder / "output.csv"
        output_df.to_csv(output_path, index=False)
        print(f"Saved submission to: {output_path}")
        
        # latest_output.csv ì—…ë°ì´íŠ¸
        latest_path = result_path / "latest_output.csv"
        output_df.to_csv(latest_path, index=False)
        print(f"Updated latest submission: {latest_path}")
        
        return output_path


# CLI ì§€ì› (ì„ íƒì‚¬í•­)
if __name__ == "__main__":
    import argparse
    from code.utils import load_config
    
    parser = argparse.ArgumentParser(description="í•™ìŠµ í›„ ì¶”ë¡  ì‹¤í–‰")
    parser.add_argument("--experiment", required=True, help="ì‹¤í—˜ëª…")
    parser.add_argument("--checkpoint", required=True, help="ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ")
    parser.add_argument("--config", required=True, help="ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--test-file", default="data/test.csv", help="í…ŒìŠ¤íŠ¸ íŒŒì¼")
    
    args = parser.parse_args()
    
    # ì„¤ì • ë¡œë“œ
    config = load_config(args.config)
    
    # ì¶”ë¡  ì‹¤í–‰
    inference = PostTrainingInference(
        experiment_name=args.experiment,
        model_path=args.checkpoint,
        config=config
    )
    
    submission_path = inference.run_test_inference(args.test_file)
    print(f"Inference completed: {submission_path}")
