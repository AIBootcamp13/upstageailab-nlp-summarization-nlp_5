{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-reference ROUGE 평가 시스템 테스트\n",
    "\n",
    "이 노트북은 대회 평가 방식인 3개의 정답 요약문에 대한 ROUGE 점수 계산을 테스트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 상위 디렉토리의 code를 import 경로에 추가\n",
    "sys.path.append(str(Path.cwd().parent / 'code'))\n",
    "\n",
    "from utils.metrics import (\n",
    "    RougeCalculator, \n",
    "    MultiReferenceROUGE,\n",
    "    calculate_rouge_scores,\n",
    "    compute_metrics_for_trainer,\n",
    "    evaluate_competition_format\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 테스트 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트용 데이터 생성\n",
    "test_predictions = [\n",
    "    \"날씨가 좋아 산책하기 좋은 날이라고 이야기했습니다.\",\n",
    "    \"프로젝트가 순조롭게 진행되고 있으며 다음 주까지 완료 예정입니다.\",\n",
    "    \"주말에 액션 영화를 보러 가기로 약속했습니다.\"\n",
    "]\n",
    "\n",
    "# 각 예측에 대한 3개의 참조 요약문\n",
    "test_references = [\n",
    "    # 첫 번째 예측에 대한 3개 참조\n",
    "    [\n",
    "        \"날씨가 좋아서 산책하기 좋다고 말했습니다.\",\n",
    "        \"좋은 날씨에 산책하면 좋겠다는 대화를 나눴습니다.\",\n",
    "        \"날씨가 좋아 산책하기 딱 좋은 날이라고 이야기했습니다.\"\n",
    "    ],\n",
    "    # 두 번째 예측에 대한 3개 참조\n",
    "    [\n",
    "        \"프로젝트가 잘 진행되고 있어 다음 주 완료 가능합니다.\",\n",
    "        \"프로젝트 진행이 순조로워 다음 주까지 끝낼 수 있습니다.\",\n",
    "        \"프로젝트가 순조롭게 진행 중이며 다음 주 완료 예정입니다.\"\n",
    "    ],\n",
    "    # 세 번째 예측에 대한 3개 참조\n",
    "    [\n",
    "        \"주말에 영화를 보러 가기로 했습니다.\",\n",
    "        \"액션 영화를 주말에 함께 보기로 약속했습니다.\",\n",
    "        \"주말에 액션 영화를 보러 가기로 했습니다.\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "print(f\"예측 수: {len(test_predictions)}\")\n",
    "print(f\"각 예측당 참조 수: {len(test_references[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 단일 참조 vs 다중 참조 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RougeCalculator 생성\n",
    "calculator = RougeCalculator(use_korean_tokenizer=True)\n",
    "\n",
    "# 첫 번째 예측에 대해 단일 참조와 다중 참조 비교\n",
    "prediction = test_predictions[0]\n",
    "references = test_references[0]\n",
    "\n",
    "print(\"예측:\", prediction)\n",
    "print(\"\\n참조 요약문들:\")\n",
    "for i, ref in enumerate(references):\n",
    "    print(f\"  {i+1}. {ref}\")\n",
    "\n",
    "print(\"\\n=== 단일 참조 ROUGE 점수 ===\")\n",
    "for i, ref in enumerate(references):\n",
    "    score = calculator.calculate_single_reference(prediction, ref)\n",
    "    print(f\"\\n참조 {i+1}과의 점수:\")\n",
    "    print(f\"  ROUGE-1 F1: {score.rouge1.f1:.4f}\")\n",
    "    print(f\"  ROUGE-2 F1: {score.rouge2.f1:.4f}\")\n",
    "    print(f\"  ROUGE-L F1: {score.rougeL.f1:.4f}\")\n",
    "    print(f\"  Combined: {score.rouge_combined_f1:.4f}\")\n",
    "\n",
    "print(\"\\n=== 다중 참조 ROUGE 점수 (최고 점수 선택) ===\")\n",
    "multi_score = calculator.calculate_multi_reference(prediction, references)\n",
    "print(f\"ROUGE-1 F1: {multi_score.rouge1.f1:.4f}\")\n",
    "print(f\"ROUGE-2 F1: {multi_score.rouge2.f1:.4f}\")\n",
    "print(f\"ROUGE-L F1: {multi_score.rougeL.f1:.4f}\")\n",
    "print(f\"Combined: {multi_score.rouge_combined_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 전체 데이터에 대한 다중 참조 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 예측에 대한 다중 참조 평가\n",
    "all_scores = []\n",
    "\n",
    "for pred, refs in zip(test_predictions, test_references):\n",
    "    score = calculator.calculate_multi_reference(pred, refs)\n",
    "    all_scores.append(score)\n",
    "\n",
    "# 평균 점수 계산\n",
    "avg_rouge1_f1 = np.mean([s.rouge1.f1 for s in all_scores])\n",
    "avg_rouge2_f1 = np.mean([s.rouge2.f1 for s in all_scores])\n",
    "avg_rougeL_f1 = np.mean([s.rougeL.f1 for s in all_scores])\n",
    "avg_combined_f1 = avg_rouge1_f1 + avg_rouge2_f1 + avg_rougeL_f1\n",
    "\n",
    "print(\"=== 전체 데이터 평균 점수 ===\")\n",
    "print(f\"ROUGE-1 F1: {avg_rouge1_f1:.4f}\")\n",
    "print(f\"ROUGE-2 F1: {avg_rouge2_f1:.4f}\")\n",
    "print(f\"ROUGE-L F1: {avg_rougeL_f1:.4f}\")\n",
    "print(f\"Combined F1 (대회 점수): {avg_combined_f1:.4f}\")\n",
    "\n",
    "# 개별 점수 확인\n",
    "print(\"\\n=== 개별 샘플 점수 ===\")\n",
    "for i, score in enumerate(all_scores):\n",
    "    print(f\"\\n샘플 {i+1}:\")\n",
    "    print(f\"  Combined F1: {score.rouge_combined_f1:.4f}\")\n",
    "    print(f\"  (R1: {score.rouge1.f1:.4f}, R2: {score.rouge2.f1:.4f}, RL: {score.rougeL.f1:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HuggingFace Trainer 호환 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가상의 토크나이저 (테스트용)\n",
    "class DummyTokenizer:\n",
    "    def __init__(self):\n",
    "        self.pad_token_id = 0\n",
    "    \n",
    "    def batch_decode(self, token_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True):\n",
    "        # 테스트를 위해 미리 정의된 텍스트 반환\n",
    "        return test_predictions[:len(token_ids)]\n",
    "\n",
    "# 토크나이저와 compute_metrics 함수 생성\n",
    "dummy_tokenizer = DummyTokenizer()\n",
    "\n",
    "# 단일 참조용 compute_metrics\n",
    "compute_metrics_single = compute_metrics_for_trainer(\n",
    "    dummy_tokenizer, \n",
    "    use_korean_tokenizer=True,\n",
    "    multi_reference=False\n",
    ")\n",
    "\n",
    "# 다중 참조용 compute_metrics\n",
    "compute_metrics_multi = compute_metrics_for_trainer(\n",
    "    dummy_tokenizer,\n",
    "    use_korean_tokenizer=True,\n",
    "    multi_reference=True\n",
    ")\n",
    "\n",
    "print(\"compute_metrics 함수 생성 완료\")\n",
    "print(f\"- 단일 참조: {type(compute_metrics_single)}\")\n",
    "print(f\"- 다중 참조: {type(compute_metrics_multi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 대회 형식 평가 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대회 형식의 테스트 데이터 생성\n",
    "# 예측 파일 (fname, summary)\n",
    "pred_data = pd.DataFrame({\n",
    "    'fname': ['dialogue_001.txt', 'dialogue_002.txt', 'dialogue_003.txt'],\n",
    "    'summary': test_predictions\n",
    "})\n",
    "\n",
    "# 정답 파일 (fname, summary1, summary2, summary3)\n",
    "truth_data = pd.DataFrame({\n",
    "    'fname': ['dialogue_001.txt', 'dialogue_002.txt', 'dialogue_003.txt'],\n",
    "    'summary1': [refs[0] for refs in test_references],\n",
    "    'summary2': [refs[1] for refs in test_references],\n",
    "    'summary3': [refs[2] for refs in test_references]\n",
    "})\n",
    "\n",
    "# 임시 파일로 저장\n",
    "pred_file = 'test_predictions.csv'\n",
    "truth_file = 'test_ground_truth.csv'\n",
    "\n",
    "pred_data.to_csv(pred_file, index=False)\n",
    "truth_data.to_csv(truth_file, index=False)\n",
    "\n",
    "print(\"예측 파일:\")\n",
    "print(pred_data)\n",
    "print(\"\\n정답 파일:\")\n",
    "print(truth_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대회 형식 평가 실행\n",
    "competition_results = evaluate_competition_format(\n",
    "    pred_file,\n",
    "    truth_file,\n",
    "    use_korean_tokenizer=True\n",
    ")\n",
    "\n",
    "# 이전 결과와 비교\n",
    "print(\"\\n=== 결과 비교 ===\")\n",
    "print(f\"수동 계산 Combined F1: {avg_combined_f1:.4f}\")\n",
    "print(f\"함수 계산 Combined F1: {competition_results['rouge_combined_f1']:.4f}\")\n",
    "print(f\"차이: {abs(avg_combined_f1 - competition_results['rouge_combined_f1']):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 파일 정리\n",
    "import os\n",
    "os.remove(pred_file)\n",
    "os.remove(truth_file)\n",
    "print(\"테스트 파일 삭제 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 극단적인 케이스 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 극단적인 케이스들\n",
    "edge_cases = [\n",
    "    # 케이스 1: 완벽한 일치\n",
    "    {\n",
    "        'prediction': \"프로젝트가 성공적으로 완료되었습니다.\",\n",
    "        'references': [\n",
    "            \"프로젝트가 성공적으로 완료되었습니다.\",\n",
    "            \"프로젝트가 성공적으로 완료되었습니다.\",\n",
    "            \"프로젝트가 성공적으로 완료되었습니다.\"\n",
    "        ]\n",
    "    },\n",
    "    # 케이스 2: 완전히 다른 내용\n",
    "    {\n",
    "        'prediction': \"날씨가 좋습니다.\",\n",
    "        'references': [\n",
    "            \"프로젝트를 진행합니다.\",\n",
    "            \"회의를 시작합니다.\",\n",
    "            \"보고서를 작성합니다.\"\n",
    "        ]\n",
    "    },\n",
    "    # 케이스 3: 참조 중 하나만 유사\n",
    "    {\n",
    "        'prediction': \"회의에서 예산을 논의했습니다.\",\n",
    "        'references': [\n",
    "            \"회의에서 예산을 논의했습니다.\",\n",
    "            \"점심 메뉴를 정했습니다.\",\n",
    "            \"주말 계획을 세웠습니다.\"\n",
    "        ]\n",
    "    },\n",
    "    # 케이스 4: 빈 문자열\n",
    "    {\n",
    "        'prediction': \"\",\n",
    "        'references': [\"요약1\", \"요약2\", \"요약3\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=== 극단적인 케이스 테스트 ===\")\n",
    "for i, case in enumerate(edge_cases):\n",
    "    print(f\"\\n케이스 {i+1}:\")\n",
    "    print(f\"예측: '{case['prediction']}'\")\n",
    "    print(f\"참조: {case['references']}\")\n",
    "    \n",
    "    score = calculator.calculate_multi_reference(\n",
    "        case['prediction'], \n",
    "        case['references']\n",
    "    )\n",
    "    \n",
    "    print(f\"결과: Combined F1 = {score.rouge_combined_f1:.4f}\")\n",
    "    print(f\"      (R1: {score.rouge1.f1:.4f}, R2: {score.rouge2.f1:.4f}, RL: {score.rougeL.f1:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 성능 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 대량 데이터 생성\n",
    "n_samples = 100\n",
    "large_predictions = test_predictions * (n_samples // 3)\n",
    "large_references = test_references * (n_samples // 3)\n",
    "\n",
    "print(f\"테스트 샘플 수: {len(large_predictions)}\")\n",
    "\n",
    "# 단일 참조 성능\n",
    "start_time = time.time()\n",
    "for pred, refs in zip(large_predictions, large_references):\n",
    "    calculator.calculate_single_reference(pred, refs[0])\n",
    "single_time = time.time() - start_time\n",
    "\n",
    "# 다중 참조 성능\n",
    "start_time = time.time()\n",
    "for pred, refs in zip(large_predictions, large_references):\n",
    "    calculator.calculate_multi_reference(pred, refs)\n",
    "multi_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n=== 성능 측정 결과 ===\")\n",
    "print(f\"단일 참조 처리 시간: {single_time:.2f}초 ({single_time/len(large_predictions)*1000:.2f}ms/샘플)\")\n",
    "print(f\"다중 참조 처리 시간: {multi_time:.2f}초 ({multi_time/len(large_predictions)*1000:.2f}ms/샘플)\")\n",
    "print(f\"속도 차이: {multi_time/single_time:.1f}배\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결론\n",
    "\n",
    "Multi-reference ROUGE 평가 시스템이 성공적으로 구현되었습니다:\n",
    "\n",
    "1. **정확한 다중 참조 처리**: 3개의 참조 요약문 중 각 메트릭별로 최고 점수를 선택\n",
    "2. **대회 평가 방식 준수**: rouge_combined_f1 = rouge1_f1 + rouge2_f1 + rougeL_f1\n",
    "3. **HuggingFace Trainer 호환**: compute_metrics_for_trainer() 함수로 쉽게 통합\n",
    "4. **대회 형식 평가 지원**: evaluate_competition_format() 함수로 제출 파일 직접 평가\n",
    "5. **안정적인 처리**: 빈 문자열, 완벽한 일치 등 극단적인 케이스도 정상 처리\n",
    "\n",
    "이제 대회의 평가 방식과 100% 일치하는 평가를 수행할 수 있습니다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}