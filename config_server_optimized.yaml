# 서버 최적화 설정 (RTX 3090 24GB, 251GB RAM, 48코어)

# eenzeenee T5 한국어 요약 모델 설정 (고성능 서버용)
eenzeenee:
  general:
    model_name: eenzeenee/t5-base-korean-summarization
    data_path: ../data/
    output_dir: ./eenzeenee_outputs/
    model_type: seq2seq
    name: eenzeenee_korean_summarization_optimized
  
  input_prefix: "summarize: "
  
  tokenizer:
    bos_token: <pad>
    eos_token: </s>
    encoder_max_len: 512
    decoder_max_len: 64
    special_tokens:
      - '#Person1#'
      - '#Person2#'
      - '#Person3#'
      - '#PhoneNumber#'
      - '#Address#'
      - '#PassportNumber#'
      - '#DateOfBirth#'
      - '#SSN#'
      - '#CardNumber#'
      - '#CarNumber#'
      - '#Email#'
  
  # 고성능 서버 최적화 추론 설정
  inference:
    batch_size: 32              # RTX 3090 24GB 활용
    generate_max_length: 64
    tags: [eenzeenee, T5-base, Korean, RTX3090]
    no_repeat_ngram_size: 2
    early_stopping: true
    length_penalty: 1.0
    do_sample: false
    result_path: ./eenzeenee_predictions/
  
  # 서버 최적화 QLoRA 설정 (Unsloth 활성화)
  qlora:
    use_unsloth: true
    use_qlora: true
    name: eenzeenee_optimized
    lora_rank: 16
    lora_alpha: 32
    lora_dropout: 0.1
    target_modules: ["q", "k", "v", "o"]
    load_in_4bit: true
    bnb_4bit_compute_dtype: "float16"
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_use_double_quant: true
  
  # 고성능 서버 학습 설정
  training:
    do_eval: true
    do_train: true
    eval_strategy: steps
    eval_steps: 200                    # 더 자주 평가
    early_stopping_patience: 3
    early_stopping_threshold: 0.001
    
    # 고성능 배치 설정
    per_device_train_batch_size: 16    # RTX 3090 24GB 최적화  
    per_device_eval_batch_size: 32     # 평가는 더 큰 배치
    gradient_accumulation_steps: 1     # 누적 불필요
    
    # 메모리 및 성능 최적화
    fp16: true                         # 필수
    gradient_checkpointing: false      # 메모리 여유로우므로 비활성화
    dataloader_num_workers: 8          # 48코어 활용
    dataloader_pin_memory: true        # 251GB RAM 활용
    group_by_length: true              # 효율성 증대
    
    # 학습 파라미터
    learning_rate: 3.0e-05
    num_train_epochs: 5
    lr_scheduler_type: cosine
    warmup_ratio: 0.1
    weight_decay: 0.01
    
    # 로깅 및 저장
    logging_steps: 50                  # 더 자주 로깅
    save_strategy: steps
    save_steps: 200
    save_total_limit: 5                # 디스크 여유로우므로 더 많이 저장
    load_best_model_at_end: true
    
    # 생성 설정
    predict_with_generate: true
    generation_num_beams: 4
    generation_max_length: 64
    
    report_to: wandb
    seed: 42

# mT5 XL-Sum 대형 모델 설정 (서버 최적화)
xlsum_mt5:
  general:
    model_name: csebuetnlp/mT5_multilingual_XLSum
    data_path: ../data/
    output_dir: ./xlsum_mt5_outputs/
    model_type: seq2seq
    input_prefix: ""
  
  tokenizer:
    bos_token: <pad>
    eos_token: </s>
    encoder_max_len: 512
    decoder_max_len: 84
    special_tokens:
      - '#Person1#'
      - '#Person2#'
      - '#Person3#'
      - '#PhoneNumber#'
      - '#Address#'
      - '#PassportNumber#'
      - '#DateOfBirth#'
      - '#SSN#'
      - '#CardNumber#'
      - '#CarNumber#'
      - '#Email#'
  
  # 대형 모델 최적화 추론 설정
  inference:
    batch_size: 16                     # 대형 모델 조정
    generate_max_length: 84
    num_beams: 4
    result_path: ./xlsum_mt5_predictions/
  
  # 대형 모델 QLoRA 설정 (Unsloth 활성화)
  qlora:
    use_unsloth: true
    r: 16
    lora_alpha: 32
    lora_dropout: 0.1
    bias: "none"
    task_type: "SEQ_2_SEQ_LM"
    target_modules: ["q", "k", "v", "o"]
    load_in_4bit: true
    bnb_4bit_compute_dtype: float16
    bnb_4bit_quant_type: nf4
    bnb_4bit_use_double_quant: true
  
  # 서버 최적화 대형 모델 학습 설정
  training:
    do_eval: true
    do_train: true
    early_stopping_patience: 3
    evaluation_strategy: steps
    eval_steps: 300
    
    # 대형 모델 배치 설정 (RTX 3090 24GB)
    per_device_train_batch_size: 8     # 대형 모델 안전 설정
    per_device_eval_batch_size: 16
    gradient_accumulation_steps: 2     # 유효 배치 크기 16
    
    # 최적화 설정
    fp16: true
    gradient_checkpointing: false      # 메모리 여유
    dataloader_num_workers: 12         # 더 많은 워커
    dataloader_pin_memory: true
    
    learning_rate: 5e-5
    num_train_epochs: 3                # 대형 모델은 적은 epoch
    lr_scheduler_type: cosine
    warmup_ratio: 0.1
    weight_decay: 0.01
    
    logging_steps: 50
    save_strategy: steps
    save_steps: 300
    save_total_limit: 3
    load_best_model_at_end: true
    
    predict_with_generate: true
    generation_num_beams: 4
    generation_max_length: 84
    
    report_to: wandb
    seed: 42

# WandB 설정
wandb:
  entity: lyjune37-juneictlab
  project: nlp-5
  name: rtx3090_optimized_lyj
  tags: [RTX3090, 24GB, high_performance, lyj]
  notes: "RTX 3090 24GB 서버 최적화 실험"
