#!/bin/bash
# λ¨λΈ μλ™ λ‹¤μ΄λ΅λ“ λ° μΊμ‹ μµμ ν™” μ¤ν¬λ¦½νΈ
# Models Auto Download and Cache Optimization Script

echo "π€ HuggingFace λ¨λΈ μΊμ‹ μµμ ν™” μ‹μ‘..."

# ν™κ²½ μ„¤μ •
export HUGGINGFACE_HUB_CACHE="/data/ephemeral/home/upstageailab-nlp-summarization-nlp_5/nlp-sum-lyj/.hf_cache"
export TRANSFORMERS_CACHE="/data/ephemeral/home/upstageailab-nlp-summarization-nlp_5/nlp-sum-lyj/.hf_cache"
export HF_HOME="/data/ephemeral/home/upstageailab-nlp-summarization-nlp_5/nlp-sum-lyj/.hf_cache"

# μΊμ‹ λ””λ ‰ν† λ¦¬ μƒμ„±
mkdir -p "$HUGGINGFACE_HUB_CACHE"

echo "π“‚ μΊμ‹ λ””λ ‰ν† λ¦¬ μ„¤μ •: $HUGGINGFACE_HUB_CACHE"

# Pythonμ„ μ‚¬μ©ν• λ¨λΈ μ‚¬μ „ λ‹¤μ΄λ΅λ“
/opt/conda/bin/python3 -c "
import os
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

print('π”— ν™κ²½ λ³€μ ν™•μΈ...')
print(f'HF_HOME: {os.environ.get(\"HF_HOME\", \"Not Set\")}')
print(f'TRANSFORMERS_CACHE: {os.environ.get(\"TRANSFORMERS_CACHE\", \"Not Set\")}')

models = [
    'csebuetnlp/mT5_multilingual_XLSum',
    'eenzeenee/t5-base-korean-summarization'
]

for model_name in models:
    print(f'π“¥ {model_name} λ‹¤μ΄λ΅λ“ μ¤‘...')
    try:
        # ν† ν¬λ‚μ΄μ € λ‹¤μ΄λ΅λ“
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=os.environ.get('HUGGINGFACE_HUB_CACHE')
        )
        print(f'β… {model_name} ν† ν¬λ‚μ΄μ € λ‹¤μ΄λ΅λ“ μ™„λ£')
        
        # λ¨λΈ λ‹¤μ΄λ΅λ“ (λ©”λ¨λ¦¬ ν¨μ¨μ„ μ„ν•΄ torch_dtype μ§€μ •)
        model = AutoModelForSeq2SeqLM.from_pretrained(
            model_name,
            cache_dir=os.environ.get('HUGGINGFACE_HUB_CACHE'),
            torch_dtype=torch.float16,
            device_map='auto'
        )
        print(f'β… {model_name} λ¨λΈ λ‹¤μ΄λ΅λ“ μ™„λ£')
        
        # λ©”λ¨λ¦¬ μ •λ¦¬
        del model
        del tokenizer
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
    except Exception as e:
        print(f'β {model_name} λ‹¤μ΄λ΅λ“ μ‹¤ν¨: {e}')
        continue

print('π‰ λ¨λ“  λ¨λΈ λ‹¤μ΄λ΅λ“ μ™„λ£!')
"

# μΊμ‹ μ •λ³΄ μ¶λ ¥
echo "π“ μΊμ‹ μƒνƒ ν™•μΈ..."
du -sh "$HUGGINGFACE_HUB_CACHE" 2>/dev/null || echo "μΊμ‹ λ””λ ‰ν† λ¦¬ ν¬κΈ° ν™•μΈ μ‹¤ν¨"
ls -la "$HUGGINGFACE_HUB_CACHE" 2>/dev/null || echo "μΊμ‹ λ””λ ‰ν† λ¦¬κ°€ μ•„μ§ μƒμ„±λμ§€ μ•μ"

echo "β¨ λ¨λΈ μλ™ λ‹¤μ΄λ΅λ“ μ„¤μ • μ™„λ£!"
echo "μ΄μ  μ‹¤ν— μ‹ λ¨λΈμ΄ μΈν„°λ„·μ—μ„ μλ™μΌλ΅ λ‹¤μ΄λ΅λ“λ©λ‹λ‹¤."
